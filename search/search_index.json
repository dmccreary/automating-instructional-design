{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Automating Instructional Design","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to Automating Instructional Design, a hands-on course for educators and training professionals.</p> <p></p>"},{"location":"about/","title":"About This Course","text":"<p>What if you could create interactive educational simulations in minutes instead of weeks\u2014without writing a single line of code?</p> <p>This course teaches you how to leverage AI-assisted tools to transform learning objectives into interactive educational simulations called MicroSims. You'll bridge the gap between abstract pedagogical goals and concrete, interactive learning experiences that engage students and deepen understanding.</p> <p>You'll learn to analyze learning objectives using Bloom's Taxonomy, select appropriate visualization paradigms, write effective specifications, and generate working simulations\u2014all without programming experience.</p>"},{"location":"about/#who-this-course-is-for","title":"Who This Course Is For","text":"<p>This course is designed for educators and learning professionals who want to create engaging interactive content:</p> <ul> <li>K-12 Teachers seeking to bring abstract concepts to life</li> <li>Corporate Training Specialists building memorable learning experiences</li> <li>Higher Education Faculty enhancing lectures with interactive demonstrations</li> <li>Instructional Designers at EdTech companies scaling content creation</li> <li>Curriculum Developers modernizing educational materials</li> <li>Subject Matter Experts who want to share knowledge visually</li> </ul> <p>No programming experience required. The course focuses on specification, evaluation, and iteration\u2014you describe what you want, and AI handles the implementation.</p>"},{"location":"about/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Bloom's Taxonomy Analysis - Classify objectives by cognitive complexity</li> <li>Visualization Paradigms - Match concepts to the right visual representation (p5.js, vis-network, Chart.js, Mermaid, and more)</li> <li>MicroSim Specifications - Write detailed blueprints that AI can implement</li> <li>Cognitive Load Design - Create effective learning experiences</li> <li>Quality Evaluation - Assess simulations across technical, pedagogical, and UX dimensions</li> <li>Iterative Refinement - Test with users and improve designs</li> </ul>"},{"location":"about/#getting-started","title":"Getting Started","text":"<p>Ready to create your first MicroSim? Start with Chapter 1: Foundations of Learning Objective Analysis, where you'll learn to deconstruct learning objectives and identify the perfect visualization approach for any concept.</p>"},{"location":"about/#navigation","title":"Navigation","text":"<p>Use the left sidebar to explore:</p> <ul> <li>Chapters - Main course content (12 modules)</li> <li>Learning Graph - Interactive concept visualization</li> <li>MicroSims - Interactive educational simulations</li> <li>Glossary - Key terms and definitions</li> <li>Stories - Narrative explorations of key concepts</li> </ul>"},{"location":"about/#background-for-the-course","title":"Background for the Course","text":""},{"location":"about/#a-30-year-journey","title":"A 30-Year Journey","text":"<p>In 1995, I read Neal Stephenson's cyberpunk novel The Diamond Age: Or, A Young Lady's Illustrated Primer. The story captivated me: a young girl acquires an AI-powered tablet that generates personalized lessons in real time, adapting to her every educational need. For three decades, that vision has driven my work. How could we actually build such a device?</p> <p>As a software engineer, I recognized that the key was representing learning paths\u2014understanding which concepts depend on others and how knowledge builds over time. But traditional database tables weren't designed for these interconnected relationships.</p>"},{"location":"about/#the-knowledge-graph-revolution","title":"The Knowledge Graph Revolution","text":"<p>In 2007, my friend Kurt Cagle introduced me to graph databases. I was amazed at how much faster development could be with the right data structures. In 2011, I launched the NoSQL Now! conference series and published Making Sense of NoSQL with Manning Publications. My research on knowledge graphs confirmed what I had suspected: intelligent textbooks needed non-tabular foundations.</p>"},{"location":"about/#the-ai-turning-point","title":"The AI Turning Point","text":"<p>On September 12, 2020, I first blogged about using GPT-3 to generate lesson plans. At the time, I was building the world's largest healthcare knowledge graph for a Fortune-10 company, earning several patents on non-relational databases in healthcare. I realized that the ultimate tool for generating intelligent textbooks would combine LLMs with knowledge graphs that store concept dependencies.</p>"},{"location":"about/#microsims-are-born","title":"MicroSims Are Born","text":"<p>In fall 2023, working with Val Lockhart, we refined the concept of AI-generated classroom simulations. Val's breakthrough insight: by having AI generate p5.js code directly, we could produce high-quality interactive visualizations without wrestling with raw HTML, SVG, and CSS. We continued refining our approach until publishing our research paper on MicroSims.</p>"},{"location":"about/#the-final-piece","title":"The Final Piece","text":"<p>The introduction of Claude Code Skills triggered a 100x productivity jump in MicroSim generation. The pieces finally came together: knowledge graphs for structure, AI for generation, and MicroSims for engagement. We're now ready to transform how instructional design is done.</p> <p>Dan McCreary December 19th, 2025</p>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"course-description/","title":"Automating Instructional Design","text":""},{"location":"course-description/#from-learning-objectives-to-interactive-microsimulations","title":"From Learning Objectives to Interactive MicroSimulations","text":"<p>Course Duration: 12 Modules (Self-Paced or 12-Week Instructor-Led) Level: Intermediate Prerequisites: Basic computer literacy; no programming experience required</p>"},{"location":"course-description/#course-overview","title":"Course Overview","text":"<p>This course teaches educators and training professionals how to leverage AI-assisted tools to transform learning objectives into interactive educational simulations called MicroSims. Participants will master the complete lifecycle of MicroSim development\u2014from analyzing pedagogical goals through deployment and learner assessment\u2014using Claude Code skills and a comprehensive library of visualization tools.</p> <p>The central challenge of instructional design automation is bridging the semantic gap between abstract learning outcomes and concrete interactive experiences. A well-crafted learning objective like \"Students will understand the relationship between supply and demand\" contains implicit assumptions about visualization, interaction, pacing, and assessment that must be made explicit before any simulation can be built. This course provides systematic methods for that translation.</p>"},{"location":"course-description/#target-audience","title":"Target Audience","text":"<p>This course is designed for working professionals who create educational content:</p> <ul> <li>K-12 Teachers seeking to supplement curriculum with interactive demonstrations</li> <li>Corporate Training Specialists developing employee onboarding and skill development programs</li> <li>Higher Education Faculty creating simulations for undergraduate and graduate courses</li> <li>Instructional Designers at educational technology companies</li> <li>Curriculum Developers building standards-aligned educational materials</li> <li>Subject Matter Experts who want to make their knowledge more accessible</li> </ul> <p>No programming background is required. The course teaches participants to work with AI tools that generate code, focusing on specification, evaluation, and iteration rather than implementation details.</p>"},{"location":"course-description/#learning-objectives","title":"Learning Objectives","text":"<p>Upon completing this course, participants will be able to:</p> <ol> <li>Analyze learning objectives to identify simulation-ready components and prerequisite concepts</li> <li>Select appropriate visualization paradigms (animation, network, timeline, chart, map) based on concept characteristics</li> <li>Write detailed MicroSim specifications that preserve pedagogical intent through AI generation</li> <li>Evaluate generated simulations against educational effectiveness criteria</li> <li>Adapt MicroSim complexity for audiences ranging from kindergarten to graduate school</li> <li>Test simulations with target learners and interpret feedback systematically</li> <li>Iterate on designs using structured refinement protocols</li> <li>Build a personal library of reusable MicroSim patterns and templates</li> </ol>"},{"location":"course-description/#the-microsim-design-challenge","title":"The MicroSim Design Challenge","text":""},{"location":"course-description/#why-this-is-hard","title":"Why This Is Hard","text":"<p>Converting a learning objective to a working simulation requires decisions at multiple levels:</p> Level Question Example Conceptual What is the core insight? Gravity accelerates all objects equally Visual How should it look? Two balls of different sizes falling side by side Interactive What can the learner control? Drop height, ball mass, air resistance toggle Temporal How does it unfold? Real-time vs. slow motion vs. step-by-step Feedback How does the learner know they understand? Prediction prompt before reveal Scaffolding What support is needed? Labels, hints, worked examples <p>Each decision affects learning outcomes. A simulation that is technically accurate but visually overwhelming fails its pedagogical purpose. A simulation that is beautifully designed but doesn't target the right misconception wastes learner time.</p>"},{"location":"course-description/#the-automation-opportunity","title":"The Automation Opportunity","text":"<p>AI tools can generate working code from natural language descriptions, but they cannot automatically determine what should be built. The instructional designer's role shifts from implementation to specification and quality assurance. This course teaches both.</p>"},{"location":"course-description/#course-modules","title":"Course Modules","text":""},{"location":"course-description/#module-1-foundations-of-learning-objective-analysis","title":"Module 1: Foundations of Learning Objective Analysis","text":"<p>Topics:</p> <ul> <li>Bloom's Taxonomy and cognitive complexity levels</li> <li>Decomposing compound objectives into atomic concepts</li> <li>Identifying implicit prerequisites and assumed knowledge</li> <li>The \"simulation readiness\" test: which objectives benefit from interaction?</li> </ul> <p>Hands-On Activity: Analyze 10 learning objectives from your own teaching context. Classify each by Bloom's level and identify which are candidates for MicroSim development.</p> <p>MicroSim for This Module: Bloom's Taxonomy Classifier \u2014 An interactive tool where users drag learning objectives to the appropriate taxonomy level, receiving immediate feedback on their classification with explanations of why objectives belong at particular levels.</p>"},{"location":"course-description/#module-2-the-microsim-pattern-library","title":"Module 2: The MicroSim Pattern Library","text":"<p>Topics:</p> <ul> <li>Overview of visualization paradigms and their pedagogical affordances</li> <li>p5.js animations: Motion, physics, dynamic systems, cause-and-effect</li> <li>Network graphs (vis-network): Relationships, hierarchies, dependencies, influence</li> <li>Timelines (vis-timeline): Sequences, history, processes, scheduling</li> <li>Charts (Chart.js, Plotly): Comparisons, trends, distributions, correlations</li> <li>Maps (Leaflet): Geography, spatial relationships, location-based data</li> <li>Diagrams (Mermaid): Flowcharts, state machines, organizational structures</li> <li>Set visualizations (Venn.js): Classification, overlap, logical relationships</li> </ul> <p>Hands-On Activity: Match 20 learning objectives to appropriate visualization types. Justify each selection based on concept characteristics.</p> <p>MicroSim for This Module: Visualization Selector \u2014 Users input a learning objective description and the system suggests appropriate visualization types with example thumbnails. Users can explore why certain visualizations work better for certain concept types.</p>"},{"location":"course-description/#module-3-writing-effective-microsim-specifications","title":"Module 3: Writing Effective MicroSim Specifications","text":"<p>Topics:</p> <ul> <li>The anatomy of a MicroSim specification document</li> <li>Describing visual elements without writing code</li> <li>Specifying interaction behaviors and constraints</li> <li>Defining success criteria and edge cases</li> <li>Common specification ambiguities and how to avoid them</li> <li>The \"telephone game\" problem: preserving intent through AI interpretation</li> </ul> <p>Hands-On Activity: Write specifications for three MicroSims. Exchange specifications with a partner and attempt to build from each other's specs. Identify gaps and ambiguities.</p> <p>MicroSim for This Module: Specification Completeness Checker \u2014 An interactive checklist that guides users through specification writing, highlighting missing elements and common pitfalls. Shows examples of vague vs. precise specifications side by side.</p>"},{"location":"course-description/#module-4-adapting-for-audience-levels","title":"Module 4: Adapting for Audience Levels","text":"<p>Topics:</p> <ul> <li>Cognitive development stages and their implications for simulation design</li> <li>Early Childhood (K-2): Large touch targets, simple cause-effect, minimal text, bright colors</li> <li>Elementary (3-5): Guided exploration, scaffolded complexity, reading support</li> <li>Middle School (6-8): Abstract concepts, multiple variables, hypothesis testing</li> <li>High School (9-12): Real-world applications, data interpretation, edge cases</li> <li>Undergraduate: Theoretical foundations, mathematical relationships, professional contexts</li> <li>Graduate: Research applications, limitations, advanced parameter spaces</li> <li>Corporate: Job-relevant scenarios, time-efficient, immediately applicable</li> </ul> <p>Hands-On Activity: Take a single concept (e.g., \"probability\") and design MicroSim specifications for three different audience levels. Document how complexity, vocabulary, and interaction patterns change.</p> <p>MicroSim for This Module: Audience Adaptation Slider \u2014 A demonstration showing the same core concept (like fractions) rendered at different complexity levels. A slider moves between kindergarten and graduate school versions, highlighting what changes at each level.</p>"},{"location":"course-description/#module-5-cognitive-load-and-visual-design","title":"Module 5: Cognitive Load and Visual Design","text":"<p>Topics:</p> <ul> <li>Cognitive load theory: intrinsic, extraneous, and germane load</li> <li>The \"split attention\" effect and how simulations can cause it</li> <li>Progressive disclosure: revealing complexity gradually</li> <li>Color, contrast, and accessibility considerations</li> <li>Animation speed and learner control</li> <li>When less is more: the case for simplicity</li> </ul> <p>Hands-On Activity: Evaluate five existing MicroSims for cognitive load issues. Redesign one to reduce extraneous load while preserving learning effectiveness.</p> <p>MicroSim for This Module: Cognitive Load Visualizer \u2014 An interactive demonstration where users can toggle various visual elements (labels, colors, animations, data points) on and off while a \"cognitive load meter\" shows estimated mental effort. Helps develop intuition for design tradeoffs.</p>"},{"location":"course-description/#module-6-anticipating-misconceptions","title":"Module 6: Anticipating Misconceptions","text":"<p>Topics:</p> <ul> <li>Common misconceptions by subject area</li> <li>How simulations can reinforce vs. correct misconceptions</li> <li>The \"productive failure\" approach: letting learners discover errors</li> <li>Building prediction prompts into simulations</li> <li>Edge cases that reveal conceptual boundaries</li> </ul> <p>Hands-On Activity: For a concept in your teaching area, list five common misconceptions. Design a MicroSim that specifically targets one misconception.</p> <p>MicroSim for This Module: Misconception Reveal \u2014 A simulation about a commonly misunderstood concept (like seasons being caused by Earth's distance from the sun) that first lets users explore their intuition, then reveals the correct model with explicit comparison.</p>"},{"location":"course-description/#module-7-generating-microsims-with-ai-tools","title":"Module 7: Generating MicroSims with AI Tools","text":"<p>Topics:</p> <ul> <li>Overview of Claude Code skills and the MicroSim generation toolkit</li> <li>Invoking the microsim-generator skill effectively</li> <li>Interpreting generated output and identifying issues</li> <li>The iterative refinement conversation: prompting for changes</li> <li>When to regenerate vs. when to manually adjust</li> <li>Version control and managing multiple iterations</li> </ul> <p>Hands-On Activity: Generate your first complete MicroSim from a specification you wrote in Module 3. Document the generation process including any refinement prompts needed.</p> <p>MicroSim for This Module: Generation Workflow Simulator \u2014 An interactive flowchart showing the MicroSim generation process. Users can click through different decision points and see example prompts and outputs for each path.</p>"},{"location":"course-description/#module-8-quality-evaluation-frameworks","title":"Module 8: Quality Evaluation Frameworks","text":"<p>Topics:</p> <ul> <li>The three-lens evaluation model:<ul> <li>Technical: Does it work? Is it responsive? Are there bugs?</li> <li>Pedagogical: Does it target the learning objective? Is the cognitive level appropriate?</li> <li>User Experience: Is it intuitive? Accessible? Engaging without being distracting?</li> </ul> </li> <li>Creating evaluation rubrics for different MicroSim types</li> <li>Automated vs. human evaluation methods</li> <li>Documentation requirements for reusability</li> </ul> <p>Hands-On Activity: Develop an evaluation rubric for your subject area. Apply it to three MicroSims (one you created, two from the library).</p> <p>MicroSim for This Module: Evaluation Rubric Builder \u2014 An interactive tool where users select criteria from categorized lists to build custom evaluation rubrics. Can export rubrics as checklists for practical use.</p>"},{"location":"course-description/#module-9-user-testing-methods","title":"Module 9: User Testing Methods","text":"<p>Topics:</p> <ul> <li>Think-aloud protocols for simulation testing</li> <li>A/B testing design for educational interventions</li> <li>Gathering feedback from learners of different ages and abilities</li> <li>Observational techniques: what to watch for</li> <li>Interpreting test results and prioritizing changes</li> <li>Ethical considerations in educational research</li> </ul> <p>Hands-On Activity: Conduct a think-aloud test with 2-3 learners using a MicroSim you created. Document observations and identify three potential improvements.</p> <p>MicroSim for This Module: Test Session Planner \u2014 An interactive guide that helps users design user testing sessions. Inputs include learner age, concept complexity, and available time. Outputs include suggested protocols, question templates, and observation checklists.</p>"},{"location":"course-description/#module-10-iteration-and-refinement","title":"Module 10: Iteration and Refinement","text":"<p>Topics:</p> <ul> <li>The design-test-refine cycle</li> <li>Prioritizing changes: critical vs. nice-to-have</li> <li>When a MicroSim needs fundamental redesign vs. incremental improvement</li> <li>Building on feedback without scope creep</li> <li>Knowing when a MicroSim is \"done enough\"</li> <li>Maintaining a change log and design rationale</li> </ul> <p>Hands-On Activity: Take a MicroSim through two complete iteration cycles. Document each change and its rationale.</p> <p>MicroSim for This Module: Iteration Tracker \u2014 A visual tool showing a MicroSim's evolution through versions. Users can see what changed between iterations, why changes were made, and how evaluation scores improved.</p>"},{"location":"course-description/#module-11-building-for-diverse-learners","title":"Module 11: Building for Diverse Learners","text":"<p>Topics:</p> <ul> <li>Universal Design for Learning (UDL) principles applied to simulations</li> <li>Accessibility requirements: screen readers, keyboard navigation, color blindness</li> <li>Language considerations: vocabulary level, multilingual support</li> <li>Cultural sensitivity in examples and contexts</li> <li>Supporting learners with different prior knowledge levels</li> <li>Differentiation strategies within a single MicroSim</li> </ul> <p>Hands-On Activity: Audit a MicroSim for accessibility issues. Implement at least three accessibility improvements.</p> <p>MicroSim for This Module: Accessibility Simulator \u2014 A tool that lets users experience a MicroSim under various simulated constraints: grayscale mode (color blindness), keyboard-only navigation, screen reader text output, and reduced motion settings.</p>"},{"location":"course-description/#module-12-deployment-and-integration","title":"Module 12: Deployment and Integration","text":"<p>Topics:</p> <ul> <li>Embedding MicroSims in learning management systems (LMS)</li> <li>Integration with intelligent textbooks and course materials</li> <li>Analytics: tracking learner interactions</li> <li>Maintenance and updates over time</li> <li>Building a personal MicroSim library</li> <li>Sharing and collaboration with other educators</li> </ul> <p>Hands-On Activity: Deploy a completed MicroSim to your teaching context. Collect initial analytics and plan for ongoing maintenance.</p> <p>MicroSim for This Module: Deployment Checklist \u2014 An interactive checklist that guides users through deployment steps, from final testing through LMS integration. Tracks completion and provides troubleshooting tips for common issues.</p>"},{"location":"course-description/#assessment-methods","title":"Assessment Methods","text":""},{"location":"course-description/#formative-assessment-throughout-course","title":"Formative Assessment (Throughout Course)","text":"<ul> <li>Module reflection journals documenting design decisions</li> <li>Peer feedback on specifications and generated MicroSims</li> <li>Self-evaluation using course rubrics</li> </ul>"},{"location":"course-description/#summative-assessment-course-completion","title":"Summative Assessment (Course Completion)","text":"<p>Portfolio Project: Create a complete MicroSim package including:</p> <ol> <li>Learning objective analysis document</li> <li>Detailed specification</li> <li>Working MicroSim (generated and refined)</li> <li>Evaluation rubric with self-assessment</li> <li>User testing report (minimum 3 participants)</li> <li>Iteration log showing refinement process</li> <li>Accessibility audit and improvements</li> <li>Deployment documentation</li> </ol> <p>Portfolio Evaluation Criteria:</p> Criterion Weight Learning objective alignment 20% Specification quality and completeness 15% Technical functionality 15% Pedagogical effectiveness 20% User testing rigor 15% Iteration quality 10% Documentation clarity 5%"},{"location":"course-description/#microsims-teaching-this-course-meta-level","title":"MicroSims Teaching This Course (Meta-Level)","text":"<p>The following MicroSims are developed specifically to teach the concepts in this course, demonstrating the principles being taught:</p> MicroSim Type Module Concept Taught Bloom's Taxonomy Classifier Drag-and-drop classification 1 Cognitive levels of learning objectives Visualization Selector Decision tree with examples 2 Matching concepts to visualization types Specification Completeness Checker Interactive checklist 3 Components of effective specifications Audience Adaptation Slider Comparison slider 4 How complexity scales with audience Cognitive Load Visualizer Toggle demonstration 5 Visual design impact on mental effort Misconception Reveal Prediction-reveal sequence 6 Designing for conceptual change Generation Workflow Simulator Interactive flowchart 7 The AI-assisted creation process Evaluation Rubric Builder Configurable checklist 8 Quality assessment frameworks Test Session Planner Guided planning tool 9 User testing methodology Iteration Tracker Version comparison 10 The refinement cycle Accessibility Simulator Constraint simulation 11 Universal design principles Deployment Checklist Progress tracker 12 Launch and integration steps"},{"location":"course-description/#required-tools-and-resources","title":"Required Tools and Resources","text":""},{"location":"course-description/#software-skill-libraries-provided-free","title":"Software (Skill Libraries Provided Free)","text":"<ul> <li>Claude Code with MicroSim generation skills ($20/month Claude Pro required)</li> <li>Access to the Claude Code Skill library on GitHub</li> <li>Web browser with developer tools</li> </ul>"},{"location":"course-description/#recommended-background-reading","title":"Recommended Background Reading","text":"<ul> <li>Mayer, R. E. (2009). Multimedia Learning (2nd ed.)</li> <li>Clark, R. C., &amp; Mayer, R. E. (2016). E-Learning and the Science of Instruction</li> <li>Sweller, J. (2011). Cognitive Load Theory</li> </ul>"},{"location":"course-description/#time-commitment","title":"Time Commitment","text":"<ul> <li>Self-paced: Approximately 60-80 hours total</li> <li>Instructor-led: 3-4 hours per week for 12 weeks</li> </ul>"},{"location":"course-description/#expected-outcomes","title":"Expected Outcomes","text":"<p>Graduates of this course will be able to:</p> <ul> <li>Produce MicroSims that effectively support specific learning objectives</li> <li>Evaluate educational simulations systematically using research-based criteria</li> <li>Adapt simulation complexity for any audience from kindergarten to graduate school</li> <li>Collaborate with AI tools effectively in the instructional design process</li> <li>Build and maintain a personal library of reusable educational simulations</li> <li>Apply universal design principles to create accessible learning experiences</li> <li>Conduct basic user testing and iterate based on learner feedback</li> </ul>"},{"location":"course-description/#who-should-not-take-this-course","title":"Who Should Not Take This Course","text":"<p>This course may not be the right fit if you:</p> <ul> <li>Want to learn programming (this course uses AI tools to generate code)</li> <li>Need simulations for entertainment rather than education</li> <li>Are looking for a library of pre-made simulations (this teaches creation)</li> <li>Expect fully automated simulation generation without human judgment</li> </ul>"},{"location":"course-description/#concepts-covered-in-this-course","title":"Concepts Covered in This Course","text":"<p>The following concepts are explicitly taught and practiced:</p>"},{"location":"course-description/#instructional-design-foundations","title":"Instructional Design Foundations","text":"<ul> <li>Learning objective analysis and decomposition</li> <li>Bloom's Taxonomy (2001 revision) and cognitive complexity levels</li> <li>Prerequisite concept identification</li> <li>Simulation readiness assessment</li> <li>Pedagogical alignment</li> </ul>"},{"location":"course-description/#visualization-and-interaction-design","title":"Visualization and Interaction Design","text":"<ul> <li>Visualization paradigm selection (animation, network, timeline, chart, map, diagram)</li> <li>Interaction pattern design (sliders, toggles, drag-and-drop, click events)</li> <li>Progressive disclosure techniques</li> <li>Feedback loop design</li> <li>Scaffolding strategies</li> </ul>"},{"location":"course-description/#cognitive-science-principles","title":"Cognitive Science Principles","text":"<ul> <li>Cognitive load theory (intrinsic, extraneous, germane load)</li> <li>Split attention effect</li> <li>Multimedia learning principles</li> <li>Misconception identification and remediation</li> <li>Mental model formation</li> </ul>"},{"location":"course-description/#specification-and-communication","title":"Specification and Communication","text":"<ul> <li>MicroSim specification document writing</li> <li>Visual element description without code</li> <li>Behavior specification and constraints</li> <li>Success criteria definition</li> <li>Edge case documentation</li> </ul>"},{"location":"course-description/#audience-adaptation","title":"Audience Adaptation","text":"<ul> <li>Cognitive development stages (Piaget, Vygotsky)</li> <li>Age-appropriate complexity calibration</li> <li>Vocabulary level adjustment</li> <li>Cultural sensitivity in design</li> <li>Prior knowledge assessment</li> </ul>"},{"location":"course-description/#quality-assurance","title":"Quality Assurance","text":"<ul> <li>Technical evaluation (functionality, responsiveness, bugs)</li> <li>Pedagogical evaluation (alignment, cognitive level, effectiveness)</li> <li>User experience evaluation (intuitiveness, accessibility, engagement)</li> <li>Rubric development and application</li> <li>Automated vs. human evaluation methods</li> </ul>"},{"location":"course-description/#user-research","title":"User Research","text":"<ul> <li>Think-aloud protocol design and facilitation</li> <li>Observational techniques</li> <li>Feedback interpretation</li> <li>A/B testing fundamentals</li> <li>Ethical considerations in educational research</li> </ul>"},{"location":"course-description/#iteration-and-refinement","title":"Iteration and Refinement","text":"<ul> <li>Design-test-refine cycle management</li> <li>Change prioritization frameworks</li> <li>Version control for educational content</li> <li>Design rationale documentation</li> <li>Scope management</li> </ul>"},{"location":"course-description/#accessibility-and-inclusion","title":"Accessibility and Inclusion","text":"<ul> <li>Universal Design for Learning (UDL) principles</li> <li>Screen reader compatibility</li> <li>Keyboard navigation requirements</li> <li>Color blindness considerations</li> <li>Reduced motion preferences</li> <li>Multilingual support strategies</li> </ul>"},{"location":"course-description/#ai-assisted-development","title":"AI-Assisted Development","text":"<ul> <li>Prompt engineering for MicroSim generation</li> <li>Iterative refinement conversations</li> <li>Output interpretation and issue identification</li> <li>When to regenerate vs. manually adjust</li> <li>Human-AI collaboration patterns</li> </ul>"},{"location":"course-description/#deployment-and-maintenance","title":"Deployment and Maintenance","text":"<ul> <li>LMS integration techniques</li> <li>Analytics implementation</li> <li>Long-term maintenance planning</li> <li>Library organization and reusability</li> <li>Collaboration and sharing practices</li> </ul>"},{"location":"course-description/#concepts-not-covered-in-this-course","title":"Concepts NOT Covered in This Course","text":"<p>The following topics are outside the scope of this course:</p>"},{"location":"course-description/#programming-and-technical-implementation","title":"Programming and Technical Implementation","text":"<ul> <li>JavaScript, HTML, or CSS coding</li> <li>p5.js, Chart.js, or other library APIs</li> <li>Debugging code errors</li> <li>Performance optimization</li> <li>Database design or backend development</li> <li>Version control systems (Git)</li> </ul>"},{"location":"course-description/#advanced-research-methods","title":"Advanced Research Methods","text":"<ul> <li>Statistical analysis of learning outcomes</li> <li>Randomized controlled trial design</li> <li>Psychometric validation</li> <li>Learning analytics at scale</li> <li>Machine learning for adaptive learning</li> </ul>"},{"location":"course-description/#graphic-design","title":"Graphic Design","text":"<ul> <li>Visual design principles (typography, color theory, composition)</li> <li>Icon and illustration creation</li> <li>Animation principles (beyond pedagogical considerations)</li> <li>Brand identity development</li> <li>Professional graphic design software (Photoshop, Illustrator, Figma)</li> </ul>"},{"location":"course-description/#learning-management-system-administration","title":"Learning Management System Administration","text":"<ul> <li>LMS configuration and setup</li> <li>User management and permissions</li> <li>Gradebook configuration</li> <li>Course structure beyond MicroSim embedding</li> <li>SCORM/xAPI technical implementation</li> </ul>"},{"location":"course-description/#content-creation-beyond-microsims","title":"Content Creation Beyond MicroSims","text":"<ul> <li>Video production and editing</li> <li>Podcast or audio content creation</li> <li>Written curriculum development</li> <li>Assessment item writing (beyond simulation-embedded)</li> <li>Gamification and game-based learning design</li> </ul>"},{"location":"course-description/#organizational-change-management","title":"Organizational Change Management","text":"<ul> <li>Institutional adoption strategies</li> <li>Faculty development programs</li> <li>Budget planning and resource allocation</li> <li>Stakeholder management</li> <li>Policy development</li> </ul>"},{"location":"course-description/#advanced-accessibility","title":"Advanced Accessibility","text":"<ul> <li>WCAG 2.1 AA/AAA compliance auditing</li> <li>Assistive technology testing</li> <li>Legal accessibility requirements by jurisdiction</li> <li>Accessibility remediation for legacy content</li> <li>Sign language interpretation</li> </ul>"},{"location":"course-description/#specialized-domains","title":"Specialized Domains","text":"<ul> <li>Simulation for high-stakes training (medical, aviation, military)</li> <li>Virtual reality or augmented reality development</li> <li>Hardware-integrated simulations</li> <li>Real-time multiplayer educational experiences</li> <li>Adaptive learning system design</li> </ul>"},{"location":"course-description/#learning-objectives-by-blooms-taxonomy-2001-revision","title":"Learning Objectives by Bloom's Taxonomy (2001 Revision)","text":"<p>The 2001 revision of Bloom's Taxonomy by Anderson and Krathwohl reorganized cognitive processes into six levels, using verbs rather than nouns, with \"Create\" as the highest level. Below are this course's learning objectives organized by taxonomy level.</p>"},{"location":"course-description/#level-1-remember","title":"Level 1: Remember","text":"<p>Retrieving relevant knowledge from long-term memory</p> <ul> <li>List the six cognitive levels of Bloom's Taxonomy in order</li> <li>Identify the seven primary visualization paradigms used in MicroSim development</li> <li>Recall the three components of cognitive load theory</li> <li>Name the three lenses of MicroSim evaluation (technical, pedagogical, user experience)</li> <li>Define key terms: learning objective, MicroSim, specification, scaffolding, progressive disclosure</li> <li>Recognize common misconception patterns in educational simulations</li> </ul>"},{"location":"course-description/#level-2-understand","title":"Level 2: Understand","text":"<p>Constructing meaning from instructional messages</p> <ul> <li>Explain why converting learning objectives to simulations is challenging</li> <li>Describe how cognitive development stages affect MicroSim design choices</li> <li>Summarize the relationship between cognitive load and visual complexity</li> <li>Interpret user testing feedback to identify design issues</li> <li>Classify learning objectives by Bloom's Taxonomy level</li> <li>Compare different visualization paradigms and their pedagogical affordances</li> <li>Exemplify how the same concept can be adapted for different audience levels</li> <li>Paraphrase specification requirements into natural language descriptions</li> </ul>"},{"location":"course-description/#level-3-apply","title":"Level 3: Apply","text":"<p>Carrying out or using a procedure in a given situation</p> <ul> <li>Implement a complete MicroSim specification document</li> <li>Execute a think-aloud user testing session</li> <li>Use the microsim-generator skill to produce working simulations</li> <li>Apply cognitive load principles to evaluate an existing MicroSim</li> <li>Demonstrate audience adaptation by modifying a MicroSim for a different age group</li> <li>Employ an evaluation rubric to assess simulation quality</li> <li>Carry out an accessibility audit on a MicroSim</li> </ul>"},{"location":"course-description/#level-4-analyze","title":"Level 4: Analyze","text":"<p>Breaking material into constituent parts and detecting relationships</p> <ul> <li>Differentiate between intrinsic, extraneous, and germane cognitive load in a simulation</li> <li>Organize learning objectives by prerequisite dependencies</li> <li>Attribute MicroSim failures to specific design decisions</li> <li>Deconstruct a complex learning objective into simulation-ready components</li> <li>Distinguish between misconception-reinforcing and misconception-correcting designs</li> <li>Examine user testing data to identify patterns across participants</li> <li>Compare multiple MicroSim approaches for the same learning objective</li> <li>Outline the decision points in the MicroSim generation workflow</li> </ul>"},{"location":"course-description/#level-5-evaluate","title":"Level 5: Evaluate","text":"<p>Making judgments based on criteria and standards</p> <ul> <li>Assess whether a MicroSim effectively targets its stated learning objective</li> <li>Critique specification documents for completeness and clarity</li> <li>Judge the appropriateness of a visualization paradigm for a given concept</li> <li>Prioritize iteration changes based on impact and effort</li> <li>Justify design decisions using pedagogical principles</li> <li>Appraise the accessibility of a MicroSim against UDL guidelines</li> <li>Defend audience adaptation choices with cognitive development theory</li> <li>Recommend whether to regenerate or manually refine a MicroSim</li> </ul>"},{"location":"course-description/#level-6-create","title":"Level 6: Create","text":"<p>Putting elements together to form a novel, coherent whole</p> <ul> <li>Design a complete MicroSim specification from a learning objective</li> <li>Construct an evaluation rubric tailored to a specific subject area</li> <li>Develop a user testing protocol for a target learner population</li> <li>Compose iterative refinement prompts that preserve pedagogical intent</li> <li>Produce a MicroSim that addresses a documented misconception</li> <li>Generate multiple MicroSim variations for different audience levels</li> <li>Formulate a maintenance and update plan for deployed simulations</li> <li>Assemble a personal MicroSim library with consistent documentation</li> <li>Synthesize feedback from multiple sources into prioritized design changes</li> <li>Author a complete portfolio demonstrating mastery of the MicroSim development lifecycle</li> </ul>"},{"location":"course-description/#getting-started","title":"Getting Started","text":"<p>To begin this course:</p> <ol> <li>Ensure you have access to Claude Code with MicroSim skills enabled</li> <li>Identify 5-10 learning objectives from your teaching context to use as practice material</li> <li>Complete the Module 1 readings on Bloom's Taxonomy</li> <li>Begin the learning objective analysis activity</li> </ol> <p>Welcome to the future of instructional design! - Dan McCreary</p>"},{"location":"faq/","title":"Automating Instructional Design FAQ","text":"<p>This FAQ addresses common questions about the Automating Instructional Design course, covering everything from getting started to advanced topics.</p>"},{"location":"faq/#getting-started","title":"Getting Started","text":""},{"location":"faq/#what-is-this-course-about","title":"What is this course about?","text":"<p>This course teaches educators and training professionals how to transform learning objectives into interactive educational simulations called MicroSims using AI-assisted tools. You'll master the complete lifecycle of MicroSim development\u2014from analyzing pedagogical goals through deployment and learner assessment\u2014using Claude Code skills and visualization libraries.</p> <p>The central challenge addressed is bridging the semantic gap between abstract learning outcomes and concrete interactive experiences. A learning objective like \"Students will understand the relationship between supply and demand\" contains implicit assumptions about visualization, interaction, and assessment that must be made explicit before any simulation can be built.</p> <p>Example: Converting the abstract concept of \"exponential growth\" into an interactive simulation where learners adjust growth rates and see real-time population curves.</p>"},{"location":"faq/#who-is-this-course-designed-for","title":"Who is this course designed for?","text":"<p>This course is designed for working professionals who create educational content:</p> <ul> <li>K-12 teachers seeking interactive curriculum supplements</li> <li>Corporate training specialists developing employee programs</li> <li>Higher education faculty creating course simulations</li> <li>Instructional designers at educational technology companies</li> <li>Curriculum developers building standards-aligned materials</li> <li>Subject matter experts making their knowledge more accessible</li> </ul> <p>No programming background is required. The course teaches you to work with AI tools that generate code, focusing on specification, evaluation, and iteration rather than implementation.</p>"},{"location":"faq/#what-prerequisites-do-i-need","title":"What prerequisites do I need?","text":"<p>You need only basic computer literacy\u2014no programming experience required. The course teaches you to specify what simulations should do, not how to code them. You should be comfortable with:</p> <ul> <li>Using web browsers and basic applications</li> <li>Writing clear, descriptive text</li> <li>Thinking systematically about learning goals</li> </ul> <p>If you can write a detailed lesson plan, you have the skills to succeed in this course.</p>"},{"location":"faq/#how-long-does-this-course-take-to-complete","title":"How long does this course take to complete?","text":"<p>The course offers two completion paths:</p> <ul> <li>Self-paced: Approximately 60-80 hours total, progressing through 12 modules at your own speed</li> <li>Instructor-led: 3-4 hours per week for 12 weeks, with guided activities and peer feedback</li> </ul> <p>Each module includes readings, hands-on activities, and a MicroSim project, so actual time varies based on the complexity of your subject area.</p>"},{"location":"faq/#what-tools-and-software-do-i-need","title":"What tools and software do I need?","text":"<p>All required tools are provided:</p> <ul> <li>Claude Code with MicroSim generation skills</li> <li>Access to the MicroSim template library</li> <li>A modern web browser with developer tools</li> </ul> <p>You'll also benefit from having 5-10 learning objectives from your own teaching context to use as practice material throughout the course.</p>"},{"location":"faq/#how-is-this-different-from-learning-to-code","title":"How is this different from learning to code?","text":"<p>This course explicitly does not teach programming. Instead, you learn to:</p> <ul> <li>Specify what simulations should look like and how they should behave</li> <li>Evaluate whether generated simulations meet learning objectives</li> <li>Iterate on designs through structured refinement conversations with AI</li> </ul> <p>The instructional designer's role shifts from implementation to specification and quality assurance. You describe what you want; AI generates the code.</p>"},{"location":"faq/#what-will-i-be-able-to-do-after-completing-this-course","title":"What will I be able to do after completing this course?","text":"<p>Graduates can:</p> <ul> <li>Produce MicroSims that effectively support specific learning objectives</li> <li>Evaluate educational simulations using research-based criteria</li> <li>Adapt simulation complexity for any audience from kindergarten to graduate school</li> <li>Collaborate with AI tools effectively in instructional design</li> <li>Build and maintain a personal library of reusable simulations</li> <li>Apply universal design principles for accessible learning experiences</li> <li>Conduct user testing and iterate based on learner feedback</li> </ul>"},{"location":"faq/#how-do-i-access-the-course-materials","title":"How do I access the course materials?","text":"<p>The course is delivered as an intelligent textbook hosted at https://dmccreary.github.io/automating-instructional-design/. All 12 modules, practice activities, and example MicroSims are freely accessible. You'll need Claude Code access for the hands-on generation activities.</p>"},{"location":"faq/#what-is-a-microsim","title":"What is a MicroSim?","text":"<p>A MicroSim (Micro Simulation) is a small, focused interactive simulation designed to teach a specific concept or skill through exploration and visualization. Unlike comprehensive simulations that model entire systems, MicroSims target atomic concepts that can be understood in 5-15 minutes of interaction.</p> <p>Example: An interactive supply-and-demand curve where learners adjust price and quantity sliders to see equilibrium points change in real-time.</p>"},{"location":"faq/#what-makes-a-learning-objective-simulation-ready","title":"What makes a learning objective \"simulation-ready\"?","text":"<p>Not all learning objectives benefit from interactive simulation. Simulation-ready objectives typically involve:</p> <ul> <li>Dynamic relationships that change over time</li> <li>Multiple variables that interact with each other</li> <li>Abstract concepts that benefit from visualization</li> <li>Cause-effect relationships that learners can explore</li> <li>Common misconceptions that can be revealed through exploration</li> </ul> <p>Objectives focused purely on factual recall or procedural steps may be better served by other instructional methods.</p>"},{"location":"faq/#can-i-use-microsims-i-create-in-my-own-courses","title":"Can I use MicroSims I create in my own courses?","text":"<p>Yes. All MicroSims you create during this course are yours to use in your teaching. The course also teaches you how to deploy MicroSims to learning management systems, embed them in course materials, and share them with other educators under appropriate licenses.</p>"},{"location":"faq/#core-concepts","title":"Core Concepts","text":""},{"location":"faq/#what-is-blooms-taxonomy-and-why-does-it-matter","title":"What is Bloom's Taxonomy and why does it matter?","text":"<p>Bloom's Taxonomy is a hierarchical classification system for cognitive complexity levels. The 2001 revision by Anderson and Krathwohl identifies six levels, using verbs to emphasize active learning:</p> <ol> <li>Remember \u2014 Retrieve facts from memory</li> <li>Understand \u2014 Construct meaning from information</li> <li>Apply \u2014 Use procedures in given situations</li> <li>Analyze \u2014 Break material into parts and detect relationships</li> <li>Evaluate \u2014 Make judgments based on criteria</li> <li>Create \u2014 Put elements together to form novel wholes</li> </ol> <p>This matters for MicroSim design because different cognitive levels require different types of interactions. A \"Remember\" objective might need a matching exercise; a \"Create\" objective needs an open-ended design tool.</p> <p>Example: An objective at the Apply level\u2014\"Calculate standard deviation for a data set\"\u2014requires an interactive calculator simulation, while an Analyze objective\u2014\"Compare sorting algorithms\"\u2014needs side-by-side visualization.</p>"},{"location":"faq/#how-do-i-decompose-compound-learning-objectives","title":"How do I decompose compound learning objectives?","text":"<p>Compound objectives combine multiple skills that should be taught and assessed separately. To decompose them:</p> <ol> <li>Identify action verbs \u2014 Each verb represents a separate skill</li> <li>List prerequisite concepts \u2014 What must learners know first?</li> <li>Map dependencies \u2014 Which skills build on others?</li> <li>Create atomic objectives \u2014 One measurable skill per statement</li> </ol> <p>Example: \"Design and implement a RESTful API with authentication\" becomes: - Explain REST principles (Understand) - Design API endpoint structure (Create) - Implement basic API endpoints (Apply) - Explain authentication methods (Understand) - Implement authentication (Apply) - Integrate authentication with API (Apply)</p>"},{"location":"faq/#what-are-the-main-visualization-paradigms-for-microsims","title":"What are the main visualization paradigms for MicroSims?","text":"<p>The course covers seven primary visualization paradigms:</p> Paradigm Best For Library Animation Motion, physics, dynamic systems, cause-effect p5.js Network Relationships, hierarchies, dependencies vis-network Timeline Sequences, history, processes vis-timeline Chart Comparisons, trends, distributions Chart.js, Plotly Map Geography, spatial relationships Leaflet Diagram Flowcharts, state machines Mermaid Set Classification, overlap, logic Venn.js <p>Choosing the right paradigm depends on the concept's characteristics\u2014whether it involves change over time, relationships between entities, or categorical membership.</p>"},{"location":"faq/#what-is-cognitive-load-theory","title":"What is Cognitive Load Theory?","text":"<p>Cognitive Load Theory explains how the cognitive demands of instruction affect learning, based on working memory limitations. It identifies three types of load:</p> <ul> <li>Intrinsic load \u2014 The inherent complexity of the material itself</li> <li>Extraneous load \u2014 Wasted effort from poor instructional design</li> <li>Germane load \u2014 Productive effort building mental schemas</li> </ul> <p>Good MicroSim design minimizes extraneous load (reducing visual clutter, avoiding split attention) while maximizing germane load (engaging learners in meaningful processing). Intrinsic load is managed through sequencing and scaffolding.</p> <p>Example: A cluttered interface with labels far from the elements they describe creates extraneous load. Moving labels directly onto elements reduces split attention and frees cognitive resources for learning.</p>"},{"location":"faq/#what-is-the-split-attention-effect","title":"What is the \"split attention effect\"?","text":"<p>The split attention effect occurs when learners must mentally integrate information from physically or temporally separated sources. This increases cognitive load and impairs learning.</p> <p>In MicroSims, split attention happens when: - Labels are placed in legends instead of on diagram elements - Instructions appear in one location while actions happen in another - Text explanations are separated from the visualizations they describe</p> <p>Solution: Integrate related information spatially. Place labels directly on elements. Show instructions adjacent to where actions occur.</p>"},{"location":"faq/#how-do-i-identify-concepts-that-are-simulation-ready","title":"How do I identify concepts that are \"simulation-ready\"?","text":"<p>Simulation-ready concepts share these characteristics:</p> <ol> <li>Dynamic behavior \u2014 The concept involves change or process</li> <li>Controllable variables \u2014 Learners can manipulate inputs</li> <li>Observable outcomes \u2014 Changes produce visible results</li> <li>Exploration value \u2014 Trying different values reveals insights</li> <li>Misconception potential \u2014 Incorrect intuitions can be tested</li> </ol> <p>Not simulation-ready: Memorizing vocabulary, following fixed procedures, understanding static facts.</p> <p>Simulation-ready: Exploring how interest rates affect loan payments, testing how mass affects acceleration, discovering equilibrium in supply-demand.</p>"},{"location":"faq/#what-is-a-learning-graph","title":"What is a learning graph?","text":"<p>A learning graph is a directed graph where nodes represent concepts and edges represent prerequisite dependencies. If concept B depends on concept A, there's an edge from A to B.</p> <p>Learning graphs help instructional designers: - Sequence content appropriately - Identify prerequisite gaps - Find bottleneck concepts (many dependencies) - Create personalized learning paths</p> <p>The course's learning graph contains 200 concepts across 12 categories, from Foundation Concepts to Capstone activities.</p>"},{"location":"faq/#how-does-progressive-disclosure-work-in-microsims","title":"How does progressive disclosure work in MicroSims?","text":"<p>Progressive disclosure reveals information and complexity gradually rather than all at once. This reduces initial cognitive load while allowing access to advanced features when learners are ready.</p> <p>Implementation strategies: - Start with simple defaults, offer \"advanced options\" toggle - Unlock features after learners demonstrate mastery - Use layered interfaces (basic \u2192 intermediate \u2192 advanced views) - Provide optional hints that can be expanded</p> <p>Example: A physics simulation starts with only mass and velocity controls. After learners explore basic motion, air resistance and friction options become available.</p>"},{"location":"faq/#what-is-the-difference-between-intrinsic-and-extraneous-cognitive-load","title":"What is the difference between intrinsic and extraneous cognitive load?","text":"<p>Intrinsic load is determined by: - The inherent complexity of the material - Element interactivity (how many elements must be processed simultaneously) - Learner's prior knowledge</p> <p>You can't eliminate intrinsic load\u2014calculus is inherently more complex than addition. But you can manage it through sequencing and prerequisite building.</p> <p>Extraneous load is caused by: - Poor instructional design - Visual clutter - Split attention - Unnecessary complexity in the interface</p> <p>Extraneous load wastes cognitive resources and should always be minimized.</p>"},{"location":"faq/#what-are-action-verbs-and-why-are-they-important","title":"What are action verbs and why are they important?","text":"<p>Action verbs in learning objectives describe observable, measurable behaviors that demonstrate learning. They're important because:</p> <ul> <li>They make objectives assessable</li> <li>They align with specific Bloom's Taxonomy levels</li> <li>They guide the design of appropriate activities and assessments</li> </ul> <p>Weak verbs: Understand, appreciate, know, learn, become familiar with</p> <p>Strong verbs: Calculate, compare, design, evaluate, classify, construct, demonstrate</p> <p>Example: \"Students will understand photosynthesis\" is vague. \"Students will diagram the inputs and outputs of photosynthesis\" is measurable.</p>"},{"location":"faq/#how-do-misconceptions-affect-microsim-design","title":"How do misconceptions affect MicroSim design?","text":"<p>Misconceptions are incorrect beliefs that learners bring to instruction. They matter for MicroSim design because:</p> <ul> <li>Simulations can inadvertently reinforce misconceptions if designed poorly</li> <li>Well-designed simulations can reveal and correct misconceptions</li> <li>Prediction prompts make learner thinking visible</li> </ul> <p>Design strategy: Include edge cases and scenarios where intuitive but incorrect beliefs produce surprising results. Let learners predict outcomes before running simulations.</p> <p>Example: Many students believe heavier objects fall faster. A MicroSim that only shows objects falling in sequence might reinforce this. A well-designed MicroSim lets learners adjust mass and see that acceleration is constant (in vacuum), directly addressing the misconception.</p>"},{"location":"faq/#what-is-a-specification-document","title":"What is a specification document?","text":"<p>A specification document is a detailed written description of what a MicroSim should contain, how it should behave, and what it should teach. It serves as the \"blueprint\" that AI uses to generate code.</p> <p>A complete specification includes: - Learning objective and target audience - Visual layout and elements - Interaction behaviors and constraints - Parameter ranges and default values - Edge cases and error handling - Success criteria</p> <p>The quality of your specification directly determines the quality of the generated MicroSim.</p>"},{"location":"faq/#what-is-intent-preservation-in-ai-assisted-design","title":"What is \"intent preservation\" in AI-assisted design?","text":"<p>Intent preservation means ensuring that your pedagogical purpose survives the translation from specification to generated output. AI systems may interpret instructions differently than intended.</p> <p>To preserve intent: - Be specific rather than vague (\"blue circle, 50px diameter\" not \"colorful shape\") - Include concrete examples - Specify what should NOT happen - Review generated output against original objectives - Iterate with refinement prompts when needed</p>"},{"location":"faq/#what-is-the-difference-between-formative-and-summative-assessment","title":"What is the difference between formative and summative assessment?","text":"<p>Formative assessment occurs during learning to provide feedback and guide improvement. In this course: - Module reflection journals - Peer feedback on specifications - Self-evaluation using rubrics</p> <p>Summative assessment evaluates learning at the end. In this course: - Portfolio project demonstrating complete MicroSim development cycle - Eight deliverables evaluated against published criteria</p> <p>MicroSims can support both\u2014embedded formative feedback during interaction, and data collection for summative evaluation.</p>"},{"location":"faq/#technical-details","title":"Technical Details","text":""},{"location":"faq/#what-javascript-libraries-are-used-for-microsims","title":"What JavaScript libraries are used for MicroSims?","text":"<p>The course covers seven primary libraries:</p> Library Purpose Strengths p5.js Animations, physics, creative coding Easy learning curve, excellent for motion vis-network Network graphs, dependencies Interactive node-edge visualization vis-timeline Timelines, scheduling Range-based temporal data Chart.js Charts, graphs Clean defaults, responsive Plotly Advanced charts, 3D Publication quality, scientific Leaflet Maps, geography Tile-based, mobile-friendly Mermaid Diagrams from text Flowcharts, sequence diagrams <p>You don't need to learn these libraries\u2014the AI generates appropriate code. But understanding their capabilities helps you write better specifications.</p>"},{"location":"faq/#how-do-i-choose-between-chartjs-and-plotly","title":"How do I choose between Chart.js and Plotly?","text":"<p>Choose Chart.js when: - You need standard chart types (bar, line, pie, scatter) - Clean, responsive defaults matter - The audience is general</p> <p>Choose Plotly when: - You need 3D visualizations - Scientific/publication quality is required - You need advanced interactivity (zoom, pan, hover details) - The audience is technical/research-oriented</p> <p>For most educational contexts, Chart.js is sufficient and simpler.</p>"},{"location":"faq/#what-makes-p5js-good-for-educational-animations","title":"What makes p5.js good for educational animations?","text":"<p>p5.js excels at educational animations because:</p> <ul> <li>Immediate mode rendering \u2014 Easy to create frame-by-frame animations</li> <li>Simple coordinate system \u2014 Intuitive x/y positioning</li> <li>Built-in physics helpers \u2014 Vectors, forces, collisions</li> <li>Interactive input \u2014 Mouse, touch, keyboard events</li> <li>Setup/draw pattern \u2014 Clear separation of initialization and animation</li> </ul> <p>Example uses: Projectile motion, wave interference, particle systems, sorting algorithm visualization.</p>"},{"location":"faq/#when-should-i-use-vis-network-vs-mermaid-for-diagrams","title":"When should I use vis-network vs. Mermaid for diagrams?","text":"<p>Use vis-network when: - Users need to interact with the graph (drag, zoom, click) - The graph structure changes dynamically - You want physics-based layouts - Relationships are the main focus</p> <p>Use Mermaid when: - You need static diagram export - The diagram follows standard patterns (flowchart, sequence, state) - Text-based source is important - Quick prototyping is needed</p>"},{"location":"faq/#how-do-i-make-microsims-responsive-to-different-screen-sizes","title":"How do I make MicroSims responsive to different screen sizes?","text":"<p>Responsive design ensures MicroSims work on desktops, tablets, and phones. Key techniques:</p> <ol> <li>Use relative sizing \u2014 Percentages and viewport units, not fixed pixels</li> <li>Call updateCanvasSize() \u2014 Always first in p5.js setup() function</li> <li>Test at multiple sizes \u2014 Desktop, tablet, phone orientations</li> <li>Simplify on small screens \u2014 Hide optional elements, enlarge touch targets</li> <li>Respect aspect ratios \u2014 Some visualizations need minimum dimensions</li> </ol> <p>The MicroSim generator skill handles most responsiveness automatically, but you should test generated output.</p>"},{"location":"faq/#what-is-the-structure-of-a-microsim-file-package","title":"What is the structure of a MicroSim file package?","text":"<p>Each MicroSim in the template library contains:</p> <pre><code>docs/sims/[sim-name]/\n\u251c\u2500\u2500 index.md          # MkDocs page embedding the sim\n\u251c\u2500\u2500 main.html         # Standalone HTML file\n\u251c\u2500\u2500 [sim-name].js     # JavaScript implementation\n\u2514\u2500\u2500 local.css         # Scoped styles\n</code></pre> <p>The index.md file embeds the simulation using an iframe. The main.html file can be used standalone or for testing.</p>"},{"location":"faq/#how-do-i-embed-a-microsim-in-mkdocs","title":"How do I embed a MicroSim in MkDocs?","text":"<p>Add an iframe to your markdown file:</p> <pre><code>&lt;iframe src=\"./main.html\" width=\"100%\" height=\"450px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>Key points: - Never use a style attribute on iframe elements - Always add <code>scrolling=\"no\"</code> - Use relative paths for portability - Test at the full local URL including repository name</p>"},{"location":"faq/#what-file-formats-does-the-microsim-generator-produce","title":"What file formats does the MicroSim generator produce?","text":"<p>The microsim-generator skill produces:</p> <ul> <li>HTML \u2014 Complete standalone file with embedded CSS/JS</li> <li>JavaScript \u2014 Separate .js file for larger simulations</li> <li>CSS \u2014 Scoped styles in local.css</li> <li>Markdown \u2014 index.md for MkDocs embedding</li> <li>Metadata \u2014 Optional JSON with learning objective and parameters</li> </ul>"},{"location":"faq/#how-do-i-test-microsims-locally","title":"How do I test MicroSims locally?","text":"<p>Run the MkDocs development server:</p> <pre><code>mkdocs serve\n</code></pre> <p>Access at: <code>http://127.0.0.1:8000/automating-instructional-design/</code></p> <p>Navigate to your simulation's page. The iframe will load the main.html file. Use browser developer tools (F12) to debug JavaScript issues.</p>"},{"location":"faq/#what-is-version-control-for-microsims","title":"What is version control for MicroSims?","text":"<p>Version control tracks changes to MicroSim files over time, enabling:</p> <ul> <li>History \u2014 See what changed and when</li> <li>Rollback \u2014 Revert problematic changes</li> <li>Collaboration \u2014 Multiple designers working together</li> <li>Documentation \u2014 Commit messages explain changes</li> </ul> <p>While the course doesn't teach Git in detail, it introduces version control concepts for maintaining MicroSim libraries.</p>"},{"location":"faq/#common-challenges","title":"Common Challenges","text":""},{"location":"faq/#my-specification-is-producing-unexpected-results-what-should-i-check","title":"My specification is producing unexpected results. What should I check?","text":"<p>Common specification issues:</p> <ol> <li>Ambiguity \u2014 \"Make it interactive\" vs. \"Add a slider controlling temperature from 0-100\u00b0C\"</li> <li>Missing defaults \u2014 What values should parameters start at?</li> <li>Undefined edge cases \u2014 What happens at minimum/maximum values?</li> <li>Implicit assumptions \u2014 Color meanings, units, scales</li> <li>Conflicting requirements \u2014 \"Simple but comprehensive\"</li> </ol> <p>Solution: Write specifications as if explaining to someone who has never seen your subject area. Include concrete examples and explicit defaults.</p>"},{"location":"faq/#the-generated-microsim-doesnt-match-my-learning-objective-how-do-i-fix-this","title":"The generated MicroSim doesn't match my learning objective. How do I fix this?","text":"<p>This is a pedagogical alignment issue. Check:</p> <ol> <li>Objective clarity \u2014 Is your learning objective specific and measurable?</li> <li>Interaction match \u2014 Does the interaction actually teach the skill?</li> <li>Cognitive level \u2014 Does the simulation require the right level of thinking?</li> <li>Feedback presence \u2014 Does the learner know when they've succeeded?</li> </ol> <p>Solution: Use refinement prompts to adjust. \"The current simulation shows X, but learners need to practice Y. Change the interaction so that...\"</p>"},{"location":"faq/#my-microsim-works-on-desktop-but-not-mobile-whats-wrong","title":"My MicroSim works on desktop but not mobile. What's wrong?","text":"<p>Common mobile issues:</p> <ol> <li>Touch targets too small \u2014 Minimum 44x44 pixels</li> <li>Hover-dependent features \u2014 No hover on touchscreens</li> <li>Fixed pixel sizes \u2014 Use responsive units</li> <li>Landscape assumptions \u2014 Test portrait orientation</li> <li>Performance \u2014 Complex animations may lag</li> </ol> <p>Solution: Test early on mobile devices. Specify mobile requirements in your specification. Use progressive disclosure to hide complexity on small screens.</p>"},{"location":"faq/#learners-are-confused-by-my-microsim-how-do-i-simplify-it","title":"Learners are confused by my MicroSim. How do I simplify it?","text":"<p>Signs of cognitive overload:</p> <ul> <li>Learners don't know where to start</li> <li>They miss important features</li> <li>They make random changes without purpose</li> <li>They give up quickly</li> </ul> <p>Simplification strategies: 1. Reduce visible controls (progressive disclosure) 2. Add guided scenarios or tutorials 3. Increase visual contrast for important elements 4. Remove decorative elements 5. Add explicit instructions near interaction points</p>"},{"location":"faq/#my-microsim-reinforces-a-misconception-instead-of-correcting-it-what-happened","title":"My MicroSim reinforces a misconception instead of correcting it. What happened?","text":"<p>Misconception reinforcement occurs when:</p> <ul> <li>The simulation only shows expected cases</li> <li>Edge cases where intuition fails are missing</li> <li>Learners never see their predictions contradicted</li> <li>The visualization matches the misconception</li> </ul> <p>Solution: Include prediction prompts before revealing outcomes. Add edge cases that expose incorrect thinking. Explicitly contrast correct and incorrect models.</p> <p>Example: A seasons simulation that only shows Earth orbiting the sun might reinforce \"distance causes seasons.\" Add the ability to compare Northern and Southern hemispheres simultaneously to reveal the axial tilt explanation.</p>"},{"location":"faq/#the-ai-keeps-generating-similar-but-incorrect-code-how-do-i-break-the-pattern","title":"The AI keeps generating similar but incorrect code. How do I break the pattern?","text":"<p>When iterations aren't improving:</p> <ol> <li>Change approach \u2014 Describe the problem differently</li> <li>Provide examples \u2014 Show what correct behavior looks like</li> <li>Be more specific \u2014 Add constraints and requirements</li> <li>Start fresh \u2014 Sometimes regenerating from scratch helps</li> <li>Manual adjustment \u2014 For small fixes, edit code directly</li> </ol> <p>Example prompt: \"The previous attempts had [specific problem]. Instead of [incorrect approach], use [correct approach]. Here's an example of what the output should look like: [example].\"</p>"},{"location":"faq/#how-do-i-handle-parameter-edge-cases","title":"How do I handle parameter edge cases?","text":"<p>Edge cases are unusual or extreme parameter values. Common issues:</p> <ul> <li>Division by zero</li> <li>Negative values where only positive make sense</li> <li>Values that break the visualization scale</li> <li>Combinations that produce undefined behavior</li> </ul> <p>Specification approach: Explicitly define behavior at boundaries. \"When temperature reaches 0\u00b0C, display ice formation. When approaching 100\u00b0C, show steam bubbles. Do not allow negative temperatures.\"</p>"},{"location":"faq/#my-microsim-is-too-slow-how-do-i-improve-performance","title":"My MicroSim is too slow. How do I improve performance?","text":"<p>Performance issues in MicroSims:</p> <ol> <li>Too many animated elements \u2014 Reduce particle count, simplify physics</li> <li>Unnecessary redraws \u2014 Only redraw when state changes</li> <li>Large images \u2014 Compress or reduce resolution</li> <li>Complex calculations per frame \u2014 Cache computed values</li> </ol> <p>For AI generation, specify performance constraints: \"Animation must maintain 30fps on mobile devices. Limit to 100 particles maximum.\"</p>"},{"location":"faq/#best-practices","title":"Best Practices","text":""},{"location":"faq/#how-do-i-write-effective-microsim-specifications","title":"How do I write effective MicroSim specifications?","text":"<p>An effective specification includes:</p> <ol> <li>Clear learning objective \u2014 Specific, measurable, appropriate level</li> <li>Target audience \u2014 Age, prior knowledge, context</li> <li>Visual description \u2014 Layout, colors, elements, sizes</li> <li>Interaction behaviors \u2014 What happens when users act</li> <li>Parameter ranges \u2014 Minimum, maximum, default values</li> <li>Success criteria \u2014 How do we know it works?</li> <li>Edge cases \u2014 Unusual situations and how to handle them</li> </ol> <p>Template: \"Create a MicroSim that demonstrates [concept] for [audience]. The visualization should show [visual elements]. Users can [interactions]. Parameters include [ranges]. Success means [criteria].\"</p>"},{"location":"faq/#whats-the-right-level-of-complexity-for-different-audiences","title":"What's the right level of complexity for different audiences?","text":"Audience Complexity Guidelines Early Childhood (K-2) Large touch targets, simple cause-effect, minimal text, 1-2 controls Elementary (3-5) Guided exploration, reading support, 2-3 controls, scaffolded Middle School (6-8) Abstract concepts, multiple variables, hypothesis testing High School (9-12) Real-world data, edge cases, more parameters Undergraduate Theoretical foundations, mathematical relationships Graduate Research applications, parameter space exploration Corporate Job-relevant scenarios, time-efficient, immediately applicable"},{"location":"faq/#how-do-i-balance-engagement-and-learning","title":"How do I balance engagement and learning?","text":"<p>Engagement should focus attention on learning, not distract from it.</p> <p>Good engagement: - Interactive elements that reveal concept relationships - Immediate feedback on learner actions - Scenarios that connect to learner interests - Appropriate challenge level</p> <p>Bad engagement: - Decorative animations unrelated to content - Game elements that overshadow learning - Complexity for its own sake - Features that entertain but don't teach</p>"},{"location":"faq/#when-should-i-use-prediction-prompts","title":"When should I use prediction prompts?","text":"<p>Prediction prompts are most valuable when:</p> <ul> <li>Common misconceptions exist</li> <li>Intuition often fails</li> <li>The correct answer is surprising</li> <li>Making predictions activates prior knowledge</li> <li>You want to reveal learner thinking</li> </ul> <p>Implementation: \"Before running the simulation, pause and display: 'What do you predict will happen when [parameter] changes?' After prediction, show actual results with comparison to prediction.\"</p>"},{"location":"faq/#how-do-i-design-for-accessibility","title":"How do I design for accessibility?","text":"<p>Accessibility ensures all learners can use your MicroSims:</p> <ol> <li>Screen reader support \u2014 Semantic HTML, alt text, ARIA labels</li> <li>Keyboard navigation \u2014 All controls accessible via keyboard</li> <li>Color independence \u2014 Don't rely solely on color to convey meaning</li> <li>Reduced motion \u2014 Respect <code>prefers-reduced-motion</code> setting</li> <li>Sufficient contrast \u2014 4.5:1 ratio for text, 3:1 for large text</li> <li>Touch targets \u2014 Minimum 44x44 pixels</li> </ol>"},{"location":"faq/#how-do-i-evaluate-if-a-microsim-is-effective","title":"How do I evaluate if a MicroSim is effective?","text":"<p>Use the three-lens evaluation model:</p> <p>Technical Lens: - Does it work without errors? - Is it responsive across devices? - Are there bugs or broken features?</p> <p>Pedagogical Lens: - Does it target the stated learning objective? - Is the cognitive level appropriate? - Do interactions support understanding?</p> <p>User Experience Lens: - Is it intuitive to use? - Is it accessible to all learners? - Is engagement appropriate (not distracting)?</p>"},{"location":"faq/#what-makes-a-good-user-testing-session","title":"What makes a good user testing session?","text":"<p>Effective user testing includes:</p> <ol> <li>Clear goals \u2014 What questions are you trying to answer?</li> <li>Representative users \u2014 Match your target audience</li> <li>Think-aloud protocol \u2014 Ask users to verbalize their thoughts</li> <li>Observation focus \u2014 Watch what they do, not just what they say</li> <li>Non-leading questions \u2014 \"What are you trying to do?\" not \"Did you find it easy?\"</li> <li>Ethical considerations \u2014 Informed consent, especially with minors</li> </ol>"},{"location":"faq/#how-do-i-prioritize-changes-after-user-testing","title":"How do I prioritize changes after user testing?","text":"<p>Categorize findings:</p> Priority Criteria Action Critical Blocks learning, causes errors Fix immediately High Confuses most users, misaligns with objective Fix before release Medium Affects some users, reduces effectiveness Fix in next iteration Low Nice to have, edge cases Defer or document <p>Focus on patterns across multiple users rather than individual preferences.</p>"},{"location":"faq/#how-do-i-maintain-microsims-over-time","title":"How do I maintain MicroSims over time?","text":"<p>Maintenance planning includes:</p> <ol> <li>Regular testing \u2014 Check functionality quarterly</li> <li>Library updates \u2014 JavaScript libraries need version updates</li> <li>Content currency \u2014 Update data and examples as needed</li> <li>Feedback monitoring \u2014 Collect and respond to user reports</li> <li>Documentation \u2014 Record changes and rationale</li> <li>Backup \u2014 Version control and backup storage</li> </ol>"},{"location":"faq/#advanced-topics","title":"Advanced Topics","text":""},{"location":"faq/#how-do-i-create-microsims-that-adapt-to-learner-performance","title":"How do I create MicroSims that adapt to learner performance?","text":"<p>Adaptive MicroSims adjust difficulty based on learner success:</p> <ol> <li>Track performance \u2014 Monitor correct/incorrect responses</li> <li>Define difficulty levels \u2014 What changes between levels?</li> <li>Set thresholds \u2014 When to increase/decrease difficulty</li> <li>Provide feedback \u2014 Let learners know about adjustments</li> <li>Allow override \u2014 Let learners choose their level</li> </ol> <p>Example: A math simulation starts with single-digit multiplication. After 5 correct answers, it introduces two-digit numbers. After 3 consecutive errors, it returns to simpler problems with hints.</p>"},{"location":"faq/#how-can-i-use-microsims-for-assessment","title":"How can I use MicroSims for assessment?","text":"<p>MicroSims can support assessment through:</p> <ul> <li>Embedded questions \u2014 Predict-then-test interactions</li> <li>Performance tracking \u2014 Log parameter choices and outcomes</li> <li>Time-on-task \u2014 Measure engagement and struggle points</li> <li>Path analysis \u2014 What sequence did learners follow?</li> <li>Final state capture \u2014 What did learners create or configure?</li> </ul> <p>Integration with LMS systems enables grade reporting and learning analytics dashboards.</p>"},{"location":"faq/#how-do-i-design-for-multiple-learning-modalities","title":"How do I design for multiple learning modalities?","text":"<p>Universal Design for Learning (UDL) suggests multiple means of:</p> <p>Engagement: - Choice in how to explore - Relevance to learner interests - Self-regulation support</p> <p>Representation: - Visual, auditory, and text options - Vocabulary support - Highlighting patterns</p> <p>Action/Expression: - Multiple ways to interact - Scaffolded complexity - Feedback in multiple formats</p>"},{"location":"faq/#how-do-i-integrate-microsims-with-learning-management-systems","title":"How do I integrate MicroSims with learning management systems?","text":"<p>LMS integration options:</p> <ol> <li>Embedding \u2014 iframe in LMS content pages</li> <li>LTI \u2014 Learning Tools Interoperability for deeper integration</li> <li>xAPI \u2014 Experience API for detailed activity tracking</li> <li>SCORM \u2014 Legacy standard for completion tracking</li> </ol> <p>For most uses, simple embedding with manual completion marking is sufficient. Advanced tracking requires additional development beyond this course scope.</p>"},{"location":"faq/#how-do-i-handle-localization-and-translation","title":"How do I handle localization and translation?","text":"<p>Multilingual MicroSim strategies:</p> <ol> <li>Separate content from code \u2014 Use JSON data files for text</li> <li>Design for text expansion \u2014 Other languages may be longer</li> <li>Use icons alongside text \u2014 Universal symbols help comprehension</li> <li>Consider reading direction \u2014 RTL languages need layout adjustments</li> <li>Test with native speakers \u2014 Translation affects comprehension</li> </ol>"},{"location":"faq/#how-do-i-create-a-microsim-library-for-my-organization","title":"How do I create a MicroSim library for my organization?","text":"<p>Building an organizational library:</p> <ol> <li>Establish standards \u2014 Consistent file structure, documentation</li> <li>Create templates \u2014 Starting points for common types</li> <li>Build a catalog \u2014 Searchable index by subject, level, type</li> <li>Define workflows \u2014 Review, approval, publication processes</li> <li>Enable sharing \u2014 Licensing, attribution, modification rights</li> <li>Track usage \u2014 Analytics on which MicroSims are used most</li> </ol>"},{"location":"faq/#what-emerging-technologies-might-affect-microsim-design","title":"What emerging technologies might affect MicroSim design?","text":"<p>Future considerations:</p> <ul> <li>AI integration \u2014 More sophisticated generation and adaptation</li> <li>Voice interfaces \u2014 Audio control and feedback</li> <li>AR/VR \u2014 Immersive 3D simulations</li> <li>Haptic feedback \u2014 Touch-based learning on mobile devices</li> <li>Real-time collaboration \u2014 Multiple learners interacting simultaneously</li> </ul> <p>This course provides foundations that transfer to emerging platforms, focusing on pedagogical principles rather than specific technologies.</p>"},{"location":"faq/#how-do-i-contribute-microsims-to-the-community","title":"How do I contribute MicroSims to the community?","text":"<p>Sharing your work:</p> <ol> <li>Document thoroughly \u2014 Learning objectives, usage instructions</li> <li>Choose a license \u2014 Creative Commons for educational content</li> <li>Test accessibility \u2014 Ensure broad usability</li> <li>Publish openly \u2014 GitHub, educational repositories</li> <li>Respond to feedback \u2014 Maintain and improve shared resources</li> </ol> <p>The educational technology community benefits from open sharing of effective MicroSims.</p>"},{"location":"feedback/","title":"Feedback on Graph Data Modeling","text":"<p>You are welcome to connect with me on anytime on LinkedIn or submit any issues to GitHub Issue Log.  All pull-requests with fixes to errors or additions are always welcome.</p> <p>If you would like to fill out a short survey and give us ideas on how we can create better tools for intelligent textbooks in the future.</p>"},{"location":"glossary/","title":"Glossary of Terms","text":"<p>This glossary contains definitions for the 200 key concepts covered in the Automating Instructional Design course. Each definition follows ISO 11179 metadata registry guidelines: precise, concise, distinct, non-circular, and unencumbered with business rules.</p>"},{"location":"glossary/#ab-testing","title":"A/B Testing","text":"<p>A research method that compares two versions of a learning experience to determine which performs better based on measurable outcomes.</p> <p>A/B testing is valuable in instructional design for making data-driven decisions about MicroSim features, layouts, or interaction patterns.</p> <p>Example: Testing whether learners achieve better outcomes with a slider-based input versus a text field for parameter adjustment.</p>"},{"location":"glossary/#ability-based-feedback","title":"Ability-Based Feedback","text":"<p>Learner feedback that is tailored to accommodate different skill levels or learning capabilities.</p> <p>This approach recognizes that learners with varying abilities may need different types of support, scaffolding, or challenge levels.</p> <p>Example: Providing additional hints for struggling learners while offering extension challenges for advanced learners within the same MicroSim.</p>"},{"location":"glossary/#abstract-concepts","title":"Abstract Concepts","text":"<p>Ideas or principles that cannot be directly perceived through the senses and require mental representation to understand.</p> <p>Abstract concepts often benefit most from interactive simulations that make invisible relationships visible.</p> <p>Example: Supply and demand, gravity, or probability are abstract concepts that MicroSims can help visualize.</p>"},{"location":"glossary/#accessibility-audit","title":"Accessibility Audit","text":"<p>A systematic evaluation of a learning resource to identify barriers that prevent learners with disabilities from fully accessing the content.</p> <p>Accessibility audits typically check for screen reader compatibility, keyboard navigation, color contrast, and motion sensitivity.</p> <p>Example: Testing a MicroSim with a screen reader to ensure all interactive elements are properly labeled.</p>"},{"location":"glossary/#action-verbs","title":"Action Verbs","text":"<p>Words that describe observable, measurable behaviors used in learning objectives to specify what learners will be able to do.</p> <p>Selecting appropriate action verbs ensures that learning objectives align with the intended cognitive complexity level.</p> <p>Example: \"Calculate,\" \"compare,\" and \"design\" are action verbs; \"understand\" and \"appreciate\" are too vague for learning objectives.</p>"},{"location":"glossary/#age-based-feedback","title":"Age-Based Feedback","text":"<p>Learner feedback that is gathered or interpreted with consideration for the developmental stage of the participant.</p> <p>Different age groups may express confusion, engagement, or understanding in different ways, requiring adapted feedback collection methods.</p> <p>Example: Using emoji-based ratings for elementary students versus written reflections for adults.</p>"},{"location":"glossary/#ai-interpretation","title":"AI Interpretation","text":"<p>The process by which an artificial intelligence system converts natural language specifications into generated outputs.</p> <p>Understanding how AI interprets instructions helps instructional designers write specifications that produce desired results.</p> <p>Example: An AI may interpret \"colorful\" differently than \"using the colors red, blue, and green,\" leading to different outputs.</p>"},{"location":"glossary/#ai-assisted-design","title":"AI-Assisted Design","text":"<p>An approach to instructional design that leverages artificial intelligence tools to generate, refine, or evaluate educational content.</p> <p>AI-assisted design shifts the designer's role from implementation to specification and quality assurance.</p> <p>Example: Using Claude Code skills to generate interactive simulations from detailed specifications.</p>"},{"location":"glossary/#analyze-level","title":"Analyze Level","text":"<p>The fourth level of Bloom's Taxonomy involving breaking material into constituent parts and detecting how parts relate to each other and to an overall structure.</p> <p>Analysis requires learners to go beyond understanding to actively examine and deconstruct information.</p> <p>Example: A learning objective at this level: \"Compare three sorting algorithms in terms of time complexity.\"</p>"},{"location":"glossary/#animation-speed","title":"Animation Speed","text":"<p>The rate at which visual changes occur in a dynamic simulation or demonstration.</p> <p>Animation speed significantly affects cognitive load; speeds that are too fast overwhelm learners while speeds that are too slow lose engagement.</p> <p>Example: Complex physics simulations often need slower animation speeds than simple demonstrations.</p>"},{"location":"glossary/#apply-level","title":"Apply Level","text":"<p>The third level of Bloom's Taxonomy involving carrying out or using a procedure in a given situation.</p> <p>Application requires learners to use acquired knowledge to solve problems or complete tasks.</p> <p>Example: A learning objective at this level: \"Calculate the mean and standard deviation for a data set.\"</p>"},{"location":"glossary/#assumed-knowledge","title":"Assumed Knowledge","text":"<p>The prerequisite knowledge that learners are expected to possess before beginning instruction.</p> <p>Identifying assumed knowledge helps designers avoid unnecessary explanation while ensuring learners have the foundation needed for new content.</p> <p>Example: A calculus course assumes knowledge of algebra and trigonometry.</p>"},{"location":"glossary/#atomic-concepts","title":"Atomic Concepts","text":"<p>Single, indivisible units of knowledge or skill that can be taught and assessed independently.</p> <p>Atomic concepts are the building blocks of more complex understanding and serve as the nodes in a learning graph.</p> <p>Example: \"The definition of mean in statistics\" is an atomic concept; \"perform statistical analysis\" is not.</p>"},{"location":"glossary/#automated-evaluation","title":"Automated Evaluation","text":"<p>Assessment of learning outcomes or resource quality performed by computer systems without human intervention.</p> <p>Automated evaluation can provide immediate feedback at scale but may miss nuances that human evaluators catch.</p> <p>Example: A quiz system that grades multiple-choice questions instantly and provides feedback.</p>"},{"location":"glossary/#behavior-constraints","title":"Behavior Constraints","text":"<p>Limits or boundaries placed on the possible interactions within a MicroSim to focus learning on specific concepts.</p> <p>Well-designed constraints prevent learners from exploring unproductive paths while still allowing meaningful exploration.</p> <p>Example: Limiting a physics simulation to two dimensions to simplify learning before introducing three-dimensional concepts.</p>"},{"location":"glossary/#blooms-taxonomy","title":"Bloom's Taxonomy","text":"<p>A hierarchical classification system for cognitive complexity levels used to categorize and write learning objectives.</p> <p>The 2001 revision by Anderson and Krathwohl uses verbs (Remember, Understand, Apply, Analyze, Evaluate, Create) to emphasize active learning.</p> <p>Example: Using Bloom's Taxonomy to ensure a course includes learning objectives at multiple cognitive levels.</p>"},{"location":"glossary/#bug-identification","title":"Bug Identification","text":"<p>The process of finding and documenting defects or errors in interactive simulations or software.</p> <p>Bug identification is part of technical evaluation and ensures MicroSims function correctly before deployment.</p> <p>Example: Discovering that a slider control produces incorrect values at its boundary positions.</p>"},{"location":"glossary/#cause-effect-display","title":"Cause-Effect Display","text":"<p>A visualization that shows how changes in one variable or action lead to changes in another.</p> <p>Cause-effect displays help learners understand causal relationships that may not be obvious from static descriptions.</p> <p>Example: A simulation showing how adjusting interest rates affects economic indicators.</p>"},{"location":"glossary/#change-log","title":"Change Log","text":"<p>A documented record of modifications made to a learning resource over time, including the rationale for each change.</p> <p>Change logs support version control and help teams understand the evolution of educational content.</p> <p>Example: Recording that a simulation's default parameters were adjusted after user testing revealed confusion.</p>"},{"location":"glossary/#change-prioritization","title":"Change Prioritization","text":"<p>The process of ordering proposed modifications to a learning resource based on impact, effort, and urgency.</p> <p>Effective change prioritization ensures limited development resources address the most important improvements first.</p> <p>Example: Prioritizing a fix for a navigation bug over adding an optional advanced feature.</p>"},{"location":"glossary/#chart-visualization","title":"Chart Visualization","text":"<p>A graphical representation of data using bars, lines, areas, or other visual elements to reveal patterns, trends, or comparisons.</p> <p>Charts are appropriate for learning objectives involving data interpretation, comparison, or trend analysis.</p> <p>Example: A bar chart comparing population growth across different countries.</p>"},{"location":"glossary/#chartjs-library","title":"Chart.js Library","text":"<p>A JavaScript library for creating responsive, animated charts in web-based applications.</p> <p>Chart.js is well-suited for MicroSims that need standard chart types with good visual polish and interactivity.</p> <p>Example: Using Chart.js to create an interactive line chart showing historical temperature data.</p>"},{"location":"glossary/#classification-display","title":"Classification Display","text":"<p>A visualization that shows how items are categorized or sorted into groups based on shared characteristics.</p> <p>Classification displays help learners understand taxonomies, typologies, or categorical relationships.</p> <p>Example: A Venn diagram showing the overlap between mammals, aquatic animals, and warm-blooded creatures.</p>"},{"location":"glossary/#claude-code-skills","title":"Claude Code Skills","text":"<p>Specialized capabilities within Claude Code that enable automated generation of specific types of content, including MicroSims.</p> <p>Skills provide structured workflows for complex generation tasks, ensuring consistent output quality.</p> <p>Example: The microsim-generator skill that creates interactive simulations from specifications.</p>"},{"location":"glossary/#code-generation","title":"Code Generation","text":"<p>The automated creation of programming code by AI systems based on natural language descriptions or specifications.</p> <p>Code generation enables non-programmers to create interactive simulations by focusing on what should be built rather than how to build it.</p> <p>Example: An AI generating JavaScript code for a physics simulation from a plain-language specification.</p>"},{"location":"glossary/#cognitive-complexity","title":"Cognitive Complexity","text":"<p>The level of mental processing required to complete a learning task, as categorized by frameworks like Bloom's Taxonomy.</p> <p>Higher cognitive complexity requires more working memory resources and deeper engagement with the material.</p> <p>Example: Creating a novel solution requires higher cognitive complexity than recognizing a familiar fact.</p>"},{"location":"glossary/#cognitive-development","title":"Cognitive Development","text":"<p>The process by which thinking abilities change and grow over the lifespan, as described by theories such as Piaget's stages.</p> <p>Understanding cognitive development helps designers create age-appropriate learning experiences.</p> <p>Example: Designing simpler cause-effect relationships for elementary students than for high school students.</p>"},{"location":"glossary/#cognitive-level-match","title":"Cognitive Level Match","text":"<p>The alignment between the cognitive complexity of a learning objective and the cognitive demands of its assessment or learning activity.</p> <p>Mismatches occur when assessments test at a different level than the stated objective.</p> <p>Example: If an objective states learners will \"analyze\" but the test only asks them to \"recall,\" there is a mismatch.</p>"},{"location":"glossary/#cognitive-load-meter","title":"Cognitive Load Meter","text":"<p>A visual indicator that estimates and displays the cognitive demands being placed on a learner during an interactive experience.</p> <p>Cognitive load meters help designers evaluate their choices and help learners self-regulate their learning pace.</p> <p>Example: A gauge showing that adding more visual elements pushes the interface toward cognitive overload.</p>"},{"location":"glossary/#cognitive-load-theory","title":"Cognitive Load Theory","text":"<p>A framework explaining how the cognitive demands of instruction affect learning, based on the limitations of working memory.</p> <p>Cognitive load theory distinguishes between intrinsic, extraneous, and germane load, with different design implications for each.</p> <p>Example: Reducing visual clutter to lower extraneous load and free working memory for learning.</p>"},{"location":"glossary/#color-accessibility","title":"Color Accessibility","text":"<p>Design choices that ensure color is not the only means of conveying information, accommodating users with color vision deficiencies.</p> <p>Accessible color design often includes patterns, labels, or icons alongside color coding.</p> <p>Example: Using both color and shape to distinguish data series in a chart.</p>"},{"location":"glossary/#color-blindness-design","title":"Color Blindness Design","text":"<p>Design practices that ensure content remains usable and understandable for individuals with various types of color vision deficiency.</p> <p>Approximately 8% of males and 0.5% of females have some form of color vision deficiency.</p> <p>Example: Avoiding red-green color combinations as the sole distinguishing feature between elements.</p>"},{"location":"glossary/#common-misconceptions","title":"Common Misconceptions","text":"<p>Incorrect beliefs or mental models that are frequently held by learners in a particular domain.</p> <p>Identifying common misconceptions allows designers to create simulations that specifically address and correct these errors.</p> <p>Example: The misconception that seasons are caused by Earth's distance from the sun rather than axial tilt.</p>"},{"location":"glossary/#completion-criteria","title":"Completion Criteria","text":"<p>The standards or conditions that define when a learning resource or iteration is considered finished.</p> <p>Clear completion criteria prevent both premature release and endless refinement.</p> <p>Example: A MicroSim is complete when it passes technical testing, achieves user testing goals, and meets accessibility standards.</p>"},{"location":"glossary/#compound-objectives","title":"Compound Objectives","text":"<p>Learning objectives that combine multiple atomic concepts or skills into a single statement.</p> <p>Compound objectives should be decomposed into atomic components for effective instruction and assessment.</p> <p>Example: \"Design and implement a RESTful API with authentication\" combines multiple skills into one objective.</p>"},{"location":"glossary/#concept-characteristics","title":"Concept Characteristics","text":"<p>The properties or features of a concept that influence how it can best be taught or visualized.</p> <p>Understanding concept characteristics helps in selecting appropriate visualization paradigms.</p> <p>Example: Concepts involving change over time may be best suited for timeline or animation visualizations.</p>"},{"location":"glossary/#concept-dependencies","title":"Concept Dependencies","text":"<p>The prerequisite relationships between concepts where understanding one concept requires prior understanding of another.</p> <p>Dependencies form the edges in a learning graph, determining the order in which concepts should be taught.</p> <p>Example: Understanding \"function\" depends on first understanding \"variable\" in programming.</p>"},{"location":"glossary/#conceptual-boundary","title":"Conceptual Boundary","text":"<p>The limits or edges of where a concept applies, distinguishing it from related or similar concepts.</p> <p>Exploring conceptual boundaries helps learners understand when and where to apply their knowledge.</p> <p>Example: Understanding where Newtonian physics stops working and relativistic physics becomes necessary.</p>"},{"location":"glossary/#conceptual-change","title":"Conceptual Change","text":"<p>The process by which learners revise or replace existing mental models with more accurate ones.</p> <p>Conceptual change is often necessary when learners hold misconceptions that conflict with accurate understanding.</p> <p>Example: Shifting from believing that heavier objects fall faster to understanding uniform acceleration.</p>"},{"location":"glossary/#constraint-simulation","title":"Constraint Simulation","text":"<p>A learning activity that simulates the experience of operating under specific limitations, such as accessibility constraints.</p> <p>Constraint simulations help designers develop empathy and understanding for diverse learner needs.</p> <p>Example: Using a MicroSim in grayscale mode to experience how color-blind users perceive the interface.</p>"},{"location":"glossary/#content-sharing","title":"Content Sharing","text":"<p>The practice of making educational resources available to other educators for reuse, adaptation, or collaboration.</p> <p>Content sharing increases the return on instructional design investment and promotes best practices.</p> <p>Example: Publishing MicroSims under Creative Commons licenses for other educators to use.</p>"},{"location":"glossary/#contrast-design","title":"Contrast Design","text":"<p>The use of differences in color, size, or other visual properties to make important elements stand out from their surroundings.</p> <p>Adequate contrast is essential for accessibility and reduces extraneous cognitive load.</p> <p>Example: Ensuring text has at least a 4.5:1 contrast ratio against its background color.</p>"},{"location":"glossary/#conversation-prompting","title":"Conversation Prompting","text":"<p>The practice of refining AI-generated outputs through iterative dialogue with the AI system.</p> <p>Effective conversation prompting treats AI interaction as a collaborative refinement process rather than a single query.</p> <p>Example: Asking an AI to \"make the colors more distinct\" after reviewing initial output.</p>"},{"location":"glossary/#correlation-display","title":"Correlation Display","text":"<p>A visualization showing how two or more variables change in relation to each other.</p> <p>Correlation displays help learners identify relationships without implying causation.</p> <p>Example: A scatter plot showing the relationship between study hours and test scores.</p>"},{"location":"glossary/#create-level","title":"Create Level","text":"<p>The sixth and highest level of Bloom's Taxonomy involving putting elements together to form a novel, coherent whole.</p> <p>Creation requires synthesis of knowledge to produce something new rather than reproducing existing information.</p> <p>Example: A learning objective at this level: \"Design a MicroSim specification for a given learning objective.\"</p>"},{"location":"glossary/#critical-changes","title":"Critical Changes","text":"<p>Modifications to a learning resource that are essential for it to function correctly or achieve its learning objectives.</p> <p>Critical changes take priority over enhancements during the iteration process.</p> <p>Example: Fixing a bug that causes the simulation to display incorrect data.</p>"},{"location":"glossary/#cultural-sensitivity","title":"Cultural Sensitivity","text":"<p>Awareness of and respect for cultural differences in designing educational content, avoiding assumptions or biases.</p> <p>Culturally sensitive design uses inclusive examples and avoids content that may alienate learners from different backgrounds.</p> <p>Example: Using names and scenarios from diverse cultural contexts in example problems.</p>"},{"location":"glossary/#data-interpretation","title":"Data Interpretation","text":"<p>The skill of extracting meaning and insights from data presented in various formats.</p> <p>Data interpretation is a common learning objective at the Analyze and Evaluate levels of Bloom's Taxonomy.</p> <p>Example: Reading a population pyramid chart to draw conclusions about demographic trends.</p>"},{"location":"glossary/#dependency-mapping","title":"Dependency Mapping","text":"<p>The process of documenting and visualizing the prerequisite relationships between concepts in a curriculum.</p> <p>Dependency mapping reveals the structure needed for effective learning sequences and identifies bottleneck concepts.</p> <p>Example: Creating a directed graph showing which concepts must be learned before others.</p>"},{"location":"glossary/#design-rationale","title":"Design Rationale","text":"<p>The documented reasoning behind design decisions, explaining why particular choices were made.</p> <p>Recording design rationale helps future designers understand and build upon previous work.</p> <p>Example: Noting that a particular color scheme was chosen for accessibility rather than aesthetics.</p>"},{"location":"glossary/#design-tradeoffs","title":"Design Tradeoffs","text":"<p>The competing considerations that require compromises in instructional design, where improving one aspect may diminish another.</p> <p>Understanding tradeoffs helps designers make conscious choices rather than inadvertently sacrificing important qualities.</p> <p>Example: Increasing visual simplicity may reduce information completeness.</p>"},{"location":"glossary/#design-test-refine-cycle","title":"Design-Test-Refine Cycle","text":"<p>An iterative process of creating a design, evaluating it with users, and making improvements based on findings.</p> <p>Multiple cycles of design-test-refine lead to progressively better learning experiences.</p> <p>Example: Testing a MicroSim with learners, gathering feedback, making changes, and testing again.</p>"},{"location":"glossary/#diagram-visualization","title":"Diagram Visualization","text":"<p>A visual representation using shapes, lines, and labels to show relationships, processes, or structures.</p> <p>Diagrams are effective for learning objectives involving understanding of systems, flows, or hierarchies.</p> <p>Example: A flowchart showing the steps in a decision-making process.</p>"},{"location":"glossary/#differentiation-strategy","title":"Differentiation Strategy","text":"<p>An approach that adapts instruction to meet the varying needs of different learners within the same experience.</p> <p>Differentiation can address differences in prior knowledge, learning pace, or preferred modalities.</p> <p>Example: Providing optional hints that struggling learners can access while others proceed independently.</p>"},{"location":"glossary/#distribution-chart","title":"Distribution Chart","text":"<p>A visualization showing how data values are spread across a range, revealing patterns of frequency or density.</p> <p>Distribution charts help learners understand concepts like normal distribution, skewness, or outliers.</p> <p>Example: A histogram showing the distribution of test scores in a class.</p>"},{"location":"glossary/#documentation-standard","title":"Documentation Standard","text":"<p>A consistent format and level of detail for recording information about learning resources to support reuse and maintenance.</p> <p>Documentation standards ensure that MicroSims can be understood, modified, and maintained by future designers.</p> <p>Example: A template requiring each MicroSim to include learning objectives, user guide, and technical requirements.</p>"},{"location":"glossary/#dynamic-systems","title":"Dynamic Systems","text":"<p>Systems in which elements interact and change over time, often exhibiting emergent behavior.</p> <p>Dynamic systems are well-suited for simulation-based learning because static representations cannot capture their behavior.</p> <p>Example: An ecosystem simulation showing predator-prey population dynamics.</p>"},{"location":"glossary/#early-childhood-design","title":"Early Childhood Design","text":"<p>Design approaches tailored for learners approximately ages 3-7, emphasizing large touch targets, simple cause-effect relationships, and minimal text.</p> <p>Early childhood designs must accommodate developing motor skills and pre-reading abilities.</p> <p>Example: Using drag-and-drop interactions with large, colorful objects rather than typed input.</p>"},{"location":"glossary/#edge-case-definition","title":"Edge Case Definition","text":"<p>The specification of how a MicroSim should behave under unusual, extreme, or boundary conditions.</p> <p>Defining edge cases prevents unexpected behavior that could confuse learners or produce incorrect results.</p> <p>Example: Specifying what happens when a slider is moved to its minimum or maximum position.</p>"},{"location":"glossary/#educational-technology","title":"Educational Technology","text":"<p>Tools, systems, and digital resources used to facilitate, enhance, or assess learning.</p> <p>Educational technology encompasses learning management systems, interactive simulations, assessment tools, and AI assistants.</p> <p>Example: A MicroSim is an example of educational technology used for concept demonstration.</p>"},{"location":"glossary/#educator-collaboration","title":"Educator Collaboration","text":"<p>Working together with other educators to share resources, exchange ideas, and improve instructional practices.</p> <p>Collaboration multiplies the impact of instructional design efforts and promotes continuous improvement.</p> <p>Example: A team of teachers contributing to and refining a shared library of MicroSims.</p>"},{"location":"glossary/#effectiveness-measure","title":"Effectiveness Measure","text":"<p>A quantifiable indicator of how well a learning resource achieves its intended learning objectives.</p> <p>Effectiveness measures enable evidence-based decisions about instructional design choices.</p> <p>Example: Comparing pre-test and post-test scores to measure learning gains from a MicroSim.</p>"},{"location":"glossary/#elementary-design","title":"Elementary Design","text":"<p>Design approaches tailored for learners approximately ages 8-10, featuring guided exploration, scaffolded complexity, and reading support.</p> <p>Elementary designs can include more text and abstract concepts than early childhood but still need significant scaffolding.</p> <p>Example: Including optional text-to-speech for written instructions.</p>"},{"location":"glossary/#engagement-balance","title":"Engagement Balance","text":"<p>The appropriate level of interest and motivation generated by a learning experience without creating distraction from learning.</p> <p>Engagement that focuses attention on learning content is beneficial; engagement through irrelevant features is harmful.</p> <p>Example: Interactive elements that help learners explore concepts versus decorative animations that distract.</p>"},{"location":"glossary/#ethical-research","title":"Ethical Research","text":"<p>Research practices that respect participant rights, ensure informed consent, and protect privacy and well-being.</p> <p>User testing of MicroSims with learners must follow ethical guidelines, especially with children or vulnerable populations.</p> <p>Example: Obtaining parental consent before testing educational software with minors.</p>"},{"location":"glossary/#evaluate-level","title":"Evaluate Level","text":"<p>The fifth level of Bloom's Taxonomy involving making judgments based on criteria and standards.</p> <p>Evaluation requires both understanding the subject matter and applying appropriate standards to assess quality or effectiveness.</p> <p>Example: A learning objective at this level: \"Assess whether a MicroSim effectively targets its stated learning objective.\"</p>"},{"location":"glossary/#evaluation-rubric","title":"Evaluation Rubric","text":"<p>A structured guide with criteria and standards for assessing the quality of learning resources or learner performance.</p> <p>Rubrics provide consistency in evaluation and make expectations explicit to both evaluators and creators.</p> <p>Example: A rubric scoring MicroSims on technical functionality, pedagogical alignment, and accessibility.</p>"},{"location":"glossary/#extraneous-load","title":"Extraneous Load","text":"<p>The cognitive effort wasted on activities that do not contribute to learning, caused by poor instructional design.</p> <p>Minimizing extraneous load frees working memory resources for actual learning.</p> <p>Example: Cognitive effort spent searching for related information that is scattered across the screen.</p>"},{"location":"glossary/#flowchart","title":"Flowchart","text":"<p>A diagram using standardized shapes and arrows to represent a process, workflow, or algorithm.</p> <p>Flowcharts help learners understand sequential processes with decision points.</p> <p>Example: A flowchart showing the steps to troubleshoot a technical problem.</p>"},{"location":"glossary/#functionality-testing","title":"Functionality Testing","text":"<p>The process of verifying that all features of a MicroSim work correctly under normal usage conditions.</p> <p>Functionality testing catches bugs and errors before learners encounter them.</p> <p>Example: Testing that all buttons, sliders, and interactive elements respond as expected.</p>"},{"location":"glossary/#fundamental-redesign","title":"Fundamental Redesign","text":"<p>A major revision of a learning resource that changes its core approach, structure, or implementation.</p> <p>Fundamental redesign is necessary when incremental improvements cannot address significant problems.</p> <p>Example: Rebuilding a simulation with a different visualization paradigm after testing shows the original approach was ineffective.</p>"},{"location":"glossary/#generation-workflow","title":"Generation Workflow","text":"<p>The sequence of steps involved in using AI tools to create educational content, from initial prompt to final output.</p> <p>Understanding the generation workflow helps designers use AI tools efficiently and iteratively.</p> <p>Example: Writing a specification, generating output, reviewing, providing refinement prompts, and repeating.</p>"},{"location":"glossary/#germane-load","title":"Germane Load","text":"<p>The cognitive effort dedicated to learning processes that build schemas and transfer knowledge to long-term memory.</p> <p>Germane load represents productive mental effort that should be maximized.</p> <p>Example: Mental effort spent connecting new concepts to prior knowledge or generating explanations.</p>"},{"location":"glossary/#graduate-design","title":"Graduate Design","text":"<p>Design approaches tailored for graduate-level learners, featuring research applications, theoretical depth, and exploration of limitations.</p> <p>Graduate designs assume substantial prior knowledge and can present advanced parameter spaces and edge cases.</p> <p>Example: A simulation allowing exploration of parameter ranges beyond typical undergraduate scenarios.</p>"},{"location":"glossary/#guided-exploration","title":"Guided Exploration","text":"<p>A learning approach that provides structure and direction while allowing learners to make choices and discoveries.</p> <p>Guided exploration balances learner autonomy with instructional guidance to prevent unproductive wandering.</p> <p>Example: A simulation with suggested scenarios but freedom to modify parameters.</p>"},{"location":"glossary/#hierarchy-display","title":"Hierarchy Display","text":"<p>A visualization showing levels of organization or relationships of subordination within a structure.</p> <p>Hierarchies are effective for concepts involving classification, organization, or levels of abstraction.</p> <p>Example: An organizational chart showing reporting relationships in a company.</p>"},{"location":"glossary/#high-school-design","title":"High School Design","text":"<p>Design approaches tailored for learners approximately ages 14-18, emphasizing real-world applications, data interpretation, and edge cases.</p> <p>High school designs can include significant complexity and abstract reasoning.</p> <p>Example: A physics simulation with real-world friction and air resistance considerations.</p>"},{"location":"glossary/#human-evaluation","title":"Human Evaluation","text":"<p>Assessment of learning resources by human reviewers rather than automated systems.</p> <p>Human evaluation captures nuances, contextual appropriateness, and pedagogical judgment that automated systems may miss.</p> <p>Example: An expert instructional designer reviewing a MicroSim for pedagogical effectiveness.</p>"},{"location":"glossary/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>A scientific approach involving making predictions and gathering evidence to support or refute them.</p> <p>Hypothesis testing is a valuable learning activity that engages higher-order thinking.</p> <p>Example: A simulation where learners predict an outcome, then run the simulation to test their prediction.</p>"},{"location":"glossary/#incremental-improvement","title":"Incremental Improvement","text":"<p>Small, gradual modifications to a learning resource that progressively enhance its quality.</p> <p>Incremental improvements are appropriate when the fundamental design is sound but refinements are needed.</p> <p>Example: Adjusting color contrast after accessibility testing identifies issues.</p>"},{"location":"glossary/#influence-diagram","title":"Influence Diagram","text":"<p>A visualization showing how factors or variables affect one another within a system.</p> <p>Influence diagrams help learners understand complex causal relationships and feedback loops.</p> <p>Example: A diagram showing how price, demand, supply, and competition influence each other.</p>"},{"location":"glossary/#information-density","title":"Information Density","text":"<p>The amount of information presented within a given visual space.</p> <p>Optimal information density varies based on learner expertise, content complexity, and display context.</p> <p>Example: Novice learners typically need lower information density than experts.</p>"},{"location":"glossary/#instructional-design","title":"Instructional Design","text":"<p>The systematic process of creating educational experiences that make learning efficient, effective, and engaging.</p> <p>Instructional designers analyze learning needs, design experiences, and evaluate outcomes.</p> <p>Example: Planning a course that progresses from foundational concepts to advanced applications.</p>"},{"location":"glossary/#intelligent-textbook","title":"Intelligent Textbook","text":"<p>An interactive digital textbook that adapts to learner needs, tracks progress, and provides personalized learning paths.</p> <p>Intelligent textbooks often incorporate embedded simulations and assessments.</p> <p>Example: A textbook that recommends review content when a learner struggles with an assessment.</p>"},{"location":"glossary/#intent-preservation","title":"Intent Preservation","text":"<p>Maintaining the original pedagogical purpose throughout the process of creating or generating learning content.</p> <p>Intent preservation ensures that AI-generated content serves the stated learning objectives.</p> <p>Example: Ensuring a specification's learning goals are reflected in the final MicroSim implementation.</p>"},{"location":"glossary/#interaction-behavior","title":"Interaction Behavior","text":"<p>How a MicroSim responds to user inputs such as clicks, drags, or parameter changes.</p> <p>Well-designed interaction behavior provides clear feedback and supports the learning objectives.</p> <p>Example: A slider that updates a graph in real-time as the user moves it.</p>"},{"location":"glossary/#interaction-tracking","title":"Interaction Tracking","text":"<p>Recording and analyzing how learners interact with a MicroSim to understand usage patterns and learning behavior.</p> <p>Interaction tracking data informs both assessment and iterative improvement.</p> <p>Example: Logging which parameters learners adjust most frequently in a simulation.</p>"},{"location":"glossary/#interactive-simulation","title":"Interactive Simulation","text":"<p>A digital experience that responds to user input and models real or abstract systems dynamically.</p> <p>Interactive simulations allow learners to explore cause-and-effect relationships through experimentation.</p> <p>Example: A simulation of planetary orbits where learners can adjust masses and velocities.</p>"},{"location":"glossary/#intrinsic-load","title":"Intrinsic Load","text":"<p>The cognitive effort required by the inherent complexity of the learning material itself.</p> <p>Intrinsic load is determined by element interactivity and learner prior knowledge; it can be managed but not eliminated.</p> <p>Example: Calculus has higher intrinsic load than basic arithmetic regardless of how it is taught.</p>"},{"location":"glossary/#intuitiveness","title":"Intuitiveness","text":"<p>The quality of a design that allows users to understand how to use it without explicit instruction.</p> <p>Intuitive designs follow familiar conventions and provide clear affordances.</p> <p>Example: A play button that learners immediately recognize and know how to use.</p>"},{"location":"glossary/#issue-identification","title":"Issue Identification","text":"<p>The process of recognizing and documenting problems in AI-generated content that need correction.</p> <p>Issue identification is a critical skill for quality assurance in AI-assisted design workflows.</p> <p>Example: Noticing that generated code produces incorrect output for certain input values.</p>"},{"location":"glossary/#iteration-management","title":"Iteration Management","text":"<p>The practice of organizing and tracking multiple cycles of design refinement efficiently.</p> <p>Good iteration management prevents duplication of effort and maintains progress toward completion.</p> <p>Example: Using version control and change logs to track improvements across multiple iterations.</p>"},{"location":"glossary/#iterative-refinement","title":"Iterative Refinement","text":"<p>The process of progressively improving a learning resource through repeated cycles of testing and modification.</p> <p>Iterative refinement accepts that initial designs are rarely optimal and builds in improvement processes.</p> <p>Example: Testing a MicroSim, gathering feedback, making changes, and testing again until quality goals are met.</p>"},{"location":"glossary/#job-relevant-scenarios","title":"Job-Relevant Scenarios","text":"<p>Learning contexts and examples drawn from authentic workplace situations that learners will encounter professionally.</p> <p>Job-relevant scenarios increase motivation and transfer of learning to real performance.</p> <p>Example: A corporate training simulation using actual company processes and data.</p>"},{"location":"glossary/#keyboard-navigation","title":"Keyboard Navigation","text":"<p>The ability to operate all interactive elements using only keyboard inputs without requiring a mouse or touch.</p> <p>Keyboard navigation is essential for users who cannot use pointing devices and for screen reader users.</p> <p>Example: Using Tab to move between controls and Enter to activate buttons.</p>"},{"location":"glossary/#leaflet-library","title":"Leaflet Library","text":"<p>An open-source JavaScript library for creating interactive maps in web-based applications.</p> <p>Leaflet is appropriate for MicroSims involving geographic or spatial learning objectives.</p> <p>Example: A simulation showing historical migration patterns on an interactive map.</p>"},{"location":"glossary/#learner-control","title":"Learner Control","text":"<p>Design features that allow learners to make choices about pacing, sequence, or depth of content.</p> <p>Learner control accommodates individual differences and supports self-regulated learning.</p> <p>Example: Speed controls that let learners slow down or speed up animations.</p>"},{"location":"glossary/#learner-feedback","title":"Learner Feedback","text":"<p>Information gathered from learners about their experience with a learning resource.</p> <p>Feedback informs iterative improvement and validates design decisions.</p> <p>Example: Surveys, interviews, or observation notes from user testing sessions.</p>"},{"location":"glossary/#learning-analytics","title":"Learning Analytics","text":"<p>The measurement, collection, analysis, and reporting of data about learners and their contexts.</p> <p>Learning analytics help identify struggling learners, effective content, and opportunities for improvement.</p> <p>Example: Dashboards showing completion rates and performance patterns across a course.</p>"},{"location":"glossary/#learning-objective","title":"Learning Objective","text":"<p>A clear, specific statement describing what learners will be able to do after completing an instructional experience.</p> <p>Effective learning objectives use action verbs and describe observable, measurable behaviors.</p> <p>Example: \"Given a data set, the learner will calculate the standard deviation with 90% accuracy.\"</p>"},{"location":"glossary/#learning-outcome","title":"Learning Outcome","text":"<p>The actual result of a learning process, representing what learners can demonstrably do after instruction.</p> <p>Learning outcomes are assessed to determine whether learning objectives were achieved.</p> <p>Example: Post-test scores showing that learners can perform the targeted skill.</p>"},{"location":"glossary/#learning-pathway","title":"Learning Pathway","text":"<p>A recommended sequence through concepts or modules based on dependencies and learning efficiency.</p> <p>Learning pathways guide learners through content in an order that respects prerequisite relationships.</p> <p>Example: A visual map showing the recommended order for completing course modules.</p>"},{"location":"glossary/#library-organization","title":"Library Organization","text":"<p>The systematic arrangement and cataloging of learning resources for efficient storage, retrieval, and reuse.</p> <p>Good library organization enables educators to find and adapt existing resources rather than creating from scratch.</p> <p>Example: Tagging MicroSims by subject area, grade level, and visualization type.</p>"},{"location":"glossary/#lms-integration","title":"LMS Integration","text":"<p>The connection of learning resources with a Learning Management System to enable assignment, tracking, and grading.</p> <p>LMS integration allows MicroSims to be part of a cohesive course experience with centralized data.</p> <p>Example: Embedding a MicroSim in Canvas or Moodle so completion is automatically recorded.</p>"},{"location":"glossary/#long-term-memory","title":"Long-Term Memory","text":"<p>The memory system with virtually unlimited capacity for storing information over extended periods.</p> <p>Knowledge in long-term memory is organized in schemas and must be retrieved to be used.</p> <p>Example: Recalling the multiplication table learned years ago.</p>"},{"location":"glossary/#maintenance-planning","title":"Maintenance Planning","text":"<p>The process of anticipating and preparing for ongoing updates, fixes, and improvements to learning resources.</p> <p>Maintenance planning ensures resources remain functional and current over time.</p> <p>Example: Scheduling quarterly reviews to check for broken links or outdated content.</p>"},{"location":"glossary/#manual-adjustment","title":"Manual Adjustment","text":"<p>Modifications made directly to generated code or content by a human rather than through AI regeneration.</p> <p>Manual adjustment is appropriate for small fixes that don't warrant a full regeneration cycle.</p> <p>Example: Changing a color value in generated code rather than asking the AI to regenerate.</p>"},{"location":"glossary/#map-visualization","title":"Map Visualization","text":"<p>A graphical representation using geographic features to display spatial relationships or location-based data.</p> <p>Maps are appropriate for learning objectives involving geography, spatial patterns, or location-based analysis.</p> <p>Example: A choropleth map showing population density by region.</p>"},{"location":"glossary/#mathematical-relations","title":"Mathematical Relations","text":"<p>Quantitative relationships between variables expressed through equations, functions, or other mathematical representations.</p> <p>Understanding mathematical relations is often a learning objective at the undergraduate and graduate levels.</p> <p>Example: The relationship between force, mass, and acceleration expressed as F = ma.</p>"},{"location":"glossary/#measurable-outcomes","title":"Measurable Outcomes","text":"<p>Statements specifying what learners will do, how well they will do it, and under what conditions.</p> <p>Measurable outcomes enable objective assessment of learning achievement.</p> <p>Example: \"Without reference materials, learners will list all 50 U.S. states within 5 minutes.\"</p>"},{"location":"glossary/#mental-effort","title":"Mental Effort","text":"<p>The cognitive resources a learner is actively expending during a learning experience.</p> <p>Mental effort is a limited resource that should be directed toward productive learning activities.</p> <p>Example: The concentration required to understand a complex diagram.</p>"},{"location":"glossary/#mental-model","title":"Mental Model","text":"<p>An internal representation of how something works, used to understand, predict, and reason about systems.</p> <p>MicroSims can help learners build accurate mental models of abstract or invisible processes.</p> <p>Example: A learner's understanding of how electricity flows through a circuit.</p>"},{"location":"glossary/#mermaid-library","title":"Mermaid Library","text":"<p>A JavaScript-based diagramming and charting tool that generates diagrams from text-based descriptions.</p> <p>Mermaid is useful for quickly creating flowcharts, sequence diagrams, and other structured visualizations.</p> <p>Example: Generating a flowchart from a simple text description of process steps.</p>"},{"location":"glossary/#microsim","title":"MicroSim","text":"<p>A small, focused interactive simulation designed to teach a specific concept or skill through exploration and visualization.</p> <p>MicroSims bridge the gap between abstract learning objectives and concrete interactive experiences.</p> <p>Example: An interactive supply-and-demand curve that responds to user-adjusted parameters.</p>"},{"location":"glossary/#microsim-generator","title":"MicroSim Generator","text":"<p>An AI skill that creates interactive simulations from natural language specifications.</p> <p>The MicroSim generator translates pedagogical intent into working code and visual designs.</p> <p>Example: Using the microsim-generator skill to create a physics simulation from a written specification.</p>"},{"location":"glossary/#middle-school-design","title":"Middle School Design","text":"<p>Design approaches tailored for learners approximately ages 11-13, introducing abstract concepts, multiple variables, and hypothesis testing.</p> <p>Middle school designs can include more complexity than elementary but need clear scaffolding.</p> <p>Example: A simulation allowing manipulation of two variables simultaneously to explore their relationship.</p>"},{"location":"glossary/#misconception","title":"Misconception","text":"<p>An incorrect belief or understanding that a learner holds about a concept or phenomenon.</p> <p>Misconceptions can be resistant to change and may interfere with learning accurate information.</p> <p>Example: The belief that summer is hotter because Earth is closer to the sun.</p>"},{"location":"glossary/#misconception-correction","title":"Misconception Correction","text":"<p>Instructional strategies specifically designed to help learners identify and replace incorrect beliefs.</p> <p>Effective correction often involves making the misconception explicit before presenting accurate information.</p> <p>Example: A simulation that first reveals learners' predictions, then shows results that contradict common misconceptions.</p>"},{"location":"glossary/#misconception-reinforcement","title":"Misconception Reinforcement","text":"<p>When instruction inadvertently strengthens incorrect beliefs rather than correcting them.</p> <p>Poorly designed simulations may reinforce misconceptions if they don't directly address common errors.</p> <p>Example: A simulation that never shows edge cases where intuitive assumptions break down.</p>"},{"location":"glossary/#model-comparison","title":"Model Comparison","text":"<p>A learning activity involving examining multiple representations or explanations of the same phenomenon.</p> <p>Comparing models helps learners understand both the strengths and limitations of each representation.</p> <p>Example: Comparing the wave and particle models of light.</p>"},{"location":"glossary/#motion-simulation","title":"Motion Simulation","text":"<p>An animation showing objects moving through space over time.</p> <p>Motion simulations are effective for physics concepts, spatial relationships, and process flows.</p> <p>Example: A simulation of projectile motion showing trajectory paths.</p>"},{"location":"glossary/#multilingual-support","title":"Multilingual Support","text":"<p>Design features that make content accessible to speakers of different languages.</p> <p>Multilingual support may include translation, localization, or language selection options.</p> <p>Example: A MicroSim with labels available in English, Spanish, and Mandarin.</p>"},{"location":"glossary/#multiple-variables","title":"Multiple Variables","text":"<p>Learning situations involving manipulation or analysis of more than one changing quantity simultaneously.</p> <p>Multiple variables introduce complexity appropriate for middle school and beyond.</p> <p>Example: A simulation where learners adjust both temperature and pressure to observe effects.</p>"},{"location":"glossary/#network-graph","title":"Network Graph","text":"<p>A visualization showing entities and their connections or relationships as nodes and edges.</p> <p>Network graphs are effective for concepts involving relationships, dependencies, or social structures.</p> <p>Example: A graph showing how characters in a novel are connected to each other.</p>"},{"location":"glossary/#nice-to-have-changes","title":"Nice-to-Have Changes","text":"<p>Improvements to a learning resource that would enhance quality but are not essential for function or objectives.</p> <p>Nice-to-have changes are lower priority than critical changes and may be deferred or dropped.</p> <p>Example: Adding a feature that no user has specifically requested.</p>"},{"location":"glossary/#objective-alignment","title":"Objective Alignment","text":"<p>The correspondence between stated learning objectives and the content, activities, or assessments in a resource.</p> <p>Strong alignment ensures that everything in a MicroSim serves the stated learning goals.</p> <p>Example: Verifying that every interactive element helps learners achieve the specified objective.</p>"},{"location":"glossary/#objective-decomposition","title":"Objective Decomposition","text":"<p>The process of breaking compound learning objectives into their atomic component concepts or skills.</p> <p>Decomposition reveals prerequisites, appropriate sequencing, and points where assessment can occur.</p> <p>Example: Breaking \"design and implement an API\" into separate design and implementation objectives.</p>"},{"location":"glossary/#observation-technique","title":"Observation Technique","text":"<p>A method for gathering information by watching how learners interact with a learning resource.</p> <p>Observational data reveals usability issues and learning behaviors that surveys may miss.</p> <p>Example: Noting where learners pause, show confusion, or request help during testing.</p>"},{"location":"glossary/#output-interpretation","title":"Output Interpretation","text":"<p>The process of reviewing and understanding AI-generated content to evaluate its quality and correctness.</p> <p>Skilled output interpretation enables designers to identify what needs refinement.</p> <p>Example: Reviewing generated code to verify it produces the intended behavior.</p>"},{"location":"glossary/#output-validation","title":"Output Validation","text":"<p>The process of verifying that AI-generated content meets requirements and functions correctly.</p> <p>Output validation catches errors before learners encounter them.</p> <p>Example: Testing a generated simulation with various inputs to confirm correct behavior.</p>"},{"location":"glossary/#p5js-animation","title":"p5.js Animation","text":"<p>Animations created using the p5.js JavaScript library, particularly suited for creative and interactive visual content.</p> <p>p5.js provides an accessible way to create custom animations and interactive visualizations.</p> <p>Example: A particle system demonstrating Brownian motion.</p>"},{"location":"glossary/#paradigm-selection","title":"Paradigm Selection","text":"<p>The process of choosing an appropriate visualization type based on concept characteristics and learning objectives.</p> <p>Matching paradigms to concepts improves learning effectiveness.</p> <p>Example: Choosing a timeline visualization for a history concept rather than a network graph.</p>"},{"location":"glossary/#parameter-space","title":"Parameter Space","text":"<p>The range of possible values for all adjustable parameters in a simulation or model.</p> <p>Understanding parameter space helps learners explore boundaries and edge cases.</p> <p>Example: The full range of possible masses, velocities, and angles in a projectile simulation.</p>"},{"location":"glossary/#pedagogical-evaluation","title":"Pedagogical Evaluation","text":"<p>Assessment of whether a learning resource effectively supports its stated learning objectives.</p> <p>Pedagogical evaluation goes beyond technical function to examine educational effectiveness.</p> <p>Example: Determining whether a simulation actually helps learners understand the targeted concept.</p>"},{"location":"glossary/#peer-feedback","title":"Peer Feedback","text":"<p>Input and critique provided by fellow learners or colleagues rather than instructors.</p> <p>Peer feedback provides multiple perspectives and develops evaluative skills in the reviewers.</p> <p>Example: Learners reviewing each other's MicroSim specifications and suggesting improvements.</p>"},{"location":"glossary/#physics-simulation","title":"Physics Simulation","text":"<p>An interactive visualization that models physical phenomena such as motion, forces, or energy.</p> <p>Physics simulations make invisible forces and relationships visible through animation and interaction.</p> <p>Example: A simulation showing gravitational attraction between objects of different masses.</p>"},{"location":"glossary/#piaget-stages","title":"Piaget Stages","text":"<p>Jean Piaget's theory describing four stages of cognitive development from infancy through adolescence.</p> <p>Piaget's stages inform age-appropriate design decisions for educational content.</p> <p>Example: Designing concrete, hands-on simulations for learners in the concrete operational stage.</p>"},{"location":"glossary/#plotly-library","title":"Plotly Library","text":"<p>A JavaScript library for creating interactive, publication-quality graphs and charts.</p> <p>Plotly offers more sophisticated charting options than simpler libraries, suitable for advanced data visualization.</p> <p>Example: Creating an interactive 3D scatter plot for multivariate data exploration.</p>"},{"location":"glossary/#portfolio-project","title":"Portfolio Project","text":"<p>A comprehensive culminating project that demonstrates mastery of course concepts through a complete design artifact.</p> <p>Portfolios allow learners to integrate and apply skills across multiple areas.</p> <p>Example: Creating a complete MicroSim package including specification, implementation, testing, and documentation.</p>"},{"location":"glossary/#prediction-prompt","title":"Prediction Prompt","text":"<p>An instructional technique that asks learners to predict an outcome before revealing it.</p> <p>Predictions engage learners actively and make misconceptions visible when predictions are incorrect.</p> <p>Example: \"Before clicking Run, predict what will happen when mass is doubled.\"</p>"},{"location":"glossary/#prerequisite-knowledge","title":"Prerequisite Knowledge","text":"<p>Concepts or skills that must be understood before new material can be effectively learned.</p> <p>Identifying prerequisites helps sequence instruction and diagnose learning difficulties.</p> <p>Example: Understanding fractions is prerequisite to learning about ratios.</p>"},{"location":"glossary/#prior-knowledge-support","title":"Prior Knowledge Support","text":"<p>Design features that help learners activate or build the background knowledge needed for new content.</p> <p>Supporting prior knowledge reduces intrinsic load and improves learning outcomes.</p> <p>Example: Including a brief review module before introducing advanced topics.</p>"},{"location":"glossary/#process-timeline","title":"Process Timeline","text":"<p>A visualization showing steps or phases of a process arranged in chronological order.</p> <p>Process timelines help learners understand sequential procedures or historical developments.</p> <p>Example: A timeline showing the stages of the water cycle.</p>"},{"location":"glossary/#productive-failure","title":"Productive Failure","text":"<p>A learning approach that allows learners to struggle with challenging problems before receiving instruction.</p> <p>Productive failure can lead to deeper understanding than immediate correct answers.</p> <p>Example: Letting learners attempt to solve a problem and discover why their approach doesn't work.</p>"},{"location":"glossary/#progressive-disclosure","title":"Progressive Disclosure","text":"<p>A design strategy that reveals information gradually rather than all at once.</p> <p>Progressive disclosure reduces initial cognitive load while allowing access to complexity when needed.</p> <p>Example: A MicroSim that starts with basic features and unlocks advanced options as learners progress.</p>"},{"location":"glossary/#prompt-engineering","title":"Prompt Engineering","text":"<p>The skill of crafting effective instructions and queries for AI systems to produce desired outputs.</p> <p>Effective prompts are specific, provide context, and anticipate how AI will interpret the request.</p> <p>Example: Including concrete examples in a prompt to guide AI output style.</p>"},{"location":"glossary/#reading-support","title":"Reading Support","text":"<p>Design features that help learners with developing reading skills access text content.</p> <p>Reading support may include text-to-speech, simplified vocabulary, or visual aids for text.</p> <p>Example: Icons accompanying menu items so learners don't rely solely on reading.</p>"},{"location":"glossary/#real-world-application","title":"Real-World Application","text":"<p>The use of authentic, practical contexts to demonstrate how concepts apply outside educational settings.</p> <p>Real-world applications increase motivation and support transfer of learning.</p> <p>Example: A simulation using actual economic data rather than simplified hypothetical scenarios.</p>"},{"location":"glossary/#reduced-motion","title":"Reduced Motion","text":"<p>A design accommodation that minimizes or eliminates animation for users who are sensitive to motion.</p> <p>Operating systems provide reduced motion preferences that well-designed MicroSims should respect.</p> <p>Example: Providing a static alternative for users who have enabled reduced motion settings.</p>"},{"location":"glossary/#refinement-prompt","title":"Refinement Prompt","text":"<p>A follow-up instruction to an AI system that requests specific modifications to previously generated content.</p> <p>Effective refinement prompts are specific about what should change and why.</p> <p>Example: \"Change the graph colors to be more distinct for colorblind users.\"</p>"},{"location":"glossary/#reflection-journal","title":"Reflection Journal","text":"<p>A document where learners record thoughts, questions, and insights about their learning process.</p> <p>Journaling supports metacognition and self-regulated learning.</p> <p>Example: Documenting design decisions and lessons learned during a portfolio project.</p>"},{"location":"glossary/#regeneration-decision","title":"Regeneration Decision","text":"<p>The choice between asking an AI to generate new content versus manually editing existing output.</p> <p>Regeneration is appropriate for substantial changes; manual editing is faster for small fixes.</p> <p>Example: Regenerating when the fundamental approach is wrong; editing when only values need adjustment.</p>"},{"location":"glossary/#relationship-graph","title":"Relationship Graph","text":"<p>A visualization showing connections between entities, emphasizing how things are related to each other.</p> <p>Relationship graphs help learners understand networks, dependencies, and social structures.</p> <p>Example: A graph showing trade relationships between countries.</p>"},{"location":"glossary/#remember-level","title":"Remember Level","text":"<p>The first and foundational level of Bloom's Taxonomy involving retrieving knowledge from long-term memory.</p> <p>Remembering provides the foundation for higher-order thinking but is not sufficient on its own.</p> <p>Example: A learning objective at this level: \"List the six levels of Bloom's Taxonomy in order.\"</p>"},{"location":"glossary/#research-applications","title":"Research Applications","text":"<p>Uses of concepts in formal research contexts, typically at graduate or professional levels.</p> <p>Research applications extend beyond standard use cases to cutting-edge exploration.</p> <p>Example: Using a simulation to test theoretical predictions in a research study.</p>"},{"location":"glossary/#responsiveness-testing","title":"Responsiveness Testing","text":"<p>Evaluation of how a learning resource adapts to different screen sizes and device types.</p> <p>Responsive design ensures accessibility across desktops, tablets, and mobile phones.</p> <p>Example: Testing a MicroSim on various screen sizes to verify layout adjusts appropriately.</p>"},{"location":"glossary/#reusability","title":"Reusability","text":"<p>The quality of a learning resource that enables it to be used in multiple contexts without modification.</p> <p>High reusability increases return on development investment and promotes consistency.</p> <p>Example: A well-documented MicroSim that can be embedded in different courses.</p>"},{"location":"glossary/#rubric-development","title":"Rubric Development","text":"<p>The process of creating structured evaluation guides with explicit criteria and performance levels.</p> <p>Well-developed rubrics ensure consistent, fair, and transparent assessment.</p> <p>Example: Creating a rubric for evaluating MicroSim specifications before generation.</p>"},{"location":"glossary/#scaffolded-complexity","title":"Scaffolded Complexity","text":"<p>An approach that gradually increases difficulty or complexity as learners demonstrate mastery.</p> <p>Scaffolding supports learners through challenging content without overwhelming them.</p> <p>Example: A simulation that adds new variables only after learners master simpler configurations.</p>"},{"location":"glossary/#schema-formation","title":"Schema Formation","text":"<p>The process of building organized knowledge structures in long-term memory.</p> <p>Schemas enable efficient processing by chunking related information together.</p> <p>Example: Developing a mental framework for understanding different types of graphs.</p>"},{"location":"glossary/#scope-creep-prevention","title":"Scope Creep Prevention","text":"<p>Practices that prevent unplanned expansion of a project beyond its original goals.</p> <p>Preventing scope creep ensures resources are focused on priority features.</p> <p>Example: Maintaining a strict list of requirements and deferring \"nice-to-have\" features.</p>"},{"location":"glossary/#screen-reader-support","title":"Screen Reader Support","text":"<p>Design features that enable users of screen reading software to access and navigate content.</p> <p>Proper screen reader support includes semantic markup, alternative text, and logical reading order.</p> <p>Example: Adding descriptive alt text to all images and ensuring controls are properly labeled.</p>"},{"location":"glossary/#self-evaluation","title":"Self-Evaluation","text":"<p>The process of learners assessing their own work against specified criteria.</p> <p>Self-evaluation develops metacognitive skills and promotes self-regulated learning.</p> <p>Example: Using a rubric to rate your own MicroSim before peer review.</p>"},{"location":"glossary/#sequence-display","title":"Sequence Display","text":"<p>A visualization showing items arranged in a meaningful order, often temporal or procedural.</p> <p>Sequence displays help learners understand steps, phases, or progressions.</p> <p>Example: A timeline showing the order of steps in a laboratory procedure.</p>"},{"location":"glossary/#set-visualization","title":"Set Visualization","text":"<p>A graphical representation showing collections of items and their membership in categories.</p> <p>Set visualizations like Venn diagrams help learners understand classification and overlap.</p> <p>Example: A Venn diagram showing the intersection of mammals and aquatic animals.</p>"},{"location":"glossary/#simple-cause-effect","title":"Simple Cause-Effect","text":"<p>A straightforward relationship where one action leads to one predictable outcome.</p> <p>Simple cause-effect relationships are appropriate for early childhood and elementary learners.</p> <p>Example: \"Press the button, the light turns on.\"</p>"},{"location":"glossary/#simulation-readiness","title":"Simulation Readiness","text":"<p>The degree to which a learning objective is appropriate for delivery through interactive simulation.</p> <p>Not all objectives benefit from simulation; some are better served by other methods.</p> <p>Example: Objectives involving dynamic processes or exploration are more simulation-ready than factual recall.</p>"},{"location":"glossary/#spatial-visualization","title":"Spatial Visualization","text":"<p>A representation showing physical arrangement, location, or geographic relationships.</p> <p>Spatial visualizations help learners understand concepts involving position, distance, or geographic context.</p> <p>Example: A map showing the locations of historical events.</p>"},{"location":"glossary/#specification-ambiguity","title":"Specification Ambiguity","text":"<p>Unclear or imprecise language in a MicroSim specification that could be interpreted multiple ways.</p> <p>Ambiguity leads to AI generating output that doesn't match designer intent.</p> <p>Example: \"Make it colorful\" is ambiguous; \"Use blue, red, and yellow with high contrast\" is specific.</p>"},{"location":"glossary/#specification-document","title":"Specification Document","text":"<p>A detailed written description of what a MicroSim should contain, how it should behave, and what it should teach.</p> <p>Complete specifications include learning objectives, visual descriptions, interactions, and success criteria.</p> <p>Example: A multi-page document describing a physics simulation in enough detail for AI generation.</p>"},{"location":"glossary/#split-attention-effect","title":"Split Attention Effect","text":"<p>The increased cognitive load that occurs when learners must mentally integrate information from separate sources.</p> <p>Avoiding split attention by integrating related information improves learning efficiency.</p> <p>Example: Placing labels directly on diagram elements rather than in a separate legend.</p>"},{"location":"glossary/#state-machine-diagram","title":"State Machine Diagram","text":"<p>A visualization showing the possible states of a system and the transitions between them.</p> <p>State diagrams help learners understand systems with discrete conditions and rules for changing between them.</p> <p>Example: A diagram showing the states of water (solid, liquid, gas) and transition conditions.</p>"},{"location":"glossary/#success-criteria","title":"Success Criteria","text":"<p>The specific conditions that define whether a learning resource or learning activity has achieved its goals.</p> <p>Clear success criteria enable objective evaluation of outcomes.</p> <p>Example: \"Learners will correctly identify the equilibrium point in 8 out of 10 trials.\"</p>"},{"location":"glossary/#technical-evaluation","title":"Technical Evaluation","text":"<p>Assessment of whether a learning resource functions correctly from an engineering perspective.</p> <p>Technical evaluation addresses bugs, performance, compatibility, and responsiveness.</p> <p>Example: Testing that a simulation runs without errors on target browsers and devices.</p>"},{"location":"glossary/#template-library","title":"Template Library","text":"<p>A collection of reusable patterns or starting points for creating new learning resources.</p> <p>Templates speed development and promote consistency across resources.</p> <p>Example: A library of common MicroSim interaction patterns that can be adapted for new content.</p>"},{"location":"glossary/#test-interpretation","title":"Test Interpretation","text":"<p>The process of analyzing results from user testing to draw meaningful conclusions.</p> <p>Skillful interpretation distinguishes between individual quirks and systematic usability issues.</p> <p>Example: Recognizing that multiple testers struggling with the same feature indicates a design problem.</p>"},{"location":"glossary/#theoretical-foundations","title":"Theoretical Foundations","text":"<p>The underlying principles, models, or frameworks that explain phenomena at a conceptual level.</p> <p>Theoretical understanding is typically emphasized at undergraduate and graduate levels.</p> <p>Example: Understanding why the laws of thermodynamics constrain possible engine designs.</p>"},{"location":"glossary/#think-aloud-protocol","title":"Think-Aloud Protocol","text":"<p>A research method where participants verbalize their thoughts while completing a task.</p> <p>Think-aloud protocols reveal cognitive processes and usability issues not visible from behavior alone.</p> <p>Example: Asking a learner to describe what they're thinking while using a MicroSim.</p>"},{"location":"glossary/#time-efficient-design","title":"Time-Efficient Design","text":"<p>Design approaches that respect learners' limited time by focusing on essential content and minimizing inefficiencies.</p> <p>Time efficiency is particularly important for corporate training where opportunity costs are high.</p> <p>Example: Removing optional content that doesn't contribute to core learning objectives.</p>"},{"location":"glossary/#timeline-visualization","title":"Timeline Visualization","text":"<p>A graphical representation showing events or data points arranged along a temporal axis.</p> <p>Timelines help learners understand chronology, duration, and temporal relationships.</p> <p>Example: A timeline showing the sequence of events leading to World War I.</p>"},{"location":"glossary/#touch-target-size","title":"Touch Target Size","text":"<p>The physical dimensions of interactive elements on touch screens, affecting usability and accessibility.</p> <p>Larger touch targets reduce errors and accommodate users with motor difficulties.</p> <p>Example: Making buttons at least 44x44 pixels for comfortable touch interaction.</p>"},{"location":"glossary/#trend-chart","title":"Trend Chart","text":"<p>A visualization showing how values change over time, revealing patterns, cycles, or directions.</p> <p>Trend charts help learners understand temporal patterns and make predictions.</p> <p>Example: A line chart showing stock price movements over a year.</p>"},{"location":"glossary/#udl-principles","title":"UDL Principles","text":"<p>The three principles of Universal Design for Learning: multiple means of engagement, representation, and action/expression.</p> <p>UDL provides a framework for creating learning experiences accessible to diverse learners.</p> <p>Example: Offering content in both text and audio formats to support different learning preferences.</p>"},{"location":"glossary/#undergraduate-design","title":"Undergraduate Design","text":"<p>Design approaches tailored for undergraduate-level learners, featuring theoretical foundations and mathematical relationships.</p> <p>Undergraduate designs assume more prior knowledge and can include more complex interactions.</p> <p>Example: A simulation requiring manipulation of mathematical equations to explore relationships.</p>"},{"location":"glossary/#understand-level","title":"Understand Level","text":"<p>The second level of Bloom's Taxonomy involving constructing meaning from instructional messages.</p> <p>Understanding goes beyond recall to demonstrate comprehension through explanation, classification, or comparison.</p> <p>Example: A learning objective at this level: \"Explain the difference between formative and summative assessment.\"</p>"},{"location":"glossary/#universal-design","title":"Universal Design","text":"<p>An approach to design that creates products and environments usable by all people without need for adaptation.</p> <p>Universal design benefits everyone, not just those with identified disabilities.</p> <p>Example: Captioning videos benefits deaf users but also those in noisy environments or learning new languages.</p>"},{"location":"glossary/#ux-evaluation","title":"UX Evaluation","text":"<p>Assessment of the user experience aspects of a learning resource, including usability, satisfaction, and engagement.</p> <p>UX evaluation ensures that learners can use the resource effectively and enjoyably.</p> <p>Example: Evaluating whether learners can navigate a MicroSim without confusion or frustration.</p>"},{"location":"glossary/#venn-diagram","title":"Venn Diagram","text":"<p>A diagram using overlapping circles to show logical relationships between sets or categories.</p> <p>Venn diagrams help learners understand inclusion, exclusion, and intersection of categories.</p> <p>Example: A diagram showing which animals are both mammals and aquatic.</p>"},{"location":"glossary/#version-control","title":"Version Control","text":"<p>The practice of tracking and managing changes to files, code, or content over time.</p> <p>Version control enables collaboration, rollback of problematic changes, and documentation of evolution.</p> <p>Example: Using Git to track changes to MicroSim code and specifications.</p>"},{"location":"glossary/#vis-network-library","title":"vis-network Library","text":"<p>A JavaScript library for creating interactive network graphs with nodes and edges.</p> <p>vis-network is well-suited for MicroSims involving relationship visualization or dependency mapping.</p> <p>Example: Creating an interactive concept map where learners can explore connections.</p>"},{"location":"glossary/#vis-timeline-library","title":"vis-timeline Library","text":"<p>A JavaScript library for creating interactive timelines with events and ranges.</p> <p>vis-timeline is appropriate for MicroSims involving historical sequences or project scheduling.</p> <p>Example: An interactive timeline of the Civil Rights Movement.</p>"},{"location":"glossary/#visual-affordances","title":"Visual Affordances","text":"<p>Design properties that suggest how an element can be used or interacted with.</p> <p>Clear affordances reduce the need for explicit instructions and lower extraneous load.</p> <p>Example: A button that appears raised suggests it can be clicked.</p>"},{"location":"glossary/#visual-description","title":"Visual Description","text":"<p>Written explanation of what visual elements should appear and how they should look.</p> <p>Detailed visual descriptions enable AI to generate designs matching designer intent.</p> <p>Example: Specifying \"a blue circle 50 pixels in diameter centered in the canvas.\"</p>"},{"location":"glossary/#visual-simplicity","title":"Visual Simplicity","text":"<p>A design quality characterized by minimal visual elements, each serving a clear purpose.</p> <p>Visual simplicity reduces extraneous cognitive load and focuses attention on learning content.</p> <p>Example: Removing decorative graphics that don't contribute to learning.</p>"},{"location":"glossary/#visualization-paradigm","title":"Visualization Paradigm","text":"<p>A general category of visual representation defined by its characteristic structure and affordances.</p> <p>Different paradigms suit different types of concepts and learning objectives.</p> <p>Example: Network graphs, timelines, and charts are different visualization paradigms.</p>"},{"location":"glossary/#vocabulary-level","title":"Vocabulary Level","text":"<p>The complexity of language used in educational content, matched to the target audience.</p> <p>Appropriate vocabulary reduces unnecessary cognitive load from unknown words.</p> <p>Example: Using \"speed\" for elementary learners and \"velocity\" for high school physics.</p>"},{"location":"glossary/#vygotsky-theory","title":"Vygotsky Theory","text":"<p>Lev Vygotsky's theory emphasizing social interaction and the zone of proximal development in learning.</p> <p>Vygotsky's ideas inform scaffolding approaches and peer learning designs.</p> <p>Example: Providing support that enables learners to accomplish more than they could independently.</p>"},{"location":"glossary/#working-memory","title":"Working Memory","text":"<p>The cognitive system for temporarily holding and manipulating information during mental tasks.</p> <p>Working memory has limited capacity (4-7 items) and serves as the bottleneck in learning.</p> <p>Example: Holding multiple numbers in mind while performing mental arithmetic.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"license/#commercial-licensing","title":"Commercial Licensing","text":"<p>Commercial rights are reserved by the copyright holder. For commercial licensing, publication inquiries, or permission to use this work in commercial contexts, please contact Dan McCreary on LinkedIn.</p>"},{"location":"references/","title":"References","text":"<p>This textbook draws upon the following high-quality resources organized by topic.</p>"},{"location":"references/#microsims-and-ai-assisted-education","title":"MicroSims and AI-Assisted Education","text":"<ol> <li> <p>MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support - 2025-11-25 - arXiv - Presents a framework for rapidly creating lightweight, interactive educational simulations using AI, enabling educators to generate customized learning tools without programming expertise.</p> </li> <li> <p>Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities - 2025-09-10 - arXiv - Explores how AI tools can be deployed in technical education, addressing challenges and opportunities in signal processing curriculum development.</p> </li> <li> <p>Five Levels of Intelligent Textbooks - 2024 - Medium - Describes the progression from static digital textbooks to fully adaptive intelligent learning systems with embedded interactivity and personalization.</p> </li> </ol>"},{"location":"references/#blooms-taxonomy-and-learning-objectives","title":"Bloom's Taxonomy and Learning Objectives","text":"<ol> <li> <p>Bloom's Taxonomy of Cognitive Learning Objectives - 2015 - PMC/NIH - Comprehensive overview of Bloom's cognitive taxonomy framework for classifying educational objectives, widely used in medical and STEM education.</p> </li> <li> <p>Using Bloom's Taxonomy to Write Effective Learning Objectives - 2024 - University of Arkansas - Practical guide for educators on crafting measurable learning objectives aligned with cognitive complexity levels.</p> </li> <li> <p>Bloom's Taxonomy of Educational Objectives - 2024 - University of Illinois Chicago - Explains the revised taxonomy (2001) with action verbs and guidance for curriculum design.</p> </li> <li> <p>Bloom's Taxonomy - 2024 - Britannica - Authoritative encyclopedia entry on the history and application of Bloom's educational framework.</p> </li> </ol>"},{"location":"references/#cognitive-load-theory-and-multimedia-learning","title":"Cognitive Load Theory and Multimedia Learning","text":"<ol> <li> <p>Cognitive Load Theory: Implications for Instructional Design in Digital Classrooms - 2024 - International Journal of Educational Narratives - Research on managing cognitive load in digital learning environments through proper multimedia integration.</p> </li> <li> <p>Enhancing the Cognitive Load Theory and Multimedia Learning Framework with AI Insight - 2025 - Discover Education/Springer - Novel framework integrating AI with CLT and Cognitive Theory of Multimedia Learning for adaptive eLearning systems.</p> </li> <li> <p>Cognitive Load Theory and Instructional Design - University of Kentucky - Foundation document explaining intrinsic, extraneous, and germane cognitive load with design implications.</p> </li> <li> <p>Cognitive Load Theory and eLearning Industry - 2024 - eLearning Industry - Practical application of CLT principles for digital course designers and instructional developers.</p> </li> </ol>"},{"location":"references/#interactive-simulations-in-education","title":"Interactive Simulations in Education","text":"<ol> <li> <p>PhET Interactive Simulations - University of Colorado Boulder - Nobel Laureate Carl Wieman's project providing over 125 free research-based interactive simulations for STEM education, used over 100 million times annually.</p> </li> <li> <p>Digital Simulations in STEM Education: Insights from Recent Empirical Studies - 2025 - MDPI - Systematic review of 31 studies showing interactive simulations as the most widely used digital tool in STEM education.</p> </li> <li> <p>Simulations for STEM Learning: Systematic Review and Meta-Analysis - SRI International - Meta-analysis demonstrating simulations can improve conceptual understanding by 30-40% compared to traditional instruction.</p> </li> <li> <p>Transforming STEM Learning at Scale: PhET Interactive Simulations - 2020 - ERIC/Childhood Education - Documents how PhET simulations have proven uniquely transformative for STEM education through hands-on simulated experimentation.</p> </li> </ol>"},{"location":"references/#universal-design-for-learning-and-accessibility","title":"Universal Design for Learning and Accessibility","text":"<ol> <li> <p>CAST Universal Design for Learning Guidelines 3.0 - 2024-07-30 - CAST - Updated UDL framework with nine guidelines and 36 actionable considerations emphasizing learner agency and inclusion.</p> </li> <li> <p>Universal Design for Learning (UDL) in Inclusive Education - 2024 - ResearchGate - Research demonstrating UDL's impact on academic achievement and social-emotional development for diverse learners.</p> </li> <li> <p>WCAG 2 Overview - W3C Web Accessibility Initiative - Official documentation on Web Content Accessibility Guidelines 2.1/2.2 technical standards.</p> </li> <li> <p>ADA Digital Accessibility Rule for Education - 2024-04-24 - ADA.gov - Federal requirements for web content and mobile app accessibility in educational institutions, requiring WCAG 2.1 AA compliance by 2026.</p> </li> </ol>"},{"location":"references/#visualization-libraries-and-tools","title":"Visualization Libraries and Tools","text":"<ol> <li> <p>p5.js - Processing Foundation - Friendly JavaScript library for creative coding and visualization, with extensive educational resources and accessible design.</p> </li> <li> <p>p5.js Education Resources - Processing Foundation - Curriculum guides, tutorials, and teaching materials for using p5.js in educational settings.</p> </li> <li> <p>vis.js Network Documentation - vis.js - Complete documentation for the vis-network library used to display dynamic, automatically organized network visualizations.</p> </li> <li> <p>Chart.js Documentation - Chart.js - Official documentation for the popular JavaScript charting library supporting line, bar, radar, pie, and other chart types.</p> </li> <li> <p>Mermaid: Generation of Diagrams from Text - GitHub/Mermaid - JavaScript-based diagramming tool using Markdown-inspired syntax for flowcharts, sequence diagrams, and more.</p> </li> </ol>"},{"location":"references/#learning-analytics-and-xapi","title":"Learning Analytics and xAPI","text":"<ol> <li> <p>Experience API (xAPI) Standard - ADL Initiative - Official xAPI specification from the Advanced Distributed Learning Initiative, now IEEE Standard 9274.1.1-2023.</p> </li> <li> <p>xAPI Overview and Getting Started - xAPI.com - Comprehensive introduction to the Experience API for tracking learning experiences beyond traditional course completions.</p> </li> <li> <p>Learning Analytics with xAPI and LRS - Lambda Solutions - Explains how Learning Record Stores work with xAPI statements to enable comprehensive learning analytics.</p> </li> </ol>"},{"location":"references/#knowledge-graphs-and-ai-in-education","title":"Knowledge Graphs and AI in Education","text":"<ol> <li> <p>ACE: AI-Assisted Construction of Educational Knowledge Graphs with Prerequisite Relations - 2024 - Journal of Educational Data Mining - Research showing students studying in AI-determined prerequisite order have better success rates.</p> </li> <li> <p>Exploring Knowledge Graphs for Identification of Concept Prerequisites - Smart Learning Environments/Springer - Methodology using knowledge graphs to identify concept prerequisites with 76-96% precision.</p> </li> <li> <p>10 Ways AI Is Transforming Instructional Design - 2023 - EDUCAUSE Review - Overview of how AI tools are reshaping course design, content generation, and personalized learning pathways.</p> </li> </ol>"},{"location":"references/#assessment-and-quiz-design","title":"Assessment and Quiz Design","text":"<ol> <li> <p>How to Use Multiple Choice Questions for Formative Assessment - Royal Society of Chemistry Education - Best practices for designing MCQs that support learning through effective feedback and misconception detection.</p> </li> <li> <p>Formative and Summative Automated Assessment with Multiple-Choice Question Banks - 2024 - Journal of Chemical Education - Research on automated assessment quality demands and alignment with learning outcomes.</p> </li> </ol>"},{"location":"references/#site-building-references","title":"Site Building References","text":"<ol> <li> <p>MkDocs - MkDocs - Tool for building the website, converting Markdown into HTML.</p> </li> <li> <p>MkDocs Material Theme - Squidfunk - Theme providing user interface elements, styling, and features like social cards.</p> </li> <li> <p>GitHub Pages - GitHub - Free tool for hosting public websites created by MkDocs.</p> </li> </ol> <p>References last updated: December 18, 2025</p>"},{"location":"chapters/","title":"Chapters","text":"<p>This textbook is organized into 12 chapters covering 200 concepts.</p>"},{"location":"chapters/#chapter-overview","title":"Chapter Overview","text":"<ol> <li> <p>Foundations of Learning Objective Analysis - Covers Bloom's Taxonomy, cognitive complexity levels, learning objective classification, and the framework for analyzing educational objectives.</p> </li> <li> <p>Prerequisite Analysis and MicroSim Fundamentals - Introduces MicroSims, interactive simulations, prerequisite knowledge identification, concept dependencies, and simulation readiness assessment.</p> </li> <li> <p>The MicroSim Pattern Library - Comprehensive coverage of visualization paradigms including motion, physics, dynamic systems, relationships, hierarchies, sequences, charts, spatial displays, and classification visuals.</p> </li> <li> <p>Visualization Libraries and Tools - Covers p5.js, vis-network, vis-timeline, Chart.js, Plotly, Leaflet, Mermaid, and Claude Code skills including the MicroSim generator.</p> </li> <li> <p>Writing Effective MicroSim Specifications - Addresses specification documents, visual descriptions, interaction behaviors, constraints, success criteria, edge cases, and intent preservation.</p> </li> <li> <p>Adapting for Audience Levels - Covers cognitive development theory and design considerations for early childhood through graduate and corporate training contexts.</p> </li> <li> <p>Cognitive Load and Visual Design - Explores working memory, long-term memory, schema formation, cognitive load theory, split attention, progressive disclosure, and design tradeoffs.</p> </li> <li> <p>Anticipating Misconceptions - Addresses mental models, common misconceptions, correction strategies, productive failure, prediction prompts, and model comparison techniques.</p> </li> <li> <p>Generating MicroSims with AI Tools - Covers prompt engineering, refinement prompts, generation workflow, output interpretation, and version control.</p> </li> <li> <p>Quality Evaluation Frameworks - Addresses technical, pedagogical, and UX evaluation; testing methods; rubric development; and documentation standards.</p> </li> <li> <p>User Testing and Iteration - Covers think-aloud protocols, A/B testing, learner feedback analysis, and the design-test-refine cycle.</p> </li> <li> <p>Accessibility, Deployment, and Course Completion - Covers universal design, UDL principles, LMS integration, analytics, maintenance planning, and portfolio assessment.</p> </li> <li> <p>Capstone Projects - Guidance in creating a project where you work in teams to build a portion of an intelligent textbook with MicroSims.</p> </li> </ol>"},{"location":"chapters/#how-to-use-this-textbook","title":"How to Use This Textbook","text":"<p>This textbook follows a carefully designed learning progression where each chapter builds on concepts from previous chapters. The dependency structure ensures that prerequisite knowledge is always introduced before it is needed. We recommend completing chapters in order, especially for your first reading, as later chapters assume familiarity with earlier material.</p> <p>Each chapter includes a list of concepts covered and identifies which previous chapters contain prerequisite material. Use these guides to create a personalized learning path if you need to focus on specific topics.</p> <p>Note: Each chapter includes a list of concepts covered. Make sure to complete prerequisites before moving to advanced chapters.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/","title":"Foundations of Learning Objective Analysis","text":""},{"location":"chapters/01-foundations-learning-objective-analysis/#summary","title":"Summary","text":"<p>This chapter establishes the foundational knowledge needed for instructional design and learning objective analysis. You will learn about Bloom's Taxonomy and its six cognitive complexity levels, understand how to classify learning objectives by cognitive demand, and master the use of action verbs to create measurable outcomes. By the end of this chapter, you will be able to analyze learning objectives systematically and identify their cognitive complexity level, setting the stage for designing effective MicroSims.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 17 concepts from the learning graph:</p> <ol> <li>Instructional Design</li> <li>Learning Objective</li> <li>Educational Technology</li> <li>Bloom's Taxonomy</li> <li>Cognitive Complexity</li> <li>Remember Level</li> <li>Understand Level</li> <li>Apply Level</li> <li>Analyze Level</li> <li>Evaluate Level</li> <li>Create Level</li> <li>Action Verbs</li> <li>Measurable Outcomes</li> <li>Learning Outcome</li> <li>Objective Decomposition</li> <li>Atomic Concepts</li> <li>Compound Objectives</li> </ol>"},{"location":"chapters/01-foundations-learning-objective-analysis/#prerequisites","title":"Prerequisites","text":"<p>This chapter assumes only the prerequisites listed in the course description. No prior chapters are required.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#welcome-to-the-ai-powered-future-of-education","title":"Welcome to the AI-Powered Future of Education","text":"<p>Here's a mind-bending statistic to kick things off: according to research from METR.org, AI capabilities are doubling every seven months when we measure the probability that large language models and AI agents will correctly complete tasks of a given complexity. Let that sink in. By the time you finish this course, the AI tools available to you will be roughly twice as capable as they were when you started reading this sentence. (Okay, maybe not that fast, but you get the point.)</p> <p>This exponential growth means we're living through the most exciting time in the history of educational technology. AI can now assist with tasks that would have seemed like science fiction just a few years ago\u2014generating interactive simulations, adapting content to individual learners, and creating personalized feedback at scale. As instructional designers, we have access to superpowers our predecessors couldn't have dreamed of.</p> <p>But here's the catch\u2014and it's a big one: AI is only as good as the instructions we give it.</p> <p>An AI that doesn't understand how humans learn is like a GPS without maps. Sure, it can process information incredibly fast, but it'll confidently direct you into a lake. To harness AI effectively for education, we need to master the fundamentals of instructional design. We need to understand what makes a learning objective good, how cognitive complexity affects learning, and why some educational experiences stick while others slide right off the brain like water off a duck.</p> <p>That's what this chapter is all about. Consider it your instruction manual for giving instructions. (How meta is that?)</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#what-is-instructional-design","title":"What Is Instructional Design?","text":"<p>Instructional design is the systematic process of creating educational experiences that make learning efficient, effective, and\u2014dare we say\u2014enjoyable. It's part art, part science, and part detective work. Instructional designers analyze what learners need to know, design experiences that bridge the gap between current and desired knowledge, and evaluate whether those experiences actually worked.</p> <p>Think of instructional designers as architects of the mind. Just as a building architect wouldn't start construction without blueprints, an instructional designer doesn't create learning materials without a clear plan. The blueprints of instructional design are called learning objectives\u2014and they're so important that we're going to spend most of this chapter talking about them.</p> Aspect Building Architecture Instructional Design Blueprint Floor plans &amp; elevations Learning objectives Foundation Concrete &amp; steel Prerequisite knowledge Structure Walls &amp; supports Content organization Inhabitants People living/working there Learners Success metric Building passes inspection Learners achieve objectives <p>The field of instructional design has evolved dramatically since its origins in World War II training programs. Early approaches focused on breaking down complex military tasks into teachable components. Today, we apply these same principles to everything from kindergarten reading programs to graduate-level quantum physics courses\u2014and increasingly, we have AI assistants helping us do it better and faster.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#educational-technology-our-toolkit-for-the-21st-century","title":"Educational Technology: Our Toolkit for the 21st Century","text":"<p>Educational technology (often shortened to \"EdTech\") encompasses all the tools, systems, and digital resources we use to facilitate learning. This includes learning management systems (LMS), interactive simulations, video platforms, assessment tools, and\u2014most relevant to this course\u2014MicroSims.</p> <p>The integration of technology into education isn't just about making things flashier or more convenient. When done well, educational technology can:</p> <ul> <li>Personalize learning experiences for individual needs</li> <li>Scale high-quality instruction to reach more learners</li> <li>Visualize abstract concepts that are hard to explain with words alone</li> <li>Provide immediate feedback so learners know where they stand</li> <li>Track progress to identify struggling students before they fall too far behind</li> <li>Enable active learning through interactive experiences</li> </ul> <p>When done poorly, educational technology becomes expensive digital distraction\u2014all sizzle, no steak. The difference between effective and ineffective EdTech often comes down to whether it's grounded in solid instructional design principles.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#diagram-educational-technology-ecosystem","title":"Diagram: Educational Technology Ecosystem","text":"Educational Technology Ecosystem <p>Type: diagram</p> <p>Purpose: Show the interconnected components of modern educational technology and how they serve different aspects of the learning experience</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Students will be able to identify the major categories of educational technology and explain how they interconnect to support learning.</p> <p>Components to show: - Center: \"Learner\" (represented as a person icon) - Inner ring (Direct Learning Tools):   - MicroSims (interactive simulations)   - Video Content   - Reading Materials   - Practice Exercises - Middle ring (Support Systems):   - Learning Management System (LMS)   - Assessment Tools   - Communication Platforms   - Content Authoring Tools - Outer ring (Infrastructure):   - Cloud Services   - Analytics Platforms   - AI/ML Services   - Accessibility Tools</p> <p>Connections: - All inner ring elements connect to center (Learner) - Middle ring elements connect to relevant inner ring tools - Outer ring provides foundation for middle ring - Bidirectional arrows showing data flow</p> <p>Visual Style: Concentric circles with icons for each component Color Scheme: - Inner ring: Bright educational blue - Middle ring: Supportive green - Outer ring: Infrastructure gray - Center: Warm orange for the learner</p> <p>Interactive Features: - Hover over any component to see description - Click to see examples of tools in that category</p> <p>Implementation: HTML/CSS/JavaScript with SVG graphics, responsive design</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#learning-objectives-the-north-star-of-instruction","title":"Learning Objectives: The North Star of Instruction","text":"<p>A learning objective is a clear, specific statement that describes what learners will be able to do after completing an instructional experience. Notice the emphasis on do\u2014good learning objectives focus on observable, measurable behaviors, not vague internal states.</p> <p>Here's the difference:</p> Vague (Not a Good Objective) Clear (Good Objective) Students will understand gravity Students will predict the trajectory of a falling object given its initial velocity Learners will appreciate Shakespeare Learners will analyze how Shakespeare uses metaphor to develop character Participants will know about databases Participants will design a normalized database schema for a given business scenario <p>The left column describes feelings or mental states that can't be directly observed. How do you know if someone \"appreciates\" something? The right column describes specific actions that can be demonstrated and measured.</p> <p>The Magic Question</p> <p>When writing a learning objective, ask yourself: \"How would I know if a student achieved this?\" If you can't describe a concrete way to assess it, the objective needs work.</p> <p>Well-crafted learning objectives serve multiple masters:</p> <ul> <li>For learners: They provide clear expectations and help focus study efforts</li> <li>For instructors: They guide content selection and assessment design</li> <li>For AI tools: They provide precise specifications for generating appropriate content</li> <li>For organizations: They ensure training programs deliver measurable results</li> </ul>"},{"location":"chapters/01-foundations-learning-objective-analysis/#blooms-taxonomy-a-framework-for-cognitive-complexity","title":"Bloom's Taxonomy: A Framework for Cognitive Complexity","text":"<p>In 1956, educational psychologist Benjamin Bloom and his colleagues published a framework for categorizing educational objectives that would become one of the most influential ideas in instructional design. The original Bloom's Taxonomy was revised in 2001 by a team led by Lorin Anderson (one of Bloom's students) and David Krathwohl, updating the categories and changing nouns to verbs to emphasize the active nature of learning.</p> <p>The revised taxonomy identifies six levels of cognitive complexity, arranged from simplest to most demanding:</p> <ol> <li>Remember \u2013 Retrieving relevant knowledge from long-term memory</li> <li>Understand \u2013 Constructing meaning from instructional messages</li> <li>Apply \u2013 Carrying out or using a procedure in a given situation</li> <li>Analyze \u2013 Breaking material into parts and detecting relationships</li> <li>Evaluate \u2013 Making judgments based on criteria and standards</li> <li>Create \u2013 Putting elements together to form a novel, coherent whole</li> </ol> <p>Think of these levels as a cognitive staircase. Each level builds on the ones below it. You can't truly analyze something you don't understand, and you can't understand something you don't remember. This hierarchical structure has profound implications for instructional design\u2014we need to ensure learners have climbed the lower stairs before asking them to tackle the higher ones.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#diagram-blooms-taxonomy-pyramid","title":"Diagram: Bloom's Taxonomy Pyramid","text":"Bloom's Taxonomy Pyramid <p>Type: infographic</p> <p>Purpose: Visualize the six levels of Bloom's Taxonomy as a hierarchical pyramid showing progression from lower-order to higher-order thinking skills</p> <p>Bloom Taxonomy Level: Remember, Understand</p> <p>Learning Objective: Students will be able to list the six levels of Bloom's Taxonomy in order and explain the hierarchical relationship between them.</p> <p>Layout: Pyramid/triangle shape divided into six horizontal sections</p> <p>Sections (bottom to top): 1. Remember (bottom, largest section)    - Color: Light blue (#E3F2FD)    - Keywords: recall, list, define, identify, name    - Icon: Brain with retrieval arrow</p> <ol> <li>Understand</li> <li>Color: Light green (#E8F5E9)</li> <li>Keywords: explain, summarize, interpret, classify</li> <li> <p>Icon: Lightbulb</p> </li> <li> <p>Apply</p> </li> <li>Color: Light yellow (#FFF9C4)</li> <li>Keywords: use, execute, implement, solve</li> <li> <p>Icon: Gear/cog</p> </li> <li> <p>Analyze</p> </li> <li>Color: Light orange (#FFE0B2)</li> <li>Keywords: differentiate, organize, attribute, compare</li> <li> <p>Icon: Magnifying glass</p> </li> <li> <p>Evaluate</p> </li> <li>Color: Light pink (#FCE4EC)</li> <li>Keywords: check, critique, judge, justify</li> <li> <p>Icon: Balance scale</p> </li> <li> <p>Create (top, smallest section)</p> </li> <li>Color: Light purple (#E1BEE7)</li> <li>Keywords: design, construct, produce, invent</li> <li>Icon: Star/lightbulb with sparkles</li> </ol> <p>Side annotations: - Left side: Arrow labeled \"Lower-Order Thinking Skills (LOTS)\" pointing up - Right side: Arrow labeled \"Higher-Order Thinking Skills (HOTS)\" pointing up - Foundation label at bottom: \"Knowledge Foundation\"</p> <p>Interactive Features: - Hover over each level to see detailed description and more example verbs - Click a level to see sample learning objectives at that level - Animation: Gentle pulse effect on each level when hovered</p> <p>Implementation: HTML/CSS with SVG pyramid, responsive design that stacks vertically on mobile</p> <p>Let's explore each level in more detail, because understanding these categories is essential for writing effective learning objectives and\u2014later in this course\u2014for selecting the right type of MicroSim for each learning goal.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#level-1-remember","title":"Level 1: Remember","text":"<p>The Remember level involves retrieving relevant knowledge from long-term memory. This is the foundation of all learning\u2014before you can do anything interesting with information, you need to be able to recall it.</p> <p>Remember-level activities include:</p> <ul> <li>Recognizing or recalling facts, terms, and basic concepts</li> <li>Retrieving definitions</li> <li>Listing items from memory</li> <li>Identifying components or features</li> </ul> <p>Example learning objectives at the Remember level:</p> <ul> <li>List the six levels of Bloom's Taxonomy in order</li> <li>Define the term \"learning objective\"</li> <li>Identify the parts of a cell from a diagram</li> <li>Recall the chemical symbols for common elements</li> </ul> <p>Remember-level knowledge is necessary but not sufficient. A student who can recite the quadratic formula but can't use it to solve problems hasn't really learned algebra. That's why Remember is just the first step on the cognitive staircase.</p> <p>The Role of Memory in Learning</p> <p>Some educators dismiss memorization as \"rote learning\" and argue we should focus only on higher-order thinking. But research consistently shows that having information readily accessible in memory enables higher-order thinking. It's hard to analyze something you can't remember.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#level-2-understand","title":"Level 2: Understand","text":"<p>The Understand level involves constructing meaning from instructional messages\u2014whether oral, written, or graphic. Understanding goes beyond mere recall to demonstrate comprehension of the material.</p> <p>Understanding includes:</p> <ul> <li>Interpreting information in one's own words</li> <li>Classifying items into categories</li> <li>Summarizing key points</li> <li>Comparing and contrasting concepts</li> <li>Explaining cause and effect relationships</li> <li>Providing examples of abstract concepts</li> </ul> <p>Example learning objectives at the Understand level:</p> <ul> <li>Explain the difference between summative and formative assessment</li> <li>Classify learning objectives by their Bloom's level</li> <li>Summarize the main arguments in a research article</li> <li>Compare depth-first and breadth-first search algorithms</li> </ul> <p>The transition from Remember to Understand is where learning starts to get interesting. A student who truly understands a concept can talk about it in their own words, not just parrot back definitions.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#level-3-apply","title":"Level 3: Apply","text":"<p>The Apply level involves carrying out or using a procedure in a given situation. This is where learners take what they know and actually do something with it.</p> <p>Application includes:</p> <ul> <li>Executing a procedure in a familiar situation</li> <li>Implementing a technique or method</li> <li>Solving problems using acquired knowledge</li> <li>Using information in new contexts</li> </ul> <p>Example learning objectives at the Apply level:</p> <ul> <li>Calculate the mean and standard deviation for a data set</li> <li>Use the Pythagorean theorem to find the length of a hypotenuse</li> <li>Apply the AIDA model to write a marketing email</li> <li>Implement a binary search algorithm in Python</li> </ul> <p>The Apply level is often where practical skills live. You can understand how to ride a bicycle in theory (Remember and Understand) without being able to actually do it (Apply). Many professional skills require extensive practice at the Apply level.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#level-4-analyze","title":"Level 4: Analyze","text":"<p>The Analyze level involves breaking material into constituent parts and determining how the parts relate to one another and to an overall structure or purpose.</p> <p>Analysis includes:</p> <ul> <li>Differentiating between relevant and irrelevant information</li> <li>Organizing components into a coherent structure</li> <li>Attributing underlying meaning or intent</li> <li>Comparing and finding patterns across examples</li> </ul> <p>Example learning objectives at the Analyze level:</p> <ul> <li>Analyze a case study to identify the root cause of a project failure</li> <li>Compare three different sorting algorithms in terms of time complexity</li> <li>Differentiate between valid and invalid arguments in a debate</li> <li>Examine the relationship between variables in a data set</li> </ul> <p>The Analyze level marks the transition into higher-order thinking skills. Analysis requires not just knowing information but actively working with it to discover patterns and relationships.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#level-5-evaluate","title":"Level 5: Evaluate","text":"<p>The Evaluate level involves making judgments based on criteria and standards. Evaluation requires both understanding the subject matter and applying appropriate criteria to assess it.</p> <p>Evaluation includes:</p> <ul> <li>Checking for internal consistency or errors</li> <li>Critiquing work based on standards</li> <li>Judging the effectiveness of approaches</li> <li>Justifying decisions with evidence</li> <li>Prioritizing options based on criteria</li> </ul> <p>Example learning objectives at the Evaluate level:</p> <ul> <li>Evaluate the strengths and weaknesses of different database designs</li> <li>Critique a research study's methodology</li> <li>Judge which investment option best meets a client's needs</li> <li>Assess whether a proposed solution meets the project requirements</li> </ul> <p>Evaluation is cognitively demanding because it requires understanding what \"good\" looks like in context. A novice can't evaluate effectively because they don't yet know the relevant criteria.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#level-6-create","title":"Level 6: Create","text":"<p>The Create level involves putting elements together to form a coherent or functional whole. This is the highest level of cognitive complexity\u2014synthesizing knowledge to produce something new.</p> <p>Creation includes:</p> <ul> <li>Generating new ideas or hypotheses</li> <li>Planning and designing solutions</li> <li>Producing original work</li> <li>Constructing new mental frameworks</li> </ul> <p>Example learning objectives at the Create level:</p> <ul> <li>Design a database schema for a new business application</li> <li>Develop a marketing strategy for a product launch</li> <li>Compose an original piece of music in the Baroque style</li> <li>Create an AI-powered MicroSim to teach a specific concept</li> </ul> <p>Notice that Create doesn't necessarily mean creating physical objects. It can mean creating plans, designs, hypotheses, or organizational structures. The key is producing something new rather than reproducing existing information.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#diagram-blooms-taxonomy-action-verb-wheel","title":"Diagram: Bloom's Taxonomy Action Verb Wheel","text":"Bloom's Taxonomy Action Verb Wheel <p>Type: microsim</p> <p>Purpose: Interactive tool for selecting appropriate action verbs when writing learning objectives at different Bloom's levels</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Given a learning goal, students will be able to select appropriate action verbs that match the desired cognitive complexity level.</p> <p>Canvas layout: - Circular design, 600x600px recommended, responsive to container width - Six wedge-shaped sections arranged as a wheel - Center area for displaying selected verb details</p> <p>Visual elements: - Outer ring divided into 6 colored sections (one per Bloom's level) - Each section contains 6-8 clickable action verbs - Center circle displays:   - Currently selected Bloom's level   - Definition of that level   - Example learning objective template</p> <p>Sections with verbs: 1. Remember (Blue): list, define, recall, identify, name, recognize, locate, describe 2. Understand (Green): explain, summarize, interpret, classify, compare, contrast, exemplify, infer 3. Apply (Yellow): use, execute, implement, solve, demonstrate, calculate, apply, practice 4. Analyze (Orange): differentiate, organize, attribute, compare, contrast, examine, deconstruct, distinguish 5. Evaluate (Pink): judge, critique, assess, justify, prioritize, recommend, validate, defend 6. Create (Purple): design, construct, develop, formulate, compose, produce, invent, generate</p> <p>Interactive controls: - Click any verb to select it - Selected verb displays definition and example usage - \"Generate Template\" button creates a learning objective template using selected verb - Hover over section to highlight and show level description - Button to randomize/suggest a verb for practice</p> <p>Default state: - No verb selected - Center shows instruction: \"Click a verb to learn more\"</p> <p>Behavior: - Clicking a verb highlights it and updates center display - Hovering over a level section shows tooltip with level description - Generate Template creates fill-in-the-blank objective: \"Students will be able to [VERB] [BLANK] by [BLANK]\" - Animation: Smooth transitions between selections</p> <p>Implementation: p5.js with responsive canvas, use updateCanvasSize() in setup()</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#action-verbs-the-secret-sauce-of-learning-objectives","title":"Action Verbs: The Secret Sauce of Learning Objectives","text":"<p>You've probably noticed that each Bloom's level is associated with specific action verbs. This isn't a coincidence\u2014it's a feature. The verb you choose for a learning objective communicates the level of cognitive complexity you're targeting.</p> <p>Consider these two objectives:</p> <ol> <li>\"Students will understand supply and demand\"</li> <li>\"Students will predict price changes based on shifts in supply and demand curves\"</li> </ol> <p>The first uses a vague verb (\"understand\") that doesn't specify what students should actually do. The second uses a precise verb (\"predict\") that describes a specific, observable action.</p> <p>Here's a handy reference of action verbs organized by Bloom's level:</p> Bloom's Level Action Verbs Remember list, define, recall, identify, name, recognize, locate, match, memorize Understand explain, summarize, interpret, classify, compare, describe, discuss, predict, translate Apply use, solve, demonstrate, calculate, apply, construct, complete, illustrate, show Analyze analyze, compare, contrast, examine, differentiate, distinguish, categorize, investigate Evaluate judge, evaluate, critique, assess, justify, recommend, defend, prioritize, rate Create design, create, develop, construct, produce, formulate, compose, devise, generate <p>Verbs to Avoid</p> <p>Some verbs are too vague to be useful in learning objectives. Avoid:</p> <ul> <li>Know \u2013 Too vague. Know what? Know how?</li> <li>Understand \u2013 Can't be directly observed</li> <li>Learn \u2013 Describes a process, not an outcome</li> <li>Appreciate \u2013 Subjective and unmeasurable</li> <li>Be aware of \u2013 What would this even look like?</li> </ul> <p>The shift from noun-based categories (Knowledge, Comprehension, etc.) in the original 1956 taxonomy to verb-based categories in the 2001 revision wasn't just wordsmithing. It emphasized that learning is about doing, not just having. Knowledge isn't a possession\u2014it's a capacity for action.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#measurable-outcomes-and-learning-outcomes","title":"Measurable Outcomes and Learning Outcomes","text":"<p>A learning outcome is the result of the learning process\u2014what students can actually do after instruction. While \"learning objective\" and \"learning outcome\" are sometimes used interchangeably, there's a subtle distinction:</p> <ul> <li>Learning objective = What we intend for students to learn (the goal)</li> <li>Learning outcome = What students actually learned (the result)</li> </ul> <p>In a perfect world, these would be identical. In reality, there's often a gap between intended objectives and actual outcomes. Good assessment helps us measure this gap.</p> <p>For outcomes to be measurable, they need to be:</p> <ul> <li>Specific \u2013 Clear about what behavior is expected</li> <li>Observable \u2013 Can be seen or demonstrated</li> <li>Measurable \u2013 Can be assessed against criteria</li> <li>Achievable \u2013 Realistic for the learners and timeframe</li> <li>Relevant \u2013 Connected to meaningful goals</li> </ul> <p>This framework (sometimes called SMART objectives) ensures that learning objectives aren't just wishful thinking but actual targets we can aim for and assess.</p> <p>Measurable outcomes are statements that specify not just what learners will do, but how well they'll do it and under what conditions. For example:</p> <ul> <li>\"Given a circuit diagram, students will calculate the total resistance with 90% accuracy.\"</li> <li>\"Without reference materials, learners will list all 50 U.S. states within 5 minutes.\"</li> <li>\"Using the provided rubric, participants will evaluate a peer's presentation and provide feedback on at least three criteria.\"</li> </ul>"},{"location":"chapters/01-foundations-learning-objective-analysis/#diagram-smart-learning-objectives-framework","title":"Diagram: SMART Learning Objectives Framework","text":"SMART Learning Objectives Framework <p>Type: infographic</p> <p>Purpose: Illustrate the five components of well-crafted learning objectives using the SMART framework</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Students will be able to explain each component of the SMART framework and apply it to evaluate learning objectives.</p> <p>Layout: Horizontal arrangement showing five connected panels, one for each letter of SMART</p> <p>Panels (left to right): 1. S - Specific    - Color: Blue    - Icon: Target/bullseye    - Key question: \"What exactly will learners do?\"    - Good example: \"Identify three causes of WWI\"    - Poor example: \"Know about WWI\"</p> <ol> <li>M - Measurable</li> <li>Color: Green</li> <li>Icon: Ruler/measuring tape</li> <li>Key question: \"How will we know they achieved it?\"</li> <li>Good example: \"Score 80% on quiz\"</li> <li> <p>Poor example: \"Do well on the test\"</p> </li> <li> <p>A - Achievable</p> </li> <li>Color: Yellow</li> <li>Icon: Mountain with flag</li> <li>Key question: \"Is this realistic for these learners?\"</li> <li>Good example: \"Write a 500-word essay\"</li> <li> <p>Poor example: \"Write a doctoral dissertation\"</p> </li> <li> <p>R - Relevant</p> </li> <li>Color: Orange</li> <li>Icon: Puzzle piece fitting</li> <li>Key question: \"Does this connect to larger goals?\"</li> <li>Good example: \"Calculate dosages (for nursing students)\"</li> <li> <p>Poor example: \"Memorize random facts\"</p> </li> <li> <p>T - Time-bound</p> </li> <li>Color: Purple</li> <li>Icon: Clock/hourglass</li> <li>Key question: \"By when should this be achieved?\"</li> <li>Good example: \"By end of module\"</li> <li>Poor example: \"Eventually\"</li> </ol> <p>Visual connections: Arrow connecting panels showing flow Bottom bar: Example complete SMART objective combining all elements</p> <p>Interactive Features: - Hover over each panel for expanded explanation - Click to see more examples (good and poor) - Toggle to show objective building mode</p> <p>Implementation: HTML/CSS/JavaScript, responsive grid layout</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#decomposing-objectives-from-compound-to-atomic","title":"Decomposing Objectives: From Compound to Atomic","text":"<p>Not all learning objectives are created equal in terms of scope. Some objectives are small and focused; others try to pack in multiple skills or concepts. Understanding this distinction is crucial for effective instructional design.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#atomic-concepts","title":"Atomic Concepts","text":"<p>An atomic concept is a single, indivisible unit of knowledge or skill. It's the smallest meaningful piece of information that can be taught and assessed independently. Atomic concepts are the building blocks of more complex understanding.</p> <p>Examples of atomic concepts:</p> <ul> <li>The definition of \"mean\" in statistics</li> <li>The symbol for addition (+)</li> <li>The fact that water boils at 100\u00b0C at sea level</li> <li>The formula for the area of a rectangle (A = l \u00d7 w)</li> </ul> <p>Atomic concepts have a few key characteristics:</p> <ul> <li>They can be understood without breaking them into smaller parts</li> <li>They can be taught in a single lesson segment</li> <li>They can be assessed with a single question or task</li> <li>They serve as prerequisites for more complex concepts</li> </ul>"},{"location":"chapters/01-foundations-learning-objective-analysis/#compound-objectives","title":"Compound Objectives","text":"<p>A compound objective combines multiple atomic concepts or skills into a single statement. Compound objectives aren't necessarily bad\u2014they often represent the integrated skills we actually want learners to develop. But they need to be handled carefully.</p> <p>Consider this objective: \"Students will design and implement a RESTful API that handles user authentication and data validation.\"</p> <p>This compound objective includes:</p> <ul> <li>Understanding RESTful architecture principles (atomic)</li> <li>Designing API endpoints (atomic)</li> <li>Implementing API endpoints in code (atomic)</li> <li>Understanding authentication concepts (atomic)</li> <li>Implementing authentication mechanisms (atomic)</li> <li>Understanding data validation requirements (atomic)</li> <li>Implementing validation logic (atomic)</li> </ul> <p>That's at least seven atomic concepts bundled into one objective! If a student struggles with this objective, where's the problem? Are they confused about REST? Authentication? Coding? It's impossible to diagnose without decomposing.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#objective-decomposition","title":"Objective Decomposition","text":"<p>Objective decomposition is the process of breaking compound objectives into their atomic components. This process reveals:</p> <ul> <li>Prerequisites \u2013 What must learners already know?</li> <li>Sequence \u2013 In what order should concepts be taught?</li> <li>Assessment points \u2013 Where can we check for understanding?</li> <li>Scaffolding needs \u2013 Where might learners need extra support?</li> </ul> <p>Here's how you might decompose the API objective above:</p> <pre><code>Compound: Design and implement a RESTful API with authentication and validation\n\n\u251c\u2500\u2500 Atomic: Define what REST stands for and its principles\n\u251c\u2500\u2500 Atomic: List the HTTP methods and their typical uses\n\u251c\u2500\u2500 Atomic: Design a resource hierarchy for a given scenario\n\u251c\u2500\u2500 Atomic: Write API endpoint specifications\n\u251c\u2500\u2500 Atomic: Implement a basic GET endpoint\n\u251c\u2500\u2500 Atomic: Implement POST, PUT, DELETE endpoints\n\u251c\u2500\u2500 Atomic: Explain authentication vs. authorization\n\u251c\u2500\u2500 Atomic: Implement token-based authentication\n\u251c\u2500\u2500 Atomic: Define data validation requirements\n\u2514\u2500\u2500 Atomic: Implement input validation logic\n</code></pre>"},{"location":"chapters/01-foundations-learning-objective-analysis/#diagram-objective-decomposition-tree","title":"Diagram: Objective Decomposition Tree","text":"Objective Decomposition Tree <p>Type: graph-model</p> <p>Purpose: Demonstrate how compound learning objectives can be decomposed into atomic concepts using an interactive tree visualization</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Students will be able to decompose a compound learning objective into its constituent atomic concepts and identify prerequisite relationships.</p> <p>Node types: 1. Compound Objective (red rounded rectangle)    - Properties: title, bloom_level, estimated_time    - Position: Top of hierarchy</p> <ol> <li>Skill Cluster (orange rounded rectangle)</li> <li>Properties: title, category</li> <li> <p>Position: Second level</p> </li> <li> <p>Atomic Concept (green circles)</p> </li> <li>Properties: title, definition, bloom_level</li> <li>Position: Leaf nodes</li> </ol> <p>Sample data structure: - Compound: \"Create a data visualization dashboard\"   \u251c\u2500\u2500 Cluster: \"Data Processing Skills\"   \u2502   \u251c\u2500\u2500 Atomic: \"Load data from CSV\"   \u2502   \u251c\u2500\u2500 Atomic: \"Clean missing values\"   \u2502   \u2514\u2500\u2500 Atomic: \"Transform data types\"   \u251c\u2500\u2500 Cluster: \"Visualization Skills\"   \u2502   \u251c\u2500\u2500 Atomic: \"Create bar chart\"   \u2502   \u251c\u2500\u2500 Atomic: \"Create line chart\"   \u2502   \u2514\u2500\u2500 Atomic: \"Add chart labels\"   \u2514\u2500\u2500 Cluster: \"Dashboard Assembly\"       \u251c\u2500\u2500 Atomic: \"Arrange components\"       \u2514\u2500\u2500 Atomic: \"Add interactivity\"</p> <p>Edge types: - DECOMPOSES_TO (solid lines): Compound \u2192 Cluster \u2192 Atomic - REQUIRES (dashed lines): Atomic \u2192 Atomic (prerequisites)</p> <p>Layout: Hierarchical, top-down tree layout</p> <p>Interactive features: - Click node to expand/collapse children - Hover to see node properties - Drag to rearrange (for practice) - Right-click to mark as \"understood\" - Color coding shows completion status</p> <p>Visual styling: - Node size based on complexity - Dashed borders for optional concepts - Bold borders for critical path items</p> <p>Implementation: vis-network with hierarchical layout Canvas: Responsive width, 500px height minimum</p> <p>Why does decomposition matter for AI-assisted instructional design? Because AI tools work best with clear, specific instructions. When you ask an AI to generate a MicroSim for a compound objective, it may not know which component to focus on. But ask it to generate a MicroSim for \"learners will calculate the mean of a data set,\" and it can create something targeted and effective.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#putting-it-all-together-the-instructional-design-mindset","title":"Putting It All Together: The Instructional Design Mindset","text":"<p>We've covered a lot of ground in this chapter. Let's connect the dots and see how these concepts work together.</p> <p>Instructional design is the systematic process of creating effective learning experiences. At the heart of instructional design are learning objectives\u2014clear, specific statements about what learners will be able to do. These objectives exist within the broader ecosystem of educational technology, which provides tools for delivering and assessing learning.</p> <p>Bloom's Taxonomy gives us a framework for understanding cognitive complexity\u2014the mental demands different tasks place on learners. From Remember through Understand, Apply, Analyze, Evaluate, and Create, each level builds on the ones below.</p> <p>Action verbs are our tools for communicating the intended cognitive level. Choosing the right verb ensures that our objectives are specific and measurable, leading to clear learning outcomes.</p> <p>When objectives become too complex, we use objective decomposition to break them into atomic concepts. This reveals the structure hidden within compound objectives and helps us design instruction that builds systematically.</p> <p>And underlying all of this is a fundamental optimism: learning can be designed. It's not magic or luck. With the right tools, knowledge, and systematic approach, we can create educational experiences that genuinely help people learn. And with AI as our partner, we can do this at scale\u2014potentially transforming education worldwide.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#diagram-concept-map-of-chapter-1-foundations","title":"Diagram: Concept Map of Chapter 1 Foundations","text":"Chapter 1 Concept Map <p>Type: graph-model</p> <p>Purpose: Show the relationships between all 17 concepts covered in this chapter as an interactive concept map</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Students will be able to trace the relationships between instructional design concepts and explain how they interconnect.</p> <p>Node types (all circles, different colors by category): 1. Core Concepts (Blue):    - Instructional Design (largest, central)    - Educational Technology    - Learning Objective</p> <ol> <li>Bloom's Taxonomy Concepts (Rainbow gradient):</li> <li>Bloom's Taxonomy (hub node)</li> <li>Cognitive Complexity</li> <li>Remember Level</li> <li>Understand Level</li> <li>Apply Level</li> <li>Analyze Level</li> <li>Evaluate Level</li> <li> <p>Create Level</p> </li> <li> <p>Objective Components (Green):</p> </li> <li>Action Verbs</li> <li>Measurable Outcomes</li> <li> <p>Learning Outcome</p> </li> <li> <p>Decomposition Concepts (Orange):</p> </li> <li>Objective Decomposition</li> <li>Atomic Concepts</li> <li>Compound Objectives</li> </ol> <p>Edge relationships: - Instructional Design USES Learning Objective - Instructional Design EMPLOYS Educational Technology - Learning Objective MEASURED_BY Measurable Outcomes - Learning Objective PRODUCES Learning Outcome - Learning Objective CATEGORIZED_BY Bloom's Taxonomy - Bloom's Taxonomy DEFINES Cognitive Complexity - Bloom's Taxonomy CONTAINS all six levels (hierarchical) - Action Verbs INDICATE Cognitive Complexity - Compound Objectives DECOMPOSE_TO Atomic Concepts - Objective Decomposition PRODUCES Atomic Concepts</p> <p>Layout: Force-directed with Bloom's Taxonomy levels arranged vertically Canvas: Responsive width, 600px height</p> <p>Interactive features: - Hover node to highlight all connected nodes - Click node to see definition - Double-click to center view on node - Zoom and pan enabled - Toggle to show/hide edge labels</p> <p>Implementation: vis-network library with physics simulation</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#chapter-summary","title":"Chapter Summary","text":"<p>Congratulations! You've climbed the first major hill in your journey toward automating instructional design. Here's what you've learned:</p> <ul> <li>Instructional design is the systematic process of creating effective learning experiences</li> <li>Learning objectives are clear statements about what learners will be able to do</li> <li>Educational technology provides tools for delivering and assessing learning</li> <li>Bloom's Taxonomy identifies six levels of cognitive complexity: Remember, Understand, Apply, Analyze, Evaluate, and Create</li> <li>Action verbs communicate the intended cognitive level of an objective</li> <li>Measurable outcomes ensure objectives can be assessed</li> <li>Compound objectives bundle multiple skills; objective decomposition breaks them into atomic concepts</li> </ul> <p>With this foundation in place, you're ready to explore how different types of learning objectives map to different visualization approaches. In the next chapter, we'll introduce MicroSims and start matching objectives to the perfect interactive experience.</p> <p>Key Takeaway</p> <p>AI can be an incredibly powerful tool for instructional design\u2014but only if we give it clear, well-crafted instructions. Understanding learning objectives and cognitive complexity isn't just academic theory; it's practical knowledge that makes AI-assisted education possible.</p> <p>Now go forth and write some beautiful learning objectives. Your future AI assistants will thank you.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/#review-questions","title":"Review Questions","text":"What are the six levels of Bloom's Taxonomy in order from lowest to highest cognitive complexity? <p>The six levels are:</p> <ol> <li>Remember</li> <li>Understand</li> <li>Apply</li> <li>Analyze</li> <li>Evaluate</li> <li>Create</li> </ol> <p>A helpful mnemonic: \"Really Understanding Always Allows Excellent Creation\"</p> Why is the verb 'understand' problematic in learning objectives? <p>\"Understand\" is problematic because it describes an internal mental state that cannot be directly observed or measured. How would you know if someone truly \"understands\" something? Instead, use observable action verbs like \"explain,\" \"compare,\" \"classify,\" or \"predict\" that demonstrate understanding through visible behavior.</p> What is the difference between an atomic concept and a compound objective? <p>An atomic concept is a single, indivisible unit of knowledge or skill that can be taught and assessed independently\u2014the smallest meaningful building block of instruction.</p> <p>A compound objective combines multiple atomic concepts or skills into one statement. Compound objectives often represent valuable integrated skills but should be decomposed for effective instruction and assessment.</p> According to METR.org research, how quickly are AI capabilities currently doubling? <p>AI capabilities are doubling approximately every seven months when measured by the probability of LLMs and agents correctly completing tasks of a given complexity. This exponential growth has profound implications for how AI can assist with instructional design.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/quiz/","title":"Quiz: Foundations of Learning Objective Analysis","text":"<p>Test your understanding of instructional design fundamentals, Bloom's Taxonomy, and learning objective analysis with these questions.</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/quiz/#1-what-is-the-definition-of-instructional-design","title":"1. What is the definition of instructional design?","text":"<ol> <li>The process of creating interactive simulations for classroom use</li> <li>The systematic process of creating educational experiences that make learning efficient, effective, and enjoyable</li> <li>A framework for categorizing educational technology tools</li> <li>The practice of using AI to generate lesson plans automatically</li> </ol> Show Answer <p>The correct answer is B. Instructional design is the systematic process of creating educational experiences that make learning efficient, effective, and enjoyable. It involves analyzing learner needs, designing experiences that bridge knowledge gaps, and evaluating whether those experiences work. Option A describes MicroSims specifically, not instructional design broadly. Option C describes taxonomies or classification systems. Option D describes one application of AI, not instructional design itself.</p> <p>Concept Tested: Instructional Design</p> <p>See: Chapter Content</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/quiz/#2-what-are-the-six-levels-of-blooms-taxonomy-in-order-from-lowest-to-highest-cognitive-complexity","title":"2. What are the six levels of Bloom's Taxonomy in order from lowest to highest cognitive complexity?","text":"<ol> <li>Know, Comprehend, Use, Examine, Judge, Build</li> <li>Remember, Understand, Apply, Analyze, Evaluate, Create</li> <li>Recall, Interpret, Execute, Compare, Critique, Design</li> <li>Identify, Explain, Solve, Differentiate, Assess, Produce</li> </ol> Show Answer <p>The correct answer is B. The six levels of Bloom's Taxonomy in the 2001 revised version are: Remember, Understand, Apply, Analyze, Evaluate, and Create. These represent increasing levels of cognitive complexity, with Remember being the foundation and Create being the highest level. The other options use similar concepts but are not the official terminology of the revised Bloom's Taxonomy.</p> <p>Concept Tested: Bloom's Taxonomy</p> <p>See: Chapter Content</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/quiz/#3-why-is-the-verb-understand-problematic-when-writing-learning-objectives","title":"3. Why is the verb \"understand\" problematic when writing learning objectives?","text":"<ol> <li>It is too difficult for students to achieve</li> <li>It only applies to the Remember level of Bloom's Taxonomy</li> <li>It describes an internal mental state that cannot be directly observed or measured</li> <li>It is considered outdated terminology in modern instructional design</li> </ol> Show Answer <p>The correct answer is C. \"Understand\" is problematic because it describes an internal mental state that cannot be directly observed or measured. Good learning objectives require observable action verbs like \"explain,\" \"compare,\" \"classify,\" or \"predict\" that demonstrate understanding through visible behavior. How would you know if someone truly \"understands\" something without observable evidence?</p> <p>Concept Tested: Action Verbs</p> <p>See: Chapter Content</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/quiz/#4-what-is-an-atomic-concept","title":"4. What is an atomic concept?","text":"<ol> <li>A learning objective that focuses on chemistry or physics topics</li> <li>A single, indivisible unit of knowledge or skill that can be taught and assessed independently</li> <li>The highest level of cognitive complexity in Bloom's Taxonomy</li> <li>A type of interactive simulation used for science education</li> </ol> Show Answer <p>The correct answer is B. An atomic concept is a single, indivisible unit of knowledge or skill that can be taught and assessed independently. It's the smallest meaningful piece of information that serves as a building block for more complex understanding. Examples include the definition of \"mean\" in statistics or the formula for the area of a rectangle. The term \"atomic\" refers to being indivisible, not to atomic physics.</p> <p>Concept Tested: Atomic Concepts</p> <p>See: Chapter Content</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/quiz/#5-according-to-the-chapter-what-distinguishes-a-good-learning-objective-from-a-vague-one","title":"5. According to the chapter, what distinguishes a good learning objective from a vague one?","text":"<ol> <li>Good objectives use complex vocabulary while vague ones use simple language</li> <li>Good objectives describe observable, measurable behaviors while vague ones describe internal mental states</li> <li>Good objectives are longer and more detailed while vague ones are short</li> <li>Good objectives focus on content while vague ones focus on activities</li> </ol> Show Answer <p>The correct answer is B. Good learning objectives describe observable, measurable behaviors (like \"predict the trajectory of a falling object\"), while vague objectives describe internal mental states that cannot be directly observed (like \"understand gravity\"). The magic question to ask is: \"How would I know if a student achieved this?\" If you can't describe a concrete way to assess it, the objective needs work.</p> <p>Concept Tested: Learning Objective</p> <p>See: Chapter Content</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/quiz/#6-which-blooms-level-involves-making-judgments-based-on-criteria-and-standards","title":"6. Which Bloom's level involves making judgments based on criteria and standards?","text":"<ol> <li>Analyze</li> <li>Create</li> <li>Evaluate</li> <li>Apply</li> </ol> Show Answer <p>The correct answer is C. The Evaluate level involves making judgments based on criteria and standards. This includes checking for errors, critiquing work, judging effectiveness, and justifying decisions with evidence. Evaluation requires both understanding the subject matter and applying appropriate criteria to assess it. Analyze involves breaking material into parts. Create involves producing something new. Apply involves using procedures.</p> <p>Concept Tested: Evaluate Level</p> <p>See: Chapter Content</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/quiz/#7-what-is-the-purpose-of-objective-decomposition","title":"7. What is the purpose of objective decomposition?","text":"<ol> <li>To simplify learning objectives so they are easier for students to understand</li> <li>To break compound objectives into their atomic components, revealing prerequisites and sequence</li> <li>To convert action verbs into Bloom's Taxonomy levels</li> <li>To eliminate unnecessary learning objectives from a curriculum</li> </ol> Show Answer <p>The correct answer is B. Objective decomposition is the process of breaking compound objectives into their atomic components. This process reveals prerequisites (what must learners already know), sequence (in what order concepts should be taught), assessment points (where we can check for understanding), and scaffolding needs (where learners might need extra support). This is especially important for AI-assisted design, as AI tools work best with clear, specific instructions.</p> <p>Concept Tested: Objective Decomposition</p> <p>See: Chapter Content</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/quiz/#8-given-the-learning-objective-students-will-analyze-a-case-study-to-identify-the-root-cause-of-a-project-failure-at-which-blooms-level-does-this-objective-operate","title":"8. Given the learning objective \"Students will analyze a case study to identify the root cause of a project failure,\" at which Bloom's level does this objective operate?","text":"<ol> <li>Understand</li> <li>Apply</li> <li>Analyze</li> <li>Evaluate</li> </ol> Show Answer <p>The correct answer is C. This objective operates at the Analyze level because it requires learners to break material (the case study) into constituent parts and determine relationships (identifying the root cause). The verb \"analyze\" directly indicates this level. Additionally, identifying root causes requires differentiating between relevant and irrelevant information and organizing findings into a coherent structure\u2014key characteristics of the Analyze level.</p> <p>Concept Tested: Analyze Level</p> <p>See: Chapter Content</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/quiz/#9-what-is-the-difference-between-a-learning-objective-and-a-learning-outcome","title":"9. What is the difference between a learning objective and a learning outcome?","text":"<ol> <li>Learning objectives are for K-12 education; learning outcomes are for higher education</li> <li>Learning objectives state what we intend for students to learn; learning outcomes are what students actually learned</li> <li>Learning objectives use action verbs; learning outcomes use noun phrases</li> <li>Learning objectives are measurable; learning outcomes are not measurable</li> </ol> Show Answer <p>The correct answer is B. A learning objective is what we intend for students to learn (the goal), while a learning outcome is what students actually learned (the result). In a perfect world, these would be identical, but in reality there's often a gap between intended objectives and actual outcomes. Good assessment helps measure this gap and determine whether instruction was effective.</p> <p>Concept Tested: Learning Outcome</p> <p>See: Chapter Content</p>"},{"location":"chapters/01-foundations-learning-objective-analysis/quiz/#10-a-curriculum-designer-writes-the-objective-students-will-design-and-implement-a-restful-api-that-handles-user-authentication-and-data-validation-what-type-of-objective-is-this-and-what-should-be-done-with-it","title":"10. A curriculum designer writes the objective: \"Students will design and implement a RESTful API that handles user authentication and data validation.\" What type of objective is this, and what should be done with it?","text":"<ol> <li>It is an atomic concept and should be used as-is for assessment</li> <li>It is a compound objective and should be decomposed into multiple atomic concepts</li> <li>It is a Remember-level objective and should be elevated to a higher Bloom's level</li> <li>It is a measurable outcome and should be converted into a learning objective</li> </ol> Show Answer <p>The correct answer is B. This is a compound objective that bundles multiple skills together: understanding RESTful architecture, designing API endpoints, implementing code, understanding authentication, implementing authentication mechanisms, understanding data validation, and implementing validation logic. For effective instruction and assessment, it should be decomposed into its atomic components. This helps diagnose where students struggle and ensures proper sequencing of instruction.</p> <p>Concept Tested: Compound Objectives</p> <p>See: Chapter Content</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/","title":"Prerequisite Analysis and MicroSim Fundamentals","text":""},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#summary","title":"Summary","text":"<p>This chapter introduces the core concepts of MicroSims and interactive simulations while teaching you how to analyze prerequisite knowledge and concept dependencies. You will learn to identify assumed knowledge, map learning pathways, and assess whether a learning objective is ready for simulation. These skills are essential for designing effective educational simulations that meet learners where they are and build on their existing knowledge.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 8 concepts from the learning graph:</p> <ol> <li>MicroSim</li> <li>Interactive Simulation</li> <li>AI-Assisted Design</li> <li>Prerequisite Knowledge</li> <li>Assumed Knowledge</li> <li>Simulation Readiness</li> <li>Concept Dependencies</li> <li>Learning Pathway</li> </ol>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Foundations of Learning Objective Analysis</li> </ul>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#the-birth-of-microsims-a-brief-history","title":"The Birth of MicroSims: A Brief History","text":"<p>In 2023, educational technologist Valarie Lockhart coined the term \"MicroSim\" to describe a new paradigm in educational simulation design. The name captures the essence of the approach: micro for small, focused, single-concept learning experiences, and sim for simulation\u2014interactive digital environments where learners can experiment, explore, and discover.</p> <p>The timing wasn't accidental. As large language models exploded in capability, Lockhart and others recognized that the bottleneck in educational content creation was shifting. It was no longer primarily about technical capability\u2014AI could now generate working code. The bottleneck had moved to pedagogical capability\u2014knowing what to build and why.</p> <p>MicroSim Research Foundation</p> <p>The theoretical framework for MicroSims was formalized in the paper \"MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support,\" published in November 2025. This paper established the principles for designing simulations that are both pedagogically sound and AI-generatable.</p> <p>By December 2025, the first courses in generating intelligent textbooks using MicroSims were being taught by Dan McCreary. These courses leveraged the growing power of large language models and, crucially, introduced the concept of Claude Code Skills\u2014structured rule sets that ensure consistency and high-quality content generation. Without such guardrails, AI-generated educational content tends to drift, vary wildly in quality, and miss pedagogical targets. Skills brought discipline to the creative process.</p> <p>Today, hundreds of working MicroSim examples spanning audiences from kindergarten to graduate school are available in the MicroSim Textbook. This open resource demonstrates the breadth of what's possible when thoughtful instructional design meets AI-assisted implementation.</p> <p>But enough history. Let's dive into what MicroSims actually are and why they matter for your work as an instructional designer.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#what-is-an-interactive-simulation","title":"What Is an Interactive Simulation?","text":"<p>An interactive simulation is a digital environment that models some aspect of the real world (or an abstract concept) and allows users to manipulate variables, observe outcomes, and develop understanding through experimentation. Unlike passive content like videos or text, simulations put learners in the driver's seat.</p> <p>Interactive simulations have been used in education for decades:</p> <ul> <li>Flight simulators train pilots without risking actual aircraft</li> <li>PhET simulations from the University of Colorado help students explore physics and chemistry</li> <li>Business simulations let MBA students run virtual companies</li> <li>Medical simulations allow practitioners to practice procedures safely</li> </ul> <p>What makes these effective isn't just the interactivity\u2014it's the feedback loop. Learners take an action, observe a consequence, adjust their mental model, and try again. This cycle mirrors how humans naturally learn through experience.</p> Passive Learning Interactive Learning Watch a video about gravity Drop virtual objects and observe acceleration Read about supply and demand Adjust price and quantity sliders, watch market response Listen to a lecture on recursion Step through a recursive algorithm visually Memorize sorting algorithm steps Watch algorithms race to sort different datasets <p>The problem with traditional interactive simulations is that they're expensive and time-consuming to build. A high-quality PhET simulation might take a team of developers, designers, and subject matter experts months to create. That's fine for foundational concepts taught to millions of students, but it doesn't scale to the long tail of specialized topics.</p> <p>Enter MicroSims.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#microsims-simulations-that-scale","title":"MicroSims: Simulations That Scale","text":"<p>A MicroSim is a small, focused interactive simulation designed to teach a single concept or learning objective. The \"micro\" isn't just about size\u2014it's about scope. Each MicroSim targets one specific learning goal and does that one thing well.</p> <p>Here's what distinguishes MicroSims from traditional educational simulations:</p> Traditional Simulation MicroSim Covers many concepts Covers one concept Complex, feature-rich Simple, focused Months to develop Hours to generate Requires development team Generated with AI assistance Standalone application Embeds in any web page One-size-fits-all Adaptable to audience level <p>The power of MicroSims comes from their composability. Instead of building one massive simulation that tries to teach everything about a topic, you build a library of focused MicroSims that can be assembled into learning pathways. It's like the difference between a single encyclopedic textbook and a modular curriculum where each piece can be updated, replaced, or recombined independently.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#diagram-traditional-simulation-vs-microsim-architecture","title":"Diagram: Traditional Simulation vs MicroSim Architecture","text":"Traditional Simulation vs MicroSim Architecture <p>Type: diagram</p> <p>Purpose: Contrast the monolithic architecture of traditional simulations with the modular, composable architecture of MicroSims</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Students will be able to explain the architectural differences between traditional simulations and MicroSim-based approaches.</p> <p>Components to show: Left side - Traditional Simulation: - Large monolithic box labeled \"Comprehensive Simulation\" - Multiple concepts listed inside (Concept A, B, C, D, E) - Single entry point at top - Heavy dependencies shown as tangled lines between concepts - Labels: \"Long development time\", \"Hard to update\", \"Fixed scope\"</p> <p>Right side - MicroSim Architecture: - Multiple small boxes, each containing single concept - MicroSim A, MicroSim B, MicroSim C, MicroSim D, MicroSim E - Clean arrows showing optional learning pathways between them - Central \"Learning Pathway Manager\" connecting to each - Labels: \"Quick to create\", \"Easy to update\", \"Flexible combinations\"</p> <p>Visual Style: Side-by-side comparison diagram Color Scheme: - Traditional: Grays and muted colors suggesting complexity - MicroSim: Bright, distinct colors for each module suggesting clarity</p> <p>Annotations: - Arrow pointing to traditional: \"Months to build\" - Arrow pointing to MicroSims: \"Hours per simulation\" - Bottom comparison: \"Update one concept = rebuild everything\" vs \"Update one concept = replace one MicroSim\"</p> <p>Implementation: HTML/CSS/SVG with responsive layout</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#characteristics-of-effective-microsims","title":"Characteristics of Effective MicroSims","text":"<p>Not every small simulation qualifies as a well-designed MicroSim. Effective MicroSims share several characteristics:</p> <ul> <li>Single Learning Objective: Each MicroSim targets exactly one learning objective at a specific Bloom's level</li> <li>Immediate Feedback: Learners receive instant visual or textual feedback on their actions</li> <li>Learner Control: Users can manipulate relevant parameters through intuitive controls (sliders, buttons, drag-and-drop)</li> <li>Progressive Disclosure: Complexity is revealed gradually, not dumped all at once</li> <li>Embedded Assessment: The simulation itself reveals whether learners understand the concept</li> <li>Universal Embedding: MicroSims work in any web context\u2014LMS, textbook, standalone page</li> <li>Responsive Design: They adapt to different screen sizes and devices</li> </ul> <p>The 30-Second Test</p> <p>A well-designed MicroSim should communicate its purpose within 30 seconds. If a learner can't figure out what they're supposed to do or learn, the simulation needs work. Complexity should come from the concept, not the interface.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#microsim-types-by-blooms-level","title":"MicroSim Types by Bloom's Level","text":"<p>Different learning objectives call for different types of MicroSims. Here's how MicroSim types map to Bloom's Taxonomy:</p> Bloom's Level MicroSim Type Example Remember Flashcard, Matching, Sorting Drag vocabulary terms to definitions Understand Visualization, Animation, Comparison Watch supply/demand curves shift and intersect Apply Calculator, Solver, Procedure Practice Calculate compound interest with adjustable parameters Analyze Data Explorer, Pattern Finder, Relationship Mapper Explore correlations in a dataset Evaluate Classifier, Rubric Applier, Decision Tree Judge whether code samples follow best practices Create Model Builder, Designer, Composer Build a state machine diagram from requirements <p>This mapping isn't rigid\u2014many MicroSims span multiple levels. But having a primary target level helps focus the design and ensures the simulation actually tests what you intend.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#diagram-microsim-types-by-blooms-level","title":"Diagram: MicroSim Types by Bloom's Level","text":"MicroSim Types by Bloom's Level <p>Type: infographic</p> <p>Purpose: Show the mapping between Bloom's Taxonomy levels and appropriate MicroSim types with visual examples</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Given a learning objective at a specific Bloom's level, students will be able to identify appropriate MicroSim types.</p> <p>Layout: Six-row table/grid format with visual icons for each MicroSim type</p> <p>Rows (bottom to top, matching Bloom's hierarchy): 1. Remember (Blue background)    - Icon: Brain with cards    - MicroSim Types: Flashcard Drill, Term Matcher, Concept Sorter    - Example thumbnail: Cards being matched    - Key action: \"Recall and recognize\"</p> <ol> <li>Understand (Green background)</li> <li>Icon: Lightbulb with animation frames</li> <li>MicroSim Types: Animated Explainer, Comparison Viewer, Concept Visualizer</li> <li>Example thumbnail: Animated diagram</li> <li> <p>Key action: \"See and comprehend\"</p> </li> <li> <p>Apply (Yellow background)</p> </li> <li>Icon: Calculator/tool</li> <li>MicroSim Types: Interactive Calculator, Step-by-Step Solver, Procedure Simulator</li> <li>Example thumbnail: Slider adjusting values</li> <li> <p>Key action: \"Use and execute\"</p> </li> <li> <p>Analyze (Orange background)</p> </li> <li>Icon: Magnifying glass over data</li> <li>MicroSim Types: Data Explorer, Pattern Detector, Relationship Grapher</li> <li>Example thumbnail: Network graph</li> <li> <p>Key action: \"Examine and connect\"</p> </li> <li> <p>Evaluate (Pink background)</p> </li> <li>Icon: Balance scale with checkmarks</li> <li>MicroSim Types: Quality Classifier, Rubric Applier, Decision Simulator</li> <li>Example thumbnail: Items being sorted into categories</li> <li> <p>Key action: \"Judge and decide\"</p> </li> <li> <p>Create (Purple background)</p> </li> <li>Icon: Pencil with sparkles</li> <li>MicroSim Types: Model Builder, Design Canvas, Composition Tool</li> <li>Example thumbnail: User-constructed diagram</li> <li>Key action: \"Design and produce\"</li> </ol> <p>Interactive Features: - Hover over any MicroSim type to see full description - Click to see live example from MicroSim library - Filter by subject area</p> <p>Implementation: HTML/CSS grid with icons and hover states, responsive design</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#ai-assisted-design-your-new-creative-partner","title":"AI-Assisted Design: Your New Creative Partner","text":"<p>AI-assisted design refers to the use of artificial intelligence tools\u2014particularly large language models like Claude\u2014to help create educational content. This isn't about replacing instructional designers; it's about amplifying their capabilities.</p> <p>Think of AI as a very fast, very knowledgeable assistant who can:</p> <ul> <li>Generate working code from natural language descriptions</li> <li>Produce multiple variations of content quickly</li> <li>Adapt existing content for different audiences</li> <li>Check specifications for completeness and consistency</li> <li>Suggest improvements based on pedagogical principles</li> </ul> <p>But AI assistants have significant limitations:</p> <ul> <li>They don't inherently understand how humans learn</li> <li>They can generate plausible-sounding but pedagogically flawed content</li> <li>They may miss cultural context or accessibility requirements</li> <li>They need clear, specific instructions to produce quality output</li> <li>They can't evaluate whether a simulation actually teaches effectively</li> </ul> <p>This is why the knowledge you're building in this course matters so much. AI is a powerful tool, but tools need skilled operators. A chainsaw in the hands of a trained arborist creates beautiful results; the same chainsaw wielded randomly creates a mess (and possibly a trip to the emergency room).</p> <p>The Plausible Nonsense Problem</p> <p>AI can generate content that sounds educational but doesn't actually support learning. It might create a simulation that's visually impressive but targets the wrong concept, or one that technically works but overwhelms learners with cognitive load. Your job as an instructional designer is to specify clearly and evaluate critically.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#the-human-ai-collaboration-loop","title":"The Human-AI Collaboration Loop","text":"<p>Effective AI-assisted design follows an iterative loop:</p> <ol> <li>Human specifies the learning objective, audience, and constraints</li> <li>AI generates a first draft (specification, code, content)</li> <li>Human evaluates against pedagogical criteria</li> <li>Human refines the specification based on what's missing or wrong</li> <li>AI regenerates with improved instructions</li> <li>Repeat until quality standards are met</li> <li>Human tests with actual learners</li> <li>Human iterates based on learner feedback</li> </ol> <p>Notice that humans bookend the process. We define what success looks like, and we verify that we've achieved it. AI accelerates the middle steps but doesn't replace human judgment at the critical points.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#diagram-human-ai-collaboration-loop","title":"Diagram: Human-AI Collaboration Loop","text":"Human-AI Collaboration Loop <p>Type: workflow</p> <p>Purpose: Illustrate the iterative collaboration between human instructional designers and AI tools in MicroSim development</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Students will be able to describe the stages of human-AI collaboration in educational content development.</p> <p>Visual style: Circular workflow diagram with alternating human and AI steps</p> <p>Steps (clockwise from top): 1. SPECIFY (Human, blue)    - Icon: Person with document    - Hover text: \"Define learning objective, audience, and requirements\"    - Position: 12 o'clock</p> <ol> <li>GENERATE (AI, purple)</li> <li>Icon: Robot/AI chip</li> <li>Hover text: \"AI produces first draft of simulation or content\"</li> <li> <p>Position: 2 o'clock</p> </li> <li> <p>EVALUATE (Human, blue)</p> </li> <li>Icon: Person with magnifying glass</li> <li>Hover text: \"Check against pedagogical criteria and learning goals\"</li> <li> <p>Position: 4 o'clock</p> </li> <li> <p>REFINE (Human, blue)</p> </li> <li>Icon: Person with pencil</li> <li>Hover text: \"Improve specification based on evaluation\"</li> <li> <p>Position: 6 o'clock</p> </li> <li> <p>REGENERATE (AI, purple)</p> </li> <li>Icon: Robot with refresh arrow</li> <li>Hover text: \"AI creates improved version with better instructions\"</li> <li> <p>Position: 8 o'clock</p> </li> <li> <p>TEST (Human, blue)</p> </li> <li>Icon: Person with group of learners</li> <li>Hover text: \"Validate with actual target learners\"</li> <li>Position: 10 o'clock</li> </ol> <p>Center element: - \"Quality MicroSim\" with star icon - Arrows showing iteration cycles</p> <p>Annotations: - Inner loop (steps 2-5): \"Rapid iteration cycle\" - Outer path (including step 6): \"Learner validation cycle\"</p> <p>Color coding: - Human steps: Educational blue - AI steps: Tech purple - Arrows: Gradient between</p> <p>Implementation: HTML/CSS/SVG with hover interactions, responsive circular layout</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#understanding-prerequisite-knowledge","title":"Understanding Prerequisite Knowledge","text":"<p>Prerequisite knowledge is what learners must already know before they can successfully engage with new content. It's the foundation on which new learning is built. Miss this foundation, and everything constructed on top will be shaky.</p> <p>Consider teaching someone to solve quadratic equations. What must they already know?</p> <ul> <li>Basic arithmetic operations</li> <li>Order of operations (PEMDAS)</li> <li>Variables and algebraic notation</li> <li>Solving linear equations</li> <li>The concept of squaring a number</li> <li>The concept of square roots</li> </ul> <p>Try teaching the quadratic formula to someone who doesn't understand what a variable is, and you'll quickly discover why prerequisite analysis matters.</p> <p>In the context of MicroSims, prerequisite knowledge determines:</p> <ul> <li>What the MicroSim can assume learners already understand</li> <li>What vocabulary can be used without explanation</li> <li>What complexity is appropriate for controls and displays</li> <li>Where learners might struggle if prerequisites are missing</li> <li>What scaffolding might be needed for learners with gaps</li> </ul>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#identifying-prerequisites","title":"Identifying Prerequisites","text":"<p>Prerequisites come in several varieties:</p> Type Description Example for \"Binary Search\" Conceptual Abstract ideas that must be understood Understanding what \"sorted\" means Procedural Skills or processes that must be mastered Ability to compare two values Terminological Vocabulary that must be known Words like \"array,\" \"index,\" \"midpoint\" Contextual Background knowledge that provides meaning Why we might need to search large datasets <p>Failing to identify prerequisites leads to the \"curse of knowledge\" problem\u2014experts forget what it's like not to know something and inadvertently skip over crucial foundations.</p> <p>The Expert Blind Spot</p> <p>Subject matter experts are often the worst at identifying prerequisites because the foundational knowledge has become so automatic they no longer notice it. This is why instructional designers add value\u2014they can ask the \"naive\" questions that experts have forgotten to ask.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#assumed-knowledge-setting-the-baseline","title":"Assumed Knowledge: Setting the Baseline","text":"<p>Assumed knowledge is a specific type of prerequisite\u2014it's knowledge that learners are expected to bring to the learning experience, which won't be taught or reviewed. It's the documented starting point.</p> <p>Every educational experience makes assumptions. A calculus course assumes algebra. A Python programming course might assume basic computer literacy but not prior programming experience. A corporate training on a new software tool might assume familiarity with the previous version.</p> <p>The key is to make these assumptions explicit. This serves multiple purposes:</p> <ul> <li>Learners can self-assess whether they're ready</li> <li>Instructional designers know what to build on vs. what to teach</li> <li>AI tools know what vocabulary and concepts to use freely</li> <li>Assessment can distinguish between missing prerequisites and failure to learn new content</li> </ul>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#diagram-prerequisite-vs-assumed-knowledge","title":"Diagram: Prerequisite vs Assumed Knowledge","text":"Prerequisite vs Assumed Knowledge <p>Type: diagram</p> <p>Purpose: Clarify the relationship between prerequisite knowledge, assumed knowledge, and new content being taught</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Students will be able to distinguish between prerequisite knowledge and assumed knowledge in instructional design.</p> <p>Layout: Layered/foundation diagram showing knowledge building blocks</p> <p>Components: Bottom layer (Foundation): - Label: \"Assumed Knowledge\" - Color: Solid gray - Description: \"What learners are expected to already know\u2014not taught in this course\" - Examples shown: \"Basic computer literacy\", \"Reading comprehension\", \"Prior course content\" - Visual: Solid, stable blocks</p> <p>Middle layer (Bridge): - Label: \"Prerequisite Knowledge\" - Color: Dotted/hatched pattern - Description: \"Required foundations\u2014may need review or scaffolding\" - Examples: \"Concepts from Chapter 1\", \"Prior module content\" - Visual: Blocks with some gaps that might need filling</p> <p>Top layer (Target): - Label: \"New Content\" - Color: Bright blue - Description: \"What this chapter/module teaches\" - Examples: \"MicroSim design\", \"Prerequisite analysis\" - Visual: New blocks being added</p> <p>Side annotations: - Arrow from Assumed to Prerequisite: \"Builds on\" - Arrow from Prerequisite to New: \"Enables\" - Gap indicator between layers: \"Potential struggle points\"</p> <p>Right side checklist: - \"Assumed Knowledge: Document but don't teach\" - \"Prerequisite Knowledge: Review or scaffold\" - \"New Content: Primary focus\"</p> <p>Implementation: HTML/CSS layered diagram, responsive design</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#documenting-assumptions","title":"Documenting Assumptions","text":"<p>When creating a MicroSim or any educational content, document assumptions clearly:</p> <pre><code>## Assumed Knowledge\n\nThis MicroSim assumes learners:\n- Can read and interpret basic graphs (x-y axes, data points)\n- Understand the concept of variables in mathematics\n- Have basic mouse/touchscreen interaction skills\n- Know what \"average\" means in everyday language\n\nThis MicroSim does NOT assume:\n- Prior statistics coursework\n- Programming knowledge\n- Familiarity with specific statistical software\n</code></pre> <p>This documentation helps everyone\u2014including the AI tools generating content\u2014understand the boundaries.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#concept-dependencies-mapping-the-learning-landscape","title":"Concept Dependencies: Mapping the Learning Landscape","text":"<p>Concept dependencies describe the relationships between concepts\u2014specifically, which concepts must be understood before others can be learned. These dependencies form a graph structure where concepts are nodes and dependencies are directed edges.</p> <p>Understanding dependencies is crucial because:</p> <ul> <li>Sequencing: You can't teach B before A if B depends on A</li> <li>Diagnosis: If a learner struggles with C, the problem might be with B or A</li> <li>Scaffolding: Knowing the chain helps identify where to provide support</li> <li>Assessment: Testing should check prerequisites before advanced concepts</li> </ul>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#types-of-dependencies","title":"Types of Dependencies","text":"<p>Dependencies come in several flavors:</p> Dependency Type Description Example Hard prerequisite Absolutely must know A before B Must know addition before multiplication Soft prerequisite Helpful to know A before B, but not strictly required Helpful to know history of AI before studying current models Corequisite A and B should be learned together Learning syntax and semantics of a programming construct Parallel A and B can be learned in any order Different sorting algorithms <p>Most learning graphs use hard prerequisites as their primary structure, with soft prerequisites as supplementary recommendations.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#diagram-concept-dependency-graph-example","title":"Diagram: Concept Dependency Graph Example","text":"Concept Dependency Graph Example <p>Type: graph-model</p> <p>Purpose: Demonstrate how concept dependencies form a directed acyclic graph (DAG) using this chapter's concepts as an example</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Students will be able to read a concept dependency graph and identify prerequisite chains.</p> <p>Node types: 1. Foundation Concepts (blue circles, largest)    - From Chapter 1: Learning Objective, Bloom's Taxonomy</p> <ol> <li>Chapter 2 Core Concepts (green circles, medium)</li> <li>Interactive Simulation</li> <li>MicroSim</li> <li>AI-Assisted Design</li> <li>Prerequisite Knowledge</li> <li>Assumed Knowledge</li> <li>Concept Dependencies</li> <li>Simulation Readiness</li> <li>Learning Pathway</li> </ol> <p>Sample dependency structure: - Learning Objective \u2192 MicroSim - Learning Objective \u2192 Prerequisite Knowledge - Interactive Simulation \u2192 MicroSim - Bloom's Taxonomy \u2192 Simulation Readiness - Prerequisite Knowledge \u2192 Assumed Knowledge - Prerequisite Knowledge \u2192 Concept Dependencies - Concept Dependencies \u2192 Learning Pathway - MicroSim + AI-Assisted Design \u2192 Simulation Readiness</p> <p>Edge types: - REQUIRES (solid arrows): Hard prerequisite - SUPPORTS (dashed arrows): Soft prerequisite</p> <p>Layout: Hierarchical left-to-right showing dependency flow</p> <p>Interactive features: - Click node to highlight all prerequisites (upstream) - Shift-click to highlight all dependents (downstream) - Hover to see concept definition - Toggle to show/hide soft prerequisites - Animation showing \"learning path\" traversal</p> <p>Visual styling: - Node size indicates number of dependents - Edge thickness indicates strength of dependency - Color gradient from blue (foundational) to green (advanced)</p> <p>Legend: - Solid arrow: \"Must learn first\" - Dashed arrow: \"Helpful to learn first\" - Node size: \"Number of concepts that depend on this\"</p> <p>Implementation: vis-network with hierarchical layout, responsive width</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#creating-dependency-maps","title":"Creating Dependency Maps","text":"<p>When designing a curriculum or analyzing existing content, creating a dependency map helps visualize the structure:</p> <ol> <li>List all concepts to be covered</li> <li>For each concept, ask: \"What must a learner know before they can understand this?\"</li> <li>Draw edges from prerequisites to dependent concepts</li> <li>Check for cycles\u2014if you find any, there's a logical error (you can't require A before B and B before A)</li> <li>Identify entry points\u2014concepts with no prerequisites (these are your starting points)</li> <li>Identify culminating concepts\u2014concepts that nothing else depends on (these are your endpoints)</li> </ol> <p>The resulting graph is a Directed Acyclic Graph (DAG)\u2014directed because dependencies have a direction, and acyclic because there can't be circular dependencies.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#learning-pathways-routes-through-the-content","title":"Learning Pathways: Routes Through the Content","text":"<p>A learning pathway is a sequence of concepts or learning experiences designed to take learners from their starting knowledge to a target goal. It's the route through the dependency graph.</p> <p>Given a well-constructed dependency map, multiple valid pathways might exist. Consider a map with these dependencies:</p> <pre><code>A \u2192 C\nB \u2192 C\nC \u2192 D\nC \u2192 E\nD \u2192 F\nE \u2192 F\n</code></pre> <p>To reach F, a learner must complete A or B (or both), then C, then both D and E. But the order of A and B, and the order of D and E, is flexible. Valid pathways include:</p> <ul> <li>A \u2192 B \u2192 C \u2192 D \u2192 E \u2192 F</li> <li>B \u2192 A \u2192 C \u2192 E \u2192 D \u2192 F</li> <li>A \u2192 C \u2192 D \u2192 B \u2192 C \u2192 E \u2192 F (with C reviewed)</li> </ul>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#personalized-pathways","title":"Personalized Pathways","text":"<p>One of the exciting possibilities with AI-assisted education is personalized pathways. Instead of all learners following the same fixed sequence, each learner's path can be:</p> <ul> <li>Adjusted based on prior knowledge: Skip what they already know</li> <li>Branched based on interests: Explore optional tangents</li> <li>Paced based on performance: Spend more time where needed</li> <li>Varied based on learning style: Different representations of the same concept</li> </ul> <p>MicroSims support personalized pathways beautifully because they're modular. A pathway system can:</p> <ol> <li>Assess what the learner already knows</li> <li>Identify the shortest path to the learning goal</li> <li>Recommend the next MicroSim to engage with</li> <li>Track progress and adjust recommendations</li> </ol>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#diagram-learning-pathway-visualization","title":"Diagram: Learning Pathway Visualization","text":"Learning Pathway Visualization <p>Type: microsim</p> <p>Purpose: Interactive demonstration of how learners can take different paths through a concept dependency graph based on their prior knowledge and goals</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Students will be able to trace valid learning pathways through a dependency graph and identify which concepts can be skipped based on prior knowledge.</p> <p>Canvas layout: - Main area (80%): Graph visualization showing concepts as nodes - Side panel (20%): Controls and pathway display</p> <p>Visual elements: - 12 concept nodes arranged in dependency graph - Nodes colored by status: gray (locked), blue (available), green (completed), gold (current target) - Directed edges showing prerequisites - Highlighted path showing current recommended route</p> <p>Sample concept graph: - Start: \"Basic Math\" (entry point, no prerequisites) - Level 1: \"Variables\", \"Equations\" (both require Basic Math) - Level 2: \"Functions\" (requires Variables), \"Inequalities\" (requires Equations) - Level 3: \"Linear Functions\", \"Quadratic Equations\" - Goal: \"Systems of Equations\"</p> <p>Interactive controls: - Checkbox list: \"Mark as already known\" for each concept - Dropdown: \"Select your goal\" - Button: \"Calculate shortest path\" - Button: \"Reset\" - Display: Current pathway as numbered list - Display: \"Estimated time to goal\"</p> <p>Behavior: - When concept marked as known, it turns green and dependents become available (blue) - Clicking \"Calculate shortest path\" highlights the recommended route - Clicking a concept shows its description and prerequisites - Invalid paths (skipping prerequisites) show warning</p> <p>Default state: - All concepts locked except entry points - Goal set to final concept - Full pathway displayed</p> <p>Animation: - Smooth transitions when concepts unlock - Path highlights with animated pulse</p> <p>Implementation: p5.js with force-directed graph layout, responsive canvas using updateCanvasSize()</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#simulation-readiness-is-this-concept-ready-for-microsim-treatment","title":"Simulation Readiness: Is This Concept Ready for MicroSim Treatment?","text":"<p>Not every learning objective benefits equally from a MicroSim. Simulation readiness is an assessment of whether a given concept or learning objective is a good candidate for interactive simulation.</p> <p>Some concepts are highly simulation-ready:</p> <ul> <li>Dynamic systems where variables interact</li> <li>Processes that unfold over time</li> <li>Cause-and-effect relationships</li> <li>Comparisons between approaches</li> <li>Concepts with visual or spatial components</li> <li>Procedures that can be practiced</li> </ul> <p>Other concepts are less simulation-ready:</p> <ul> <li>Pure memorization of facts (flashcards might be better than simulations)</li> <li>Historical events (timelines or narratives might be better)</li> <li>Definitions and terminology (glossaries or matching games)</li> <li>Emotional or ethical reasoning (discussions or case studies)</li> </ul>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#the-simulation-readiness-checklist","title":"The Simulation Readiness Checklist","text":"<p>When evaluating whether to build a MicroSim for a learning objective, consider these questions:</p> <p>Interactivity Value</p> <ul> <li>[ ] Can learners manipulate meaningful parameters?</li> <li>[ ] Do different inputs produce visibly different outputs?</li> <li>[ ] Is there something to discover through interaction?</li> <li>[ ] Would passive viewing be significantly less effective?</li> </ul> <p>Visual Representation</p> <ul> <li>[ ] Can the concept be represented visually?</li> <li>[ ] Would visualization clarify rather than complicate?</li> <li>[ ] Are there relationships that diagrams can show better than text?</li> </ul> <p>Feedback Opportunity</p> <ul> <li>[ ] Can the simulation provide immediate feedback?</li> <li>[ ] Can learners recognize when they're right or wrong?</li> <li>[ ] Is there a clear success state?</li> </ul> <p>Scope Appropriateness</p> <ul> <li>[ ] Can the concept be isolated into a single MicroSim?</li> <li>[ ] Is the concept not too trivial (making a MicroSim overkill)?</li> <li>[ ] Is the concept not too complex (requiring multiple MicroSims)?</li> </ul> <p>If you're answering \"yes\" to most of these, you have a good MicroSim candidate. If you're getting mostly \"no\" answers, consider a different content format.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#diagram-simulation-readiness-assessment","title":"Diagram: Simulation Readiness Assessment","text":"Simulation Readiness Assessment <p>Type: infographic</p> <p>Purpose: Provide a visual decision tree for assessing whether a learning objective is a good candidate for MicroSim development</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Given a learning objective, students will be able to assess its simulation readiness using established criteria.</p> <p>Layout: Flowchart/decision tree with four main evaluation branches</p> <p>Entry point: - \"Learning Objective\" box at top - Arrow down to first decision</p> <p>Decision branches:</p> <ol> <li>Interactivity Check (Blue branch)</li> <li>Question: \"Can learners manipulate meaningful parameters?\"</li> <li>Yes \u2192 Continue</li> <li> <p>No \u2192 \"Consider: Visualization, Video, or Text\"</p> </li> <li> <p>Visualization Check (Green branch)</p> </li> <li>Question: \"Can this be represented visually in a meaningful way?\"</li> <li>Yes \u2192 Continue</li> <li> <p>No \u2192 \"Consider: Audio, Discussion, or Reading\"</p> </li> <li> <p>Feedback Check (Yellow branch)</p> </li> <li>Question: \"Can the simulation provide immediate, meaningful feedback?\"</li> <li>Yes \u2192 Continue</li> <li> <p>No \u2192 \"Consider: Practice exercises with delayed feedback\"</p> </li> <li> <p>Scope Check (Orange branch)</p> </li> <li>Question: \"Can this fit in a single focused MicroSim?\"</li> <li>Yes \u2192 \"\u2713 Good MicroSim Candidate!\"</li> <li>Too small \u2192 \"Consider: Part of larger MicroSim\"</li> <li>Too large \u2192 \"Consider: Multiple MicroSims\"</li> </ol> <p>End states (color-coded boxes): - Green: \"Build a MicroSim\" - Yellow: \"Consider alternatives\" - Orange: \"Adjust scope first\"</p> <p>Side panel: Quick checklist version of all criteria</p> <p>Interactive features: - Click each decision point for detailed explanation - Hover for examples at each branch - Input field to test your own learning objective</p> <p>Implementation: HTML/CSS/JavaScript flowchart, responsive layout</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#putting-it-all-together-from-objective-to-microsim","title":"Putting It All Together: From Objective to MicroSim","text":"<p>Let's trace through the complete process of taking a learning objective and preparing it for MicroSim development.</p> <p>Starting Point: \"Students will understand how binary search works.\"</p> <p>Step 1: Refine the Objective This objective uses \"understand\"\u2014not specific enough. Let's rewrite: \"Students will predict which half of a sorted array will be searched next during binary search execution.\"</p> <p>Step 2: Identify Prerequisites What must learners already know?</p> <ul> <li>What an array is (conceptual)</li> <li>What \"sorted\" means (conceptual)</li> <li>How to compare values (procedural)</li> <li>Array indexing notation (terminological)</li> <li>Why searching matters (contextual)</li> </ul> <p>Step 3: Document Assumed Knowledge We'll assume learners know:</p> <ul> <li>Basic programming concepts</li> <li>What arrays/lists are</li> <li>How to compare numbers</li> </ul> <p>We won't assume:</p> <ul> <li>Any sorting algorithm knowledge</li> <li>Binary search specifically</li> <li>Big O notation</li> </ul> <p>Step 4: Map Dependencies Binary search understanding depends on:</p> <ul> <li>Arrays \u2192 Sorted Arrays \u2192 Binary Search</li> <li>Comparison Operations \u2192 Binary Search</li> <li>(Optional) Linear Search \u2192 Binary Search (for comparison)</li> </ul> <p>Step 5: Assess Simulation Readiness</p> <ul> <li>Interactivity: YES\u2014learners can step through and predict</li> <li>Visualization: YES\u2014array and pointer positions are visual</li> <li>Feedback: YES\u2014predictions can be immediately verified</li> <li>Scope: YES\u2014single algorithm, single MicroSim</li> </ul> <p>Step 6: Specify the MicroSim Now we're ready to write a detailed specification that an AI tool or developer can implement. We know exactly what the learner should already understand, what we're teaching, and why a simulation is the right format.</p> <p>The Power of Preparation</p> <p>This six-step process might seem like a lot of work before any \"real\" development happens. But this preparation is what makes AI-assisted generation effective. A clear specification with documented prerequisites and dependencies produces dramatically better results than a vague request like \"make a binary search simulator.\"</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#chapter-summary","title":"Chapter Summary","text":"<p>You've now learned the foundational concepts that bridge learning objectives and MicroSim development:</p> <ul> <li>MicroSims are small, focused simulations targeting single learning objectives\u2014coined by Valarie Lockhart in 2023 and formalized in the 2025 research framework</li> <li>Interactive simulations put learners in control, creating powerful feedback loops for learning</li> <li>AI-assisted design amplifies instructional designers' capabilities but requires clear specifications and human judgment</li> <li>Prerequisite knowledge is what learners must know before engaging with new content</li> <li>Assumed knowledge is the documented baseline that won't be taught</li> <li>Concept dependencies form graphs that structure curriculum sequencing</li> <li>Learning pathways are routes through the dependency graph from start to goal</li> <li>Simulation readiness assesses whether a concept is a good MicroSim candidate</li> </ul> <p>With these tools, you're ready to start analyzing any learning domain for MicroSim potential. In the next chapter, we'll explore the pattern library of visualization types and match them to different kinds of learning objectives.</p> <p>Remember: The world needs better education, and now we have tools to create it at scale. That's not just professionally exciting\u2014it's genuinely optimistic. Every well-designed MicroSim has the potential to help someone learn something they couldn't learn before. And that makes the world a little bit better, one simulation at a time.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/#review-questions","title":"Review Questions","text":"Who coined the term 'MicroSim' and when? <p>Valarie Lockhart coined the term \"MicroSim\" in 2023 to describe small, focused interactive simulations designed to teach single concepts.</p> What are the key differences between traditional educational simulations and MicroSims? <p>Key differences include:</p> Traditional Simulation MicroSim Covers many concepts Covers one concept Months to develop Hours to generate Requires development team Generated with AI assistance Standalone application Embeds in any web page One-size-fits-all Adaptable to audience <p>The fundamental shift is from monolithic, expensive productions to modular, composable, AI-assisted creations.</p> What is the difference between prerequisite knowledge and assumed knowledge? <p>Prerequisite knowledge is what learners must know before engaging with new content\u2014it may need to be reviewed or scaffolded.</p> <p>Assumed knowledge is a subset of prerequisites that is documented as the baseline expectation\u2014it won't be taught or reviewed in the current content. It's the explicit starting line.</p> What four criteria should be evaluated to assess simulation readiness? <p>The four key criteria are:</p> <ol> <li>Interactivity Value: Can learners manipulate meaningful parameters with different outcomes?</li> <li>Visual Representation: Can the concept be effectively shown visually?</li> <li>Feedback Opportunity: Can the simulation provide immediate, meaningful feedback?</li> <li>Scope Appropriateness: Can the concept fit in a single focused MicroSim?</li> </ol> <p>If most answers are \"yes,\" the concept is a good MicroSim candidate.</p> Where can you find hundreds of working MicroSim examples? <p>The MicroSim Textbook contains hundreds of working MicroSim examples spanning audiences from kindergarten to graduate school, demonstrating the breadth of what's possible with AI-assisted educational simulation design.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/quiz/","title":"Quiz: Prerequisite Analysis and MicroSim Fundamentals","text":"<p>Test your understanding of MicroSims, interactive simulations, prerequisite analysis, and concept dependencies with these questions.</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/quiz/#1-who-coined-the-term-microsim-and-when","title":"1. Who coined the term \"MicroSim\" and when?","text":"<ol> <li>Benjamin Bloom in 1956</li> <li>Valarie Lockhart in 2023</li> <li>Dan McCreary in 2025</li> <li>Lorin Anderson in 2001</li> </ol> Show Answer <p>The correct answer is B. Valarie Lockhart coined the term \"MicroSim\" in 2023 to describe small, focused interactive simulations designed to teach single concepts. The name captures the essence: micro for small, single-concept learning experiences, and sim for simulation\u2014interactive digital environments for experimentation.</p> <p>Concept Tested: MicroSim</p> <p>See: Chapter Content</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/quiz/#2-what-is-the-primary-characteristic-that-distinguishes-microsims-from-traditional-educational-simulations","title":"2. What is the primary characteristic that distinguishes MicroSims from traditional educational simulations?","text":"<ol> <li>MicroSims are more visually appealing and have better graphics</li> <li>MicroSims require a development team while traditional simulations can be built by individuals</li> <li>MicroSims cover a single concept while traditional simulations cover many concepts</li> <li>MicroSims are only for elementary education while traditional simulations are for higher education</li> </ol> Show Answer <p>The correct answer is C. The key distinguishing feature is scope: MicroSims cover one concept and do that one thing well, while traditional simulations typically cover many concepts. This modularity allows MicroSims to be created in hours (vs. months), composed into learning pathways, and easily updated independently.</p> <p>Concept Tested: MicroSim</p> <p>See: Chapter Content</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/quiz/#3-what-makes-an-interactive-simulation-effective-for-learning","title":"3. What makes an interactive simulation effective for learning?","text":"<ol> <li>High-quality graphics and animations</li> <li>The feedback loop where learners take actions, observe consequences, adjust mental models, and try again</li> <li>Integration with social media platforms for sharing progress</li> <li>The ability to work without internet connection</li> </ol> Show Answer <p>The correct answer is B. What makes interactive simulations effective isn't just interactivity\u2014it's the feedback loop. Learners take an action, observe a consequence, adjust their mental model, and try again. This cycle mirrors how humans naturally learn through experience and is the core pedagogical value of simulations.</p> <p>Concept Tested: Interactive Simulation</p> <p>See: Chapter Content</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/quiz/#4-what-is-the-difference-between-prerequisite-knowledge-and-assumed-knowledge","title":"4. What is the difference between prerequisite knowledge and assumed knowledge?","text":"<ol> <li>Prerequisite knowledge is optional; assumed knowledge is required</li> <li>Prerequisite knowledge is for advanced courses; assumed knowledge is for introductory courses</li> <li>Prerequisite knowledge may need review or scaffolding; assumed knowledge is the documented baseline that won't be taught</li> <li>There is no difference; the terms are interchangeable</li> </ol> Show Answer <p>The correct answer is C. Prerequisite knowledge is what learners must know before engaging with new content\u2014it may need to be reviewed or scaffolded for learners who are rusty. Assumed knowledge is a specific type of prerequisite that is explicitly documented as the baseline expectation and will not be taught or reviewed in the current content.</p> <p>Concept Tested: Prerequisite Knowledge, Assumed Knowledge</p> <p>See: Chapter Content</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/quiz/#5-in-the-human-ai-collaboration-loop-which-steps-are-performed-by-humans-rather-than-ai","title":"5. In the human-AI collaboration loop, which steps are performed by humans rather than AI?","text":"<ol> <li>Only the initial specification step</li> <li>Specification, evaluation, refinement, and testing with learners</li> <li>Only the final testing with learners</li> <li>None\u2014AI performs all steps in modern workflows</li> </ol> Show Answer <p>The correct answer is B. Humans bookend the AI-assisted design process. Humans specify the learning objective and constraints, evaluate against pedagogical criteria, refine specifications based on what's missing, and test with actual learners. AI accelerates the generation steps but doesn't replace human judgment at critical decision points.</p> <p>Concept Tested: AI-Assisted Design</p> <p>See: Chapter Content</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/quiz/#6-what-are-concept-dependencies","title":"6. What are concept dependencies?","text":"<ol> <li>The relationships between concepts showing which must be understood before others can be learned</li> <li>The list of software libraries required to run a MicroSim</li> <li>The connections between chapters in a textbook</li> <li>The budget required for developing educational materials</li> </ol> Show Answer <p>The correct answer is A. Concept dependencies describe the relationships between concepts\u2014specifically, which concepts must be understood before others can be learned. These dependencies form a graph structure (a DAG\u2014directed acyclic graph) where concepts are nodes and dependencies are directed edges, determining sequencing, diagnosis, and scaffolding.</p> <p>Concept Tested: Concept Dependencies</p> <p>See: Chapter Content</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/quiz/#7-a-learning-pathway-through-a-concept-dependency-graph-should-be-described-as","title":"7. A learning pathway through a concept dependency graph should be described as:","text":"<ol> <li>A fixed, predetermined sequence that all learners must follow identically</li> <li>A route through the dependency graph from starting knowledge to a target goal, which may vary based on prior knowledge</li> <li>The shortest possible path that skips all prerequisite concepts</li> <li>A path that only includes concepts at the Create level of Bloom's Taxonomy</li> </ol> Show Answer <p>The correct answer is B. A learning pathway is a sequence of concepts or learning experiences designed to take learners from their starting knowledge to a target goal. Given a dependency map, multiple valid pathways might exist. With AI-assisted education, pathways can be personalized based on prior knowledge, interests, performance, and learning style.</p> <p>Concept Tested: Learning Pathway</p> <p>See: Chapter Content</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/quiz/#8-when-assessing-simulation-readiness-which-of-the-following-is-not-one-of-the-four-key-evaluation-criteria","title":"8. When assessing simulation readiness, which of the following is NOT one of the four key evaluation criteria?","text":"<ol> <li>Interactivity Value</li> <li>Visual Representation</li> <li>Development Cost</li> <li>Scope Appropriateness</li> </ol> Show Answer <p>The correct answer is C. Development cost is not one of the four simulation readiness criteria. The four key criteria are: (1) Interactivity Value\u2014can learners manipulate meaningful parameters? (2) Visual Representation\u2014can the concept be effectively shown visually? (3) Feedback Opportunity\u2014can the simulation provide immediate feedback? (4) Scope Appropriateness\u2014can the concept fit in a single focused MicroSim?</p> <p>Concept Tested: Simulation Readiness</p> <p>See: Chapter Content</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/quiz/#9-given-a-learning-objective-about-understanding-binary-search-which-prerequisite-knowledge-would-be-most-essential","title":"9. Given a learning objective about understanding binary search, which prerequisite knowledge would be most essential?","text":"<ol> <li>Knowledge of web development and JavaScript</li> <li>Understanding of what a sorted array is and how to compare values</li> <li>Familiarity with multiple programming languages</li> <li>Experience with database query optimization</li> </ol> Show Answer <p>The correct answer is B. Binary search requires learners to understand what an array is, what \"sorted\" means, and how to compare values. Without these foundational concepts, the algorithm's logic of repeatedly dividing a sorted collection in half and comparing to find a target value cannot be understood. Web development and programming languages are not required for understanding the concept.</p> <p>Concept Tested: Prerequisite Knowledge</p> <p>See: Chapter Content</p>"},{"location":"chapters/02-prerequisite-analysis-microsim-fundamentals/quiz/#10-which-type-of-learning-objective-is-least-suited-for-microsim-treatment","title":"10. Which type of learning objective is LEAST suited for MicroSim treatment?","text":"<ol> <li>Understanding how supply and demand curves interact</li> <li>Memorizing the capitals of all 50 U.S. states</li> <li>Analyzing how different sorting algorithms compare in performance</li> <li>Applying Newton's laws to predict projectile motion</li> </ol> Show Answer <p>The correct answer is B. Pure memorization of facts (like state capitals) is less simulation-ready because there's limited interactivity value, no dynamic cause-and-effect relationships to explore, and flashcards or matching games might be more effective. The other options involve dynamic systems, cause-and-effect relationships, or comparisons that benefit from interactive visualization and exploration.</p> <p>Concept Tested: Simulation Readiness</p> <p>See: Chapter Content</p>"},{"location":"chapters/03-microsim-pattern-library/","title":"The MicroSim Pattern Library","text":""},{"location":"chapters/03-microsim-pattern-library/#summary","title":"Summary","text":"<p>This chapter provides a comprehensive overview of visualization paradigms used in MicroSim development. You will explore motion simulations, physics engines, dynamic systems, and cause-effect displays. The chapter covers relationship graphs, hierarchies, dependency mapping, and influence diagrams for network-style visualizations. You will also learn about sequence displays, timelines, charts for trends and distributions, spatial visualizations, flowcharts, state machines, and classification displays. By mastering these patterns, you will be able to select the most appropriate visualization approach for any learning objective.</p>"},{"location":"chapters/03-microsim-pattern-library/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 24 concepts from the learning graph:</p> <ol> <li>Visualization Paradigm</li> <li>Network Graph</li> <li>Venn Diagram</li> <li>Set Visualization</li> <li>Motion Simulation</li> <li>Physics Simulation</li> <li>Dynamic Systems</li> <li>Cause-Effect Display</li> <li>Relationship Graph</li> <li>Hierarchy Display</li> <li>Dependency Mapping</li> <li>Influence Diagram</li> <li>Sequence Display</li> <li>Process Timeline</li> <li>Trend Chart</li> <li>Distribution Chart</li> <li>Correlation Display</li> <li>Spatial Visualization</li> <li>Flowchart</li> <li>State Machine Diagram</li> <li>Classification Display</li> <li>Paradigm Selection</li> <li>Concept Characteristics</li> <li>Visual Affordances</li> </ol>"},{"location":"chapters/03-microsim-pattern-library/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Foundations of Learning Objective Analysis</li> <li>Chapter 2: Prerequisite Analysis and MicroSim Fundamentals</li> </ul>"},{"location":"chapters/03-microsim-pattern-library/#welcome-to-the-visualization-buffet","title":"Welcome to the Visualization Buffet","text":"<p>If instructional design were a restaurant, this chapter would be the menu. And what a menu it is! We're about to explore over two dozen ways to visualize learning\u2014from bouncing balls to branching flowcharts, from nested circles to networked nodes. By the end, you'll have a mental cookbook of visualization patterns ready to deploy whenever a learning objective crosses your desk.</p> <p>Here's the beautiful truth about MicroSim development: you don't need to reinvent the wheel every time you build something. Most educational concepts fit into a handful of visualization patterns, and once you recognize which pattern matches your content, the design process becomes dramatically simpler. It's like having a key that unlocks the right template.</p> <p>Think of this chapter as your pattern recognition training. We're going to look at each major visualization paradigm, understand when it shines, and see real examples in action. Ready to become a visualization virtuoso? Let's dive in!</p>"},{"location":"chapters/03-microsim-pattern-library/#understanding-visualization-paradigms","title":"Understanding Visualization Paradigms","text":"<p>A visualization paradigm is a fundamental approach to representing information visually. It's the \"shape\" of your visualization\u2014not the specific colors or labels, but the underlying structure that organizes information.</p> <p>Choosing the right paradigm is like choosing the right vehicle for a journey. A sedan is great for highway driving, but terrible for crossing a river. A boat handles water beautifully but won't get you far on land. Similarly, a timeline is perfect for showing historical progression but hopeless for displaying hierarchical relationships.</p> Paradigm Type Best For Examples Motion/Physics Continuous processes, cause-effect, dynamic systems Pendulums, projectiles, wave propagation Network/Graph Relationships, connections, dependencies Social networks, concept maps, organizational charts Timeline/Sequence Chronological progression, processes, workflows Historical events, project phases, algorithms Chart/Statistical Quantitative comparisons, trends, distributions Bar charts, line graphs, scatter plots Spatial/Map Geographic or spatial relationships Location data, floor plans, anatomy Flow/State Decisions, processes, state transitions Flowcharts, state machines, decision trees Set/Classification Categorization, overlapping groups, membership Venn diagrams, classification matrices <p>The magic happens when you match the paradigm to your content's inherent structure. Force a concept into the wrong paradigm, and learners struggle. Find the right fit, and understanding clicks into place like a puzzle piece finding its home.</p>"},{"location":"chapters/03-microsim-pattern-library/#diagram-visualization-paradigm-selection-guide","title":"Diagram: Visualization Paradigm Selection Guide","text":"Visualization Paradigm Selection Guide <p>Type: infographic</p> <p>Purpose: Help learners quickly identify which visualization paradigm best fits their content type</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Students will be able to select an appropriate visualization paradigm based on the characteristics of their learning content.</p> <p>Layout: Decision-tree style flowchart with branching paths</p> <p>Starting Question: \"What type of information are you visualizing?\"</p> <p>Branch 1: \"Changes over time?\" - Yes \u2192 \"Continuous or discrete?\"   - Continuous \u2192 Motion/Physics Simulation   - Discrete \u2192 Timeline/Sequence Display</p> <p>Branch 2: \"Relationships between entities?\" - Yes \u2192 \"Hierarchical or networked?\"   - Hierarchical \u2192 Hierarchy Display   - Networked \u2192 Network Graph</p> <p>Branch 3: \"Quantities or measurements?\" - Yes \u2192 \"Comparing categories or showing trends?\"   - Categories \u2192 Bar/Distribution Chart   - Trends \u2192 Line/Trend Chart   - Relationships \u2192 Correlation/Scatter Display</p> <p>Branch 4: \"Categories or classifications?\" - Yes \u2192 \"Overlapping or mutually exclusive?\"   - Overlapping \u2192 Venn Diagram   - Exclusive \u2192 Classification Display</p> <p>Branch 5: \"Processes or decisions?\" - Yes \u2192 \"Sequential or state-based?\"   - Sequential \u2192 Flowchart   - State-based \u2192 State Machine Diagram</p> <p>Branch 6: \"Physical locations?\" - Yes \u2192 Spatial/Map Visualization</p> <p>Visual Style: Clean flowchart with rounded rectangles and arrows Color Scheme: Each paradigm family gets a distinct color - Motion/Physics: Orange - Network/Graph: Blue - Timeline/Sequence: Green - Charts: Purple - Spatial: Teal - Flow/State: Red - Set/Classification: Yellow</p> <p>Interactive Features: - Click any endpoint to see example MicroSims - Hover over branches for additional decision criteria - Responsive design that adapts to window size</p> <p>Implementation: HTML/CSS/JavaScript with SVG graphics</p>"},{"location":"chapters/03-microsim-pattern-library/#motion-and-physics-simulations","title":"Motion and Physics Simulations","text":"<p>Let's start with the paradigm that brings learning to life\u2014literally. Motion simulations show objects moving through space, while physics simulations add realistic forces like gravity, friction, and acceleration. These are the rock stars of STEM education because they let learners see abstract equations become tangible reality.</p> <p>When Isaac Newton formulated $F = ma$, he probably didn't envision students dragging sliders on glowing screens to watch virtual apples fall. But here we are, living in the future, and it's glorious.</p>"},{"location":"chapters/03-microsim-pattern-library/#when-motion-simulations-shine","title":"When Motion Simulations Shine","text":"<p>Motion simulations are your go-to choice when teaching:</p> <ul> <li>Kinematics: Position, velocity, acceleration relationships</li> <li>Dynamics: Forces and their effects on motion</li> <li>Oscillations: Pendulums, springs, waves</li> <li>Collisions: Momentum and energy transfer</li> <li>Projectile motion: Trajectories under gravity</li> </ul> <p>The key characteristic of motion simulations is that time is a primary variable. Things change, move, evolve. If your concept involves something going from point A to point B (or oscillating between them), motion simulation is probably your paradigm.</p> <p>Real-World MicroSim Examples</p> <p>The MicroSim library includes numerous motion simulations such as the Integer Operations Number Line simulation, which animates addition and subtraction as movement along a number line\u2014turning abstract arithmetic into visual journeys.</p>"},{"location":"chapters/03-microsim-pattern-library/#physics-simulation-adding-realism","title":"Physics Simulation: Adding Realism","text":"<p>Physics simulations take motion simulations up a notch by incorporating realistic physical laws. Instead of just moving objects around, they calculate forces, apply constraints, and produce behavior that matches real-world expectations.</p> <p>The learning power of physics simulations comes from their predictive capability. Learners can hypothesize what will happen, run the simulation, and immediately see if their mental model was correct. This predict-observe-explain cycle is educational gold.</p> Physics Concept Simulation Approach Key Parameters Gravity Constant downward acceleration g (9.8 m/s\u00b2) Friction Velocity-dependent opposing force \u03bc (coefficient) Springs Restoring force proportional to displacement k (spring constant) Collisions Momentum and energy conservation Elasticity coefficient Air resistance Velocity-squared drag Drag coefficient"},{"location":"chapters/03-microsim-pattern-library/#diagram-physics-simulation-architecture","title":"Diagram: Physics Simulation Architecture","text":"Physics Simulation Architecture <p>Type: diagram</p> <p>Purpose: Show the computational structure of a physics simulation engine</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Students will be able to explain how physics simulations calculate motion over time using the simulation loop pattern.</p> <p>Components to show: - Input Layer:   - User Controls (sliders, buttons)   - Initial Conditions (position, velocity)   - Physical Constants (gravity, friction)</p> <ul> <li>Simulation Loop (central, emphasized):</li> <li>Calculate Forces</li> <li>Update Acceleration (F = ma)</li> <li>Update Velocity (v += a * dt)</li> <li>Update Position (x += v * dt)</li> <li> <p>Check Boundaries/Collisions</p> </li> <li> <p>Output Layer:</p> </li> <li>Render Objects</li> <li>Update Displays</li> <li>Record Data</li> </ul> <p>Connections: - Arrow from Input Layer to Simulation Loop - Circular arrow showing loop iteration - Arrow from Loop to Output Layer - Feedback arrow from Output back to Input (for interactive controls)</p> <p>Visual Style: Block diagram with clear data flow Color Scheme: - Input: Green (user interaction) - Simulation Loop: Blue (computation) - Output: Orange (visualization)</p> <p>Labels: - \"Time Step (dt)\" on the loop - \"60 FPS = 16.67ms per frame\" - \"Forces determine everything\"</p> <p>Implementation: SVG diagram with optional animation showing the loop cycling</p>"},{"location":"chapters/03-microsim-pattern-library/#dynamic-systems-when-everything-affects-everything","title":"Dynamic Systems: When Everything Affects Everything","text":"<p>Dynamic systems represent an important subset of physics simulations where multiple variables interact and influence each other over time. Unlike simple motion where one object follows a trajectory, dynamic systems involve feedback loops, equilibrium states, and emergent behavior.</p> <p>Think of a predator-prey ecosystem. Rabbits eat grass and multiply. Foxes eat rabbits and multiply. More foxes mean fewer rabbits. Fewer rabbits mean fewer foxes. Fewer foxes mean more rabbits. And round and round we go\u2014a beautiful, chaotic dance that simple equations somehow capture.</p> <p>Dynamic system simulations are particularly powerful for teaching:</p> <ul> <li>Systems thinking: Understanding how parts interact</li> <li>Equilibrium concepts: Stable vs. unstable states</li> <li>Feedback loops: Positive and negative feedback</li> <li>Emergent behavior: Complex patterns from simple rules</li> <li>Sensitivity to initial conditions: Chaos theory basics</li> </ul>"},{"location":"chapters/03-microsim-pattern-library/#cause-effect-displays-making-connections-visible","title":"Cause-Effect Displays: Making Connections Visible","text":"<p>Cause-effect displays are a specialized type of motion/dynamic simulation focused specifically on showing causal relationships. When you change X, what happens to Y? These simulations make abstract causation concrete and manipulable.</p> <p>The pedagogical power here is enormous. Instead of telling learners \"increasing temperature increases reaction rate,\" you let them drag a temperature slider and watch molecules bounce faster and collisions increase. The cause-effect relationship becomes visceral, not just verbal.</p>"},{"location":"chapters/03-microsim-pattern-library/#diagram-cause-effect-display-template","title":"Diagram: Cause-Effect Display Template","text":"Cause-Effect Display Template <p>Type: microsim</p> <p>Purpose: Demonstrate the standard structure of a cause-effect display MicroSim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Students will be able to design cause-effect MicroSims that clearly show relationships between input variables and output behaviors.</p> <p>Canvas Layout: - Top section: Title and brief instructions - Left side (40%): Control panel with input variables - Center (50%): Main visualization area - Bottom: Output metrics and explanation</p> <p>Control Panel Elements: - 2-4 sliders for independent variables - Each slider has:   - Label with current value   - Min/max range indicators   - Units displayed - Reset button - \"Show relationship\" toggle</p> <p>Visualization Area: - Animated elements that respond to slider changes - Color coding that intensifies/diminishes with effect - Clear visual metaphor for the concept - Real-time updates (no lag between input and display)</p> <p>Output Metrics: - Numerical display of dependent variable(s) - Simple graph showing relationship (optional) - Text explanation of current state</p> <p>Behavior: - Changes to any slider immediately update visualization - Smooth transitions between states - Clear visual feedback for cause-effect connection</p> <p>Design Principles: - Maximum 4 input variables (cognitive load management) - Direct manipulation (sliders, not text input) - Immediate feedback (under 100ms response) - Multiple representations (visual + numerical)</p> <p>Implementation: p5.js with responsive canvas</p>"},{"location":"chapters/03-microsim-pattern-library/#network-and-graph-visualizations","title":"Network and Graph Visualizations","text":"<p>If motion simulations show how things move, network visualizations show how things connect. This paradigm family is all about relationships\u2014who knows whom, what depends on what, how ideas link together.</p> <p>In our increasingly connected world, network thinking is becoming essential. Social networks, supply chains, neural networks, concept maps\u2014they all share the same underlying structure of nodes and edges. Master this paradigm, and you've got a tool for visualizing an enormous range of concepts.</p>"},{"location":"chapters/03-microsim-pattern-library/#network-graphs-nodes-and-edges","title":"Network Graphs: Nodes and Edges","text":"<p>A network graph (also called a graph visualization) consists of:</p> <ul> <li>Nodes (vertices): The entities being related</li> <li>Edges (links): The relationships between entities</li> </ul> <p>That's it. Two components. Yet from this simple foundation, we can represent astonishing complexity.</p> <p>Real-World MicroSim Examples</p> <p>The MicroSim library includes the Learning Graph Viewer (vis-network), which displays concept dependencies as an interactive network where learners can explore how ideas connect and build upon each other.</p> <p>Network graphs excel at teaching:</p> <ul> <li>Social relationships: Who knows whom, influence patterns</li> <li>Dependencies: What requires what, prerequisite structures</li> <li>Semantic relationships: How concepts relate</li> <li>Organizational structures: Reporting relationships, team compositions</li> <li>System architectures: Components and their connections</li> </ul> Network Property What It Reveals Teaching Application Degree (connections per node) Importance/influence Identify key concepts Clustering Community structure Find related topics Path length Separation between nodes Trace concept dependencies Centrality Most connected nodes Find prerequisite concepts Components Disconnected subgraphs Identify independent topics"},{"location":"chapters/03-microsim-pattern-library/#relationship-graphs-adding-meaning-to-connections","title":"Relationship Graphs: Adding Meaning to Connections","text":"<p>While basic network graphs just show that connections exist, relationship graphs add semantic meaning to those connections. The edge isn't just \"connected to\"\u2014it's \"depends on,\" \"influences,\" \"conflicts with,\" or \"supports.\"</p> <p>In educational contexts, relationship graphs help learners understand not just that concepts are related, but how they're related. This is crucial for building accurate mental models.</p>"},{"location":"chapters/03-microsim-pattern-library/#diagram-relationship-graph-types","title":"Diagram: Relationship Graph Types","text":"Relationship Graph Types <p>Type: graph-model</p> <p>Purpose: Demonstrate different types of relationships that can be represented in graph visualizations</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Students will be able to distinguish between different relationship types and select appropriate edge representations for their content.</p> <p>Node Types: 1. Concept Node (circles)    - Color: Light blue    - Properties: name, description</p> <ol> <li>Example Node (rounded rectangles)</li> <li>Color: Light green</li> <li>Properties: name, detail</li> </ol> <p>Edge Types (with visual encoding): 1. \"PREREQUISITE\" (solid arrow)    - Direction: From required to dependent    - Color: Dark blue    - Meaning: Must understand A before B</p> <ol> <li>\"RELATED TO\" (dashed line, no arrow)</li> <li>Direction: Bidirectional</li> <li>Color: Gray</li> <li> <p>Meaning: Conceptually connected</p> </li> <li> <p>\"CONTRASTS WITH\" (dotted line, double arrow)</p> </li> <li>Direction: Bidirectional</li> <li>Color: Red</li> <li> <p>Meaning: Important differences</p> </li> <li> <p>\"EXEMPLIFIES\" (solid arrow)</p> </li> <li>Direction: From example to concept</li> <li>Color: Green</li> <li> <p>Meaning: Instance of category</p> </li> <li> <p>\"INFLUENCES\" (wavy arrow)</p> </li> <li>Direction: Unidirectional</li> <li>Color: Orange</li> <li>Meaning: Causal relationship</li> </ol> <p>Sample Data: - \"Learning Objective\" --PREREQUISITE\u2192 \"MicroSim Design\" - \"Learning Objective\" \u2190EXEMPLIFIES-- \"Explain photosynthesis\" - \"Motion Simulation\" --CONTRASTS WITH-- \"Static Diagram\" - \"Visualization\" --RELATED TO-- \"Cognitive Load\" - \"Complexity\" --INFLUENCES\u2192 \"Learning Time\"</p> <p>Layout: Force-directed with relationship type affecting edge length - PREREQUISITE: Short (tightly coupled) - RELATED TO: Medium - CONTRASTS WITH: Long (conceptual distance)</p> <p>Interactive Features: - Hover edge to see relationship description - Click node to highlight all its relationships - Filter by relationship type - Zoom and pan</p> <p>Visual Styling: - Edge thickness based on strength (if applicable) - Animated arrows for directional relationships - Legend explaining all edge types</p> <p>Implementation: vis-network JavaScript library</p>"},{"location":"chapters/03-microsim-pattern-library/#hierarchy-displays-top-down-organization","title":"Hierarchy Displays: Top-Down Organization","text":"<p>Hierarchy displays are specialized network graphs where relationships are strictly top-down (or parent-child). Every node has at most one parent, creating a tree structure.</p> <p>Hierarchies are everywhere in education:</p> <ul> <li>Taxonomy structures: Kingdom \u2192 Phylum \u2192 Class \u2192 Order...</li> <li>Organizational charts: CEO \u2192 VPs \u2192 Directors \u2192 Managers...</li> <li>Outline structures: Chapter \u2192 Section \u2192 Subsection...</li> <li>Classification trees: Decisions branching to subcategories</li> </ul> <p>The visual convention for hierarchies places parent nodes above children, with connections flowing downward. This spatial arrangement reinforces the conceptual relationship.</p> <p>Real-World MicroSim Examples</p> <p>The Number Systems Hierarchy Diagram shows how natural numbers nest within whole numbers, which nest within integers, which nest within rationals\u2014a perfect example of hierarchical classification.</p>"},{"location":"chapters/03-microsim-pattern-library/#dependency-mapping-following-the-thread","title":"Dependency Mapping: Following the Thread","text":"<p>Dependency mapping visualizes what depends on what. Unlike general networks where edges might mean anything, dependency maps specifically show prerequisite relationships: \"Before you can understand X, you must understand Y.\"</p> <p>This paradigm is crucial for instructional design because learning itself is a dependency problem. You can't teach calculus to someone who doesn't understand algebra. You can't explain recursion to someone who doesn't grasp function calls. Dependency maps make these invisible constraints visible.</p> <p>The Learning Graph Viewer MicroSim is a prime example of dependency mapping in action\u2014showing exactly which concepts must come before others in a course structure.</p>"},{"location":"chapters/03-microsim-pattern-library/#influence-diagrams-causal-networks","title":"Influence Diagrams: Causal Networks","text":"<p>Influence diagrams show how factors affect outcomes in complex systems. Unlike simple cause-effect displays (which typically show one cause affecting one effect), influence diagrams map entire networks of causal relationships.</p> <p>These are particularly powerful for:</p> <ul> <li>Decision analysis: How choices lead to outcomes</li> <li>Systems dynamics: Feedback loops and delays</li> <li>Risk assessment: Factors affecting probability</li> <li>Policy analysis: Intervention points in complex systems</li> </ul> <p>The visual encoding typically uses: - Nodes for variables - Arrows for causal influence - Arrow thickness or color for strength of influence - Plus/minus signs for direction of effect</p>"},{"location":"chapters/03-microsim-pattern-library/#set-visualizations-and-classification","title":"Set Visualizations and Classification","text":"<p>Sometimes the key insight isn't how things connect (networks) or how they move (simulations)\u2014it's how they're categorized. Set visualizations show membership, overlap, and classification relationships.</p>"},{"location":"chapters/03-microsim-pattern-library/#venn-diagrams-the-classic-overlap","title":"Venn Diagrams: The Classic Overlap","text":"<p>Ah, the Venn diagram\u2014that beloved trio (or duo, or quartet) of overlapping circles that has graced whiteboards and textbooks for over a century. John Venn introduced them in 1880, and educators have been drawing them ever since.</p> <p>Venn diagrams are perfect for showing:</p> <ul> <li>Set membership: What belongs where</li> <li>Overlapping categories: Items that fit multiple groups</li> <li>Logical relationships: Union, intersection, difference</li> <li>Comparisons: Similarities and differences between concepts</li> </ul> <p>The power of the Venn diagram is its spatial mapping of logical relationships. Items in the overlap region share properties of both sets. Items outside all circles belong to neither category. The visual layout is the logic.</p> <p>Real-World MicroSim Examples</p> <p>The Number Systems Venn Diagram and Number Types Venn Diagram MicroSims beautifully illustrate how different number types (Natural, Whole, Integer, Rational, Real) nest within each other, with interactive hover states explaining each region.</p>"},{"location":"chapters/03-microsim-pattern-library/#diagram-venn-diagram-builder","title":"Diagram: Venn Diagram Builder","text":"Interactive Venn Diagram Builder <p>Type: microsim</p> <p>Purpose: Allow learners to create and manipulate Venn diagrams to understand set relationships</p> <p>Bloom Taxonomy Level: Apply, Analyze</p> <p>Learning Objective: Students will be able to construct Venn diagrams representing relationships between categories and correctly place items in appropriate regions.</p> <p>Canvas Layout (responsive): - Left side (30%): Item palette and controls - Center (60%): Main Venn diagram area - Right side (10%): Region labels/counts</p> <p>Control Panel: - Number of sets selector: 2, 3, or 4 - Set name text inputs (editable labels) - Item list with drag handles - \"Add Item\" button - \"Reset\" button</p> <p>Venn Diagram Area: - 2 sets: Two overlapping circles - 3 sets: Three overlapping circles (classic Venn) - 4 sets: Four overlapping ellipses (more complex) - Each region highlighted on hover - Item icons/labels positioned within regions</p> <p>Interaction: - Drag items from palette to diagram regions - Items snap to nearest valid region - Highlight region when dragging over - Double-click item to remove - Click region to see all items</p> <p>Visual Styling: - Each set: distinct semi-transparent color - Overlap regions: blended colors - Selected items: highlighted border - Hover states: subtle glow effect</p> <p>Validation: - Optional \"check\" button to verify placement - Green checkmark for correct placement - Red X with explanation for incorrect</p> <p>Implementation: p5.js or SVG with drag-and-drop</p>"},{"location":"chapters/03-microsim-pattern-library/#set-visualizations-beyond-the-circle","title":"Set Visualizations: Beyond the Circle","text":"<p>While Venn diagrams are the poster child for set visualization, they're not the only option. Set visualizations encompass any approach to showing membership and categorization:</p> <ul> <li>Euler diagrams: Like Venns, but without requiring all overlap regions</li> <li>Nested sets: Concentric shapes showing subset relationships</li> <li>UpSet plots: Matrix-based approach for many overlapping sets</li> <li>Containment maps: Rectangular nesting (like treemaps)</li> </ul> <p>The choice depends on your data. Three sets with significant overlap? Classic Venn. Seven sets with sparse overlap? UpSet plot. Strict hierarchy of containment? Nested rectangles.</p>"},{"location":"chapters/03-microsim-pattern-library/#classification-displays-sorting-and-categorizing","title":"Classification Displays: Sorting and Categorizing","text":"<p>Classification displays focus on the act of categorization itself\u2014taking items and sorting them into groups. Unlike Venn diagrams (which show the result of classification), classification displays often involve interactive sorting.</p> <p>These are fantastic for assessment because they require active demonstration of understanding. Instead of passively viewing categories, learners must actively decide where items belong.</p> <p>Types of classification displays:</p> <ul> <li>Card sorting: Drag items to category columns</li> <li>Matrix classification: Place items in row/column intersections</li> <li>Hierarchical sorting: Multi-level categorization</li> <li>Binary trees: Yes/no decision paths</li> </ul>"},{"location":"chapters/03-microsim-pattern-library/#diagram-classification-matrix-display","title":"Diagram: Classification Matrix Display","text":"Classification Matrix Display <p>Type: microsim</p> <p>Purpose: Allow learners to classify items along two dimensions simultaneously</p> <p>Bloom Taxonomy Level: Analyze, Evaluate</p> <p>Learning Objective: Students will be able to classify items using two independent criteria and justify their placement in the resulting quadrants.</p> <p>Canvas Layout: - Title and instructions at top - Main 2x2 (or larger) matrix grid in center - Item bank on left or bottom - Score/feedback area</p> <p>Matrix Structure: - Row headers: Dimension 1 categories (e.g., \"High Cost / Low Cost\") - Column headers: Dimension 2 categories (e.g., \"High Impact / Low Impact\") - Grid cells: Drop zones for items - Each cell represents combination of both dimensions</p> <p>Items: - Displayed as cards in item bank - Each card shows item name and optional description - Draggable to matrix cells - Visual feedback during drag</p> <p>Interaction: - Drag items from bank to matrix cells - Items can be moved between cells - Optional: click item for more details - \"Check\" button for assessment mode</p> <p>Feedback Modes: 1. Practice mode: Show correct answer after each placement 2. Assessment mode: Show results at end 3. Exploration mode: No right/wrong, just organization</p> <p>Visual Styling: - Distinct colors for each quadrant - Items inherit quadrant color when placed - Highlight valid drop zones during drag - Animation for placement</p> <p>Example Application: - Learning activities classified by:   - Rows: Bloom's level (Lower-order / Higher-order)   - Columns: Engagement type (Passive / Active)</p> <p>Implementation: HTML5 drag-and-drop or p5.js</p>"},{"location":"chapters/03-microsim-pattern-library/#timelines-and-sequence-displays","title":"Timelines and Sequence Displays","text":"<p>Time marches on, and sometimes that march is exactly what we need to visualize. Timeline and sequence displays show temporal relationships\u2014what came before, what comes after, and how long things lasted.</p> <p>History teachers have been drawing timelines since chalk met blackboard. But interactive digital timelines can do so much more: zoom in on specific eras, filter by category, link to detailed information, and even animate the passage of time.</p>"},{"location":"chapters/03-microsim-pattern-library/#sequence-displays-first-this-then-that","title":"Sequence Displays: First This, Then That","text":"<p>Sequence displays show ordered events without necessarily emphasizing duration or specific dates. The focus is on order: Step 1, Step 2, Step 3.</p> <p>These are essential for teaching:</p> <ul> <li>Procedures: How to do something</li> <li>Algorithms: Computational sequences</li> <li>Processes: How things unfold</li> <li>Causal chains: This leads to that leads to this</li> </ul> <p>The visual convention is typically linear (horizontal or vertical), with clear numbering or arrows indicating direction.</p>"},{"location":"chapters/03-microsim-pattern-library/#process-timelines-adding-time-to-sequence","title":"Process Timelines: Adding Time to Sequence","text":"<p>Process timelines combine sequence with duration. Not only do we see the order of events, but we see how long each phase lasts and when they overlap.</p> <p>Real-World MicroSim Examples</p> <p>The Chapter Content Generation Workflow Timeline MicroSim shows the phases of creating textbook content, with each phase's duration and dependencies clearly visualized.</p> <p>Process timelines are perfect for:</p> <ul> <li>Project management: Phases and milestones</li> <li>Development cycles: Design \u2192 Build \u2192 Test \u2192 Deploy</li> <li>Biological processes: Cell division stages</li> <li>Historical analysis: Overlapping periods and eras</li> </ul>"},{"location":"chapters/03-microsim-pattern-library/#diagram-interactive-process-timeline","title":"Diagram: Interactive Process Timeline","text":"Interactive Process Timeline <p>Type: timeline</p> <p>Purpose: Demonstrate a template for process-focused timeline MicroSims</p> <p>Bloom Taxonomy Level: Understand, Analyze</p> <p>Learning Objective: Students will be able to interpret process timelines to understand phase duration, overlap, and dependencies.</p> <p>Time Period: Flexible (adapts to content)</p> <p>Orientation: Horizontal (primary) with vertical swimlanes (optional)</p> <p>Main Timeline Elements: - Central time axis with tick marks - Duration bars for each phase/activity - Milestone markers (diamonds) for key events - Dependency arrows between related items</p> <p>Interactive Features: - Zoom: Mouse wheel or pinch to zoom in/out on timeline - Pan: Drag to scroll through time - Filter: Toggle categories on/off - Hover: Show detailed information popup - Click: Expand to full phase description</p> <p>Visual Encoding: - Bar length = duration - Bar color = category or status - Bar position (vertical) = swimlane/track - Opacity = certainty/confidence - Pattern fill = different types within category</p> <p>Example Data Structure: <pre><code>{\n  \"phases\": [\n    {\"name\": \"Analysis\", \"start\": 0, \"end\": 2, \"color\": \"#3498db\"},\n    {\"name\": \"Design\", \"start\": 1, \"end\": 4, \"color\": \"#2ecc71\"},\n    {\"name\": \"Development\", \"start\": 3, \"end\": 8, \"color\": \"#e74c3c\"},\n    {\"name\": \"Testing\", \"start\": 6, \"end\": 9, \"color\": \"#f39c12\"}\n  ],\n  \"milestones\": [\n    {\"name\": \"Kickoff\", \"time\": 0, \"icon\": \"star\"},\n    {\"name\": \"Design Review\", \"time\": 4, \"icon\": \"check\"},\n    {\"name\": \"Launch\", \"time\": 9, \"icon\": \"rocket\"}\n  ]\n}\n</code></pre></p> <p>Layout: - Main canvas: Timeline visualization - Side panel: Phase legend and details - Bottom: Time scale with labels</p> <p>Implementation: vis-timeline library or custom HTML5/SVG</p>"},{"location":"chapters/03-microsim-pattern-library/#charts-for-quantitative-data","title":"Charts for Quantitative Data","text":"<p>Numbers tell stories, and charts help us read them. When your learning objective involves quantitative comparisons, trends, or distributions, you've entered chart territory.</p> <p>Charts are so ubiquitous that it's easy to take them for granted. But choosing the right chart type is a genuine skill\u2014and the wrong choice can mislead learners rather than enlighten them.</p>"},{"location":"chapters/03-microsim-pattern-library/#trend-charts-change-over-time","title":"Trend Charts: Change Over Time","text":"<p>Trend charts (typically line charts) show how values change over continuous time or some other continuous variable. The x-axis represents the independent variable, the y-axis the dependent variable, and the connecting line shows the pattern.</p> <p>Trend charts answer questions like:</p> <ul> <li>Is it going up or down?</li> <li>How fast is it changing?</li> <li>Are there cycles or patterns?</li> <li>When did it change direction?</li> <li>How do multiple trends compare?</li> </ul> <p>Key design decisions for trend charts:</p> Decision Options When to Use Scale Linear vs. logarithmic Log for exponential growth Points Shown vs. hidden Show for sparse data Line style Solid vs. dashed Distinguish actual vs. projected Area fill None vs. filled Emphasize cumulative values Multiple series Overlaid vs. small multiples Overlaid for comparison"},{"location":"chapters/03-microsim-pattern-library/#distribution-charts-showing-spread","title":"Distribution Charts: Showing Spread","text":"<p>Distribution charts reveal how values are spread across a range. Instead of asking \"what's the value?\" they ask \"how many values fall in each range?\"</p> <p>Common distribution visualizations:</p> <ul> <li>Histograms: Bars showing frequency in ranges</li> <li>Box plots: Summary statistics (median, quartiles, outliers)</li> <li>Violin plots: Distribution shape with density</li> <li>Density plots: Smooth continuous distribution curve</li> </ul> <p>These are essential for statistics education, quality control concepts, and any domain where understanding variability matters as much as understanding averages.</p>"},{"location":"chapters/03-microsim-pattern-library/#correlation-displays-relationships-between-variables","title":"Correlation Displays: Relationships Between Variables","text":"<p>When you want to show how two quantitative variables relate, you need a correlation display\u2014typically a scatter plot. Each point represents one observation, positioned according to its values on both variables.</p> <p>Scatter plots reveal:</p> <ul> <li>Positive correlation: Points trend upward</li> <li>Negative correlation: Points trend downward</li> <li>No correlation: Points scattered randomly</li> <li>Non-linear relationships: Curved patterns</li> <li>Outliers: Points far from the trend</li> </ul> <p>Adding a trend line (linear regression) quantifies the relationship and makes it visually explicit.</p>"},{"location":"chapters/03-microsim-pattern-library/#diagram-chart-type-selection-guide","title":"Diagram: Chart Type Selection Guide","text":"Chart Type Selection Guide <p>Type: infographic</p> <p>Purpose: Help learners choose the appropriate chart type for their data</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Students will be able to select the most appropriate chart type based on the nature of their data and the question they want to answer.</p> <p>Layout: Decision matrix with data type on one axis and question type on the other</p> <p>Data Types (rows): - Categorical (names, groups) - Time series (values over time) - Distribution (spread of values) - Relationship (two variables) - Part-to-whole (percentages)</p> <p>Question Types (columns): - Compare values - Show change - Show composition - Show distribution - Show relationship</p> <p>Matrix Cells (recommended chart types): | | Compare | Change | Composition | Distribution | Relationship | |---|---------|--------|-------------|--------------|--------------| | Categorical | Bar | Grouped Bar | Stacked Bar | - | - | | Time Series | Line | Area | Stacked Area | - | Dual Axis | | Distribution | - | - | - | Histogram | - | | Relationship | - | - | - | - | Scatter | | Part-to-Whole | - | - | Pie/Donut | - | - |</p> <p>Interactive Features: - Click any cell to see example chart - Hover for description of when to use - Toggle \"advanced charts\" for more options (violin, treemap, etc.) - \"Show me\" button generates sample chart</p> <p>Visual Styling: - Color-coded by chart family - Icons representing each chart type - Highlight \"best practice\" recommendations in green - Warning symbols for commonly misused combinations</p> <p>Implementation: Interactive HTML/CSS grid with JavaScript popups</p>"},{"location":"chapters/03-microsim-pattern-library/#spatial-visualizations","title":"Spatial Visualizations","text":"<p>Sometimes location is the key dimension. Spatial visualizations show where things are\u2014on a map, in a room, across an anatomy, or through space.</p>"},{"location":"chapters/03-microsim-pattern-library/#maps-geographic-context","title":"Maps: Geographic Context","text":"<p>When your content has a geographic component, maps provide the essential context. Students studying migration patterns, regional economics, climate zones, or historical campaigns need to see where things happened.</p> <p>Interactive map features:</p> <ul> <li>Zoom and pan: Explore at different scales</li> <li>Layers: Toggle different data overlays</li> <li>Markers: Points of interest</li> <li>Regions: Colored areas (choropleth maps)</li> <li>Movement arrows: Show flows and migration</li> </ul> <p>Real-World MicroSim Examples</p> <p>The Interactive World Cities Map MicroSim uses Leaflet to display major cities across continents, allowing learners to explore geographic distribution and urban patterns.</p> <p>Maps aren't just for geography. Any spatial layout can use map-like visualization:</p> <ul> <li>Campus maps: Building locations and routes</li> <li>Anatomical diagrams: Organ locations and systems</li> <li>Network topologies: Physical device placement</li> <li>Game boards: Spatial strategy visualization</li> </ul>"},{"location":"chapters/03-microsim-pattern-library/#diagram-spatial-visualization-types","title":"Diagram: Spatial Visualization Types","text":"Spatial Visualization Types <p>Type: diagram</p> <p>Purpose: Show the variety of spatial visualization approaches and when to use each</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Students will be able to distinguish between different spatial visualization types and select appropriate approaches for location-based content.</p> <p>Layout: 2x3 grid of visualization type examples</p> <p>Visualization Types:</p> <ol> <li>Geographic Map</li> <li>Use case: Real-world locations</li> <li>Example: World map with city markers</li> <li>Library: Leaflet</li> <li> <p>Features: Zoom, layers, real geography</p> </li> <li> <p>Schematic Map</p> </li> <li>Use case: Simplified spatial relationships</li> <li>Example: Subway map, circuit diagram</li> <li>Library: SVG or Canvas</li> <li> <p>Features: Distorted for clarity, not to scale</p> </li> <li> <p>Floor Plan</p> </li> <li>Use case: Indoor spaces, architecture</li> <li>Example: Building layout, room arrangement</li> <li>Library: SVG or Canvas</li> <li> <p>Features: To scale, overhead view</p> </li> <li> <p>Anatomical Diagram</p> </li> <li>Use case: Biological structures</li> <li>Example: Human body systems</li> <li>Library: SVG with regions</li> <li> <p>Features: Labels, system highlighting</p> </li> <li> <p>Network Topology</p> </li> <li>Use case: Physical system layout</li> <li>Example: Computer network, power grid</li> <li>Library: vis-network with fixed positions</li> <li> <p>Features: Geographic placement + connections</p> </li> <li> <p>3D Spatial</p> </li> <li>Use case: Volume, depth relationships</li> <li>Example: Molecular structure, architectural walk-through</li> <li>Library: Three.js</li> <li>Features: Rotation, perspective</li> </ol> <p>Visual Styling: - Each type shown as a thumbnail with label - Distinctive visual style for each - \"Best for\" caption under each</p> <p>Interactive Features: - Click type to see larger example - Hover for use case details</p> <p>Implementation: Static SVG grid with modal popups for examples</p>"},{"location":"chapters/03-microsim-pattern-library/#flow-and-state-diagrams","title":"Flow and State Diagrams","text":"<p>Some concepts are best understood as processes with decisions, branches, and states. Flowcharts and state machine diagrams excel at showing these logical structures.</p>"},{"location":"chapters/03-microsim-pattern-library/#flowcharts-decisions-and-processes","title":"Flowcharts: Decisions and Processes","text":"<p>The humble flowchart might be the most widely recognized diagram type in business and technology. Rectangles for processes, diamonds for decisions, arrows for flow\u2014it's a visual language that most professionals already speak.</p> <p>Flowcharts are perfect for:</p> <ul> <li>Algorithms: Step-by-step computation</li> <li>Business processes: Workflows and procedures</li> <li>Troubleshooting guides: Decision trees</li> <li>System interactions: Data flow between components</li> </ul> <p>The key insight flowcharts provide is branching logic. When you reach a decision point, the path you take depends on a condition. This fundamental concept underlies all programming and much of critical thinking.</p> <p>Real-World MicroSim Examples</p> <p>The MkDocs Build Process Workflow, ADDIE Model Workflow, and Data-Driven Ethics Process Flow MicroSims all use Mermaid-based flowcharts to illustrate complex sequential processes with decision points.</p>"},{"location":"chapters/03-microsim-pattern-library/#state-machine-diagrams-being-vs-doing","title":"State Machine Diagrams: Being vs. Doing","text":"<p>While flowcharts focus on what happens, state machine diagrams focus on what state the system is in. This subtle distinction is crucial for understanding systems with persistence and memory.</p> <p>A state machine consists of:</p> <ul> <li>States: Conditions the system can be in</li> <li>Transitions: Events that cause state changes</li> <li>Guards: Conditions that enable transitions</li> <li>Actions: Activities that occur during transitions</li> </ul> <p>State machines are essential for teaching:</p> <ul> <li>User interface behavior: Button states, form validation</li> <li>Protocol design: Communication states</li> <li>Game logic: Character states, game phases</li> <li>Workflow status: Document approval stages</li> </ul>"},{"location":"chapters/03-microsim-pattern-library/#diagram-state-machine-template","title":"Diagram: State Machine Template","text":"State Machine Visualization Template <p>Type: diagram</p> <p>Purpose: Provide a template for state machine MicroSims</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Students will be able to trace state transitions in a state machine diagram and predict system behavior given a sequence of events.</p> <p>Components:</p> <p>States (rounded rectangles): - Name prominently displayed - Optional: entry actions, exit actions - Visual distinction for special states:   - Initial state: filled circle   - Final state: filled circle with ring   - Current state: highlighted border</p> <p>Transitions (arrows): - Event label above arrow - Guard condition in brackets [guard] - Action after slash /action - Self-loops for events that don't change state</p> <p>Layout Options: - Hierarchical: States arranged top-to-bottom - Circular: States around a center - Left-to-right: Timeline-like progression</p> <p>Example State Machine (Document Workflow): - States: Draft, Under Review, Approved, Rejected, Published - Transitions:   - Draft --submit\u2192 Under Review   - Under Review --approve\u2192 Approved   - Under Review --reject\u2192 Rejected   - Rejected --revise\u2192 Draft   - Approved --publish\u2192 Published</p> <p>Interactive Features: - Click state to see description - Click transition to see event details - \"Simulate\" mode: click events to watch state changes - Current state indicator moves with transitions - Event history displayed</p> <p>Visual Styling: - State colors indicate category (e.g., active vs. terminal) - Transition arrow colors indicate event type - Animation for state transitions - Pulsing effect on current state</p> <p>Implementation: Mermaid.js for static, custom SVG/JS for interactive simulation</p>"},{"location":"chapters/03-microsim-pattern-library/#paradigm-selection-the-art-of-matching","title":"Paradigm Selection: The Art of Matching","text":"<p>Now that you've seen the full buffet of visualization paradigms, the crucial skill is selection. Paradigm selection is the art of looking at a learning objective and recognizing which visualization approach will best illuminate the concept.</p> <p>This isn't always obvious. Consider teaching \"how a bill becomes a law.\" You could use:</p> <ul> <li>A flowchart (focus on decision points and branches)</li> <li>A timeline (focus on sequence and duration)</li> <li>A state machine (focus on bill status transitions)</li> <li>A network graph (focus on stakeholder relationships)</li> </ul> <p>Each reveals different aspects of the same process. The \"right\" choice depends on what aspect you want learners to understand.</p>"},{"location":"chapters/03-microsim-pattern-library/#concept-characteristics-the-diagnostic-framework","title":"Concept Characteristics: The Diagnostic Framework","text":"<p>Concept characteristics are the inherent properties of educational content that suggest appropriate visualization approaches. By analyzing these characteristics, you can systematically match concepts to paradigms.</p> <p>Key characteristics to analyze:</p> Characteristic Questions to Ask Implications Temporality Does time matter? Is there sequence? Timeline, sequence, motion Relationship structure How are parts related? Hierarchical? Networked? Graph, hierarchy, influence diagram Quantitative nature Are there numbers? Comparisons? Distributions? Charts, statistical displays Spatial organization Does location matter? Is there physical arrangement? Maps, spatial diagrams Process vs. state Focus on actions or conditions? Flowchart vs. state machine Category membership Are things being classified? Do categories overlap? Venn diagram, classification display Causality Do things cause other things? Are there feedback loops? Cause-effect, influence diagram Dynamism Does it change continuously? Are there forces? Physics simulation, animation"},{"location":"chapters/03-microsim-pattern-library/#visual-affordances-what-visualizations-suggest","title":"Visual Affordances: What Visualizations \"Suggest\"","text":"<p>Visual affordances are the actions or interpretations that a visualization naturally suggests. A button looks \"clickable.\" A slider looks \"draggable.\" A network looks \"explorable.\"</p> <p>Understanding affordances helps you match visualization to learning activity:</p> <ul> <li>Passive viewing: Diagrams, infographics</li> <li>Exploration: Networks, maps, timelines</li> <li>Manipulation: Simulations, interactive charts</li> <li>Construction: Builders, editors, sorters</li> <li>Prediction: Cause-effect displays, simulations</li> </ul> <p>The affordance should match the learning objective's verb. If learners need to \"analyze relationships,\" give them an explorable network. If they need to \"predict outcomes,\" give them a manipulable simulation. If they need to \"classify items,\" give them a sorting activity.</p>"},{"location":"chapters/03-microsim-pattern-library/#diagram-paradigm-affordance-mapping","title":"Diagram: Paradigm-Affordance Mapping","text":"Paradigm-Affordance Mapping <p>Type: infographic</p> <p>Purpose: Show how different visualization paradigms afford different types of learning interactions</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Students will be able to evaluate the match between visualization paradigms and learning objectives based on affordance alignment.</p> <p>Layout: Matrix with paradigms as rows and learning activities as columns</p> <p>Paradigms (rows): 1. Motion/Physics Simulation 2. Network Graph 3. Hierarchy Display 4. Timeline/Sequence 5. Statistical Chart 6. Spatial Map 7. Flowchart 8. State Machine 9. Venn/Set Diagram 10. Classification Display</p> <p>Learning Activities (columns): 1. Observe/Watch 2. Explore/Navigate 3. Compare/Contrast 4. Manipulate/Experiment 5. Predict/Hypothesize 6. Construct/Build 7. Classify/Sort 8. Trace/Follow 9. Analyze Relationships</p> <p>Matrix Values (color-coded strength): - Strong fit (green): Paradigm naturally affords this activity - Moderate fit (yellow): Possible with additional design work - Poor fit (red): Fighting against the paradigm's nature - Not applicable (gray): Doesn't make sense for this combination</p> <p>Example entries: - Motion Simulation \u00d7 Predict = Strong (natural experiment) - Network Graph \u00d7 Analyze Relationships = Strong (core purpose) - Timeline \u00d7 Trace/Follow = Strong (natural affordance) - Venn Diagram \u00d7 Classify/Sort = Strong (core purpose) - Statistical Chart \u00d7 Manipulate = Moderate (with interactive controls)</p> <p>Interactive Features: - Click cell to see examples of that combination - Filter by Bloom's taxonomy level - Highlight best matches for a given learning objective - Export recommendations</p> <p>Visual Styling: - Color intensity indicates strength - Icons for each activity type - Hover details with rationale</p> <p>Implementation: Interactive HTML table with filtering</p>"},{"location":"chapters/03-microsim-pattern-library/#putting-it-all-together-your-pattern-recognition-toolkit","title":"Putting It All Together: Your Pattern Recognition Toolkit","text":"<p>Congratulations! You've just toured the complete MicroSim visualization landscape. Let's consolidate what you've learned into a practical toolkit you can use immediately.</p>"},{"location":"chapters/03-microsim-pattern-library/#the-five-minute-paradigm-selection-process","title":"The Five-Minute Paradigm Selection Process","text":"<p>When faced with a new learning objective, run through this quick analysis:</p> <ol> <li>What's changing?</li> <li>Time \u2192 Timeline/Sequence</li> <li>Position \u2192 Motion Simulation</li> <li>State \u2192 State Machine</li> <li> <p>Quantity \u2192 Charts</p> </li> <li> <p>What's related?</p> </li> <li>Hierarchy \u2192 Hierarchy Display</li> <li>Network \u2192 Network Graph</li> <li>Sets \u2192 Venn Diagram</li> <li> <p>Causes \u2192 Influence Diagram</p> </li> <li> <p>What should learners do?</p> </li> <li>Observe \u2192 Any visualization</li> <li>Explore \u2192 Network, Map, Timeline</li> <li>Manipulate \u2192 Simulation, Interactive Chart</li> <li>Classify \u2192 Venn, Classification Display</li> <li> <p>Trace \u2192 Flowchart, State Machine</p> </li> <li> <p>What level of Bloom's Taxonomy?</p> </li> <li>Remember \u2192 Static diagrams, simple displays</li> <li>Understand \u2192 Explorable visualizations</li> <li>Apply \u2192 Interactive simulations</li> <li>Analyze \u2192 Network analysis, cause-effect</li> <li>Evaluate \u2192 Comparison tools, rubric applications</li> <li>Create \u2192 Builders, editors, constructors</li> </ol>"},{"location":"chapters/03-microsim-pattern-library/#common-paradigm-pairings","title":"Common Paradigm Pairings","text":"<p>Many MicroSims combine paradigms for richer learning experiences:</p> <ul> <li>Simulation + Chart: Show physics while graphing variables over time</li> <li>Network + Timeline: Display relationship evolution</li> <li>Flowchart + State Machine: Process view with state awareness</li> <li>Map + Timeline: Historical events with geographic context</li> <li>Classification + Network: Category membership plus relationships</li> </ul>"},{"location":"chapters/03-microsim-pattern-library/#your-pattern-library-is-your-superpower","title":"Your Pattern Library Is Your Superpower","text":"<p>Here's the optimistic truth that motivated this chapter: most educational concepts fit known patterns. The more patterns you recognize, the faster you can design effective MicroSims.</p> <p>You're not starting from scratch every time. You're recognizing \"Oh, this is a classification problem\u2014I'll use a Venn or sorting approach\" or \"This involves feedback loops\u2014influence diagram time.\" Pattern recognition transforms design from creative struggle to informed selection.</p> <p>And remember: the AI tools you'll use in later chapters already know these patterns too. When you tell an AI \"create a motion simulation showing harmonic oscillation\" or \"build a network graph of character relationships,\" you're speaking a shared language. The patterns are the vocabulary.</p>"},{"location":"chapters/03-microsim-pattern-library/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>Let's recap the visualization paradigms you've mastered:</p> <p>Motion and Physics Family: - Motion Simulation: Objects moving through space - Physics Simulation: Realistic forces and behaviors - Dynamic Systems: Multiple interacting variables - Cause-Effect Display: Input \u2192 Output relationships</p> <p>Network and Graph Family: - Network Graph: Nodes and edges showing connections - Relationship Graph: Semantic meaning on connections - Hierarchy Display: Tree structures, parent-child - Dependency Mapping: Prerequisites and requirements - Influence Diagram: Causal networks</p> <p>Set and Classification Family: - Venn Diagram: Overlapping categories - Set Visualization: Membership and containment - Classification Display: Sorting and categorizing</p> <p>Timeline and Sequence Family: - Sequence Display: Ordered steps - Process Timeline: Duration and overlap</p> <p>Chart Family: - Trend Chart: Change over continuous variable - Distribution Chart: Spread of values - Correlation Display: Relationship between variables</p> <p>Spatial Family: - Spatial Visualization: Location-based displays - Maps: Geographic context</p> <p>Flow and State Family: - Flowchart: Processes with decisions - State Machine Diagram: States and transitions</p> <p>Meta-Skills: - Paradigm Selection: Matching concept to visualization - Concept Characteristics: Analyzing content properties - Visual Affordances: Understanding what visualizations suggest</p> <p>With these patterns in your toolkit, you're ready to tackle any learning objective and quickly identify the visualization approach that will make it shine. In the next chapter, we'll dive into actually writing MicroSim specifications\u2014turning your paradigm selection into detailed blueprints that AI tools can build.</p> <p>Welcome to the pattern library. Now go make some learning magic happen!</p> Review Questions <ol> <li>What visualization paradigm would you choose for teaching the water cycle, and why?</li> </ol> <p>The water cycle involves continuous transformation (liquid \u2192 gas \u2192 liquid) happening in different locations. A motion/physics simulation combined with spatial visualization would work well\u2014showing particles moving through the cycle stages positioned on a simplified geography (ocean, atmosphere, mountains).</p> <ol> <li>When would you choose a state machine diagram over a flowchart?</li> </ol> <p>Choose a state machine when the current state matters for understanding behavior\u2014when the system \"remembers\" where it is and that affects what can happen next. Flowcharts are better when you're showing a linear process that starts, executes, and ends without persistent state.</p> <ol> <li>What concept characteristics suggest using a network graph?</li> </ol> <p>Network graphs are indicated when: (a) multiple entities exist that need to be shown, (b) relationships between entities are non-hierarchical, (c) the pattern of connections matters for understanding, and (d) learners need to explore or analyze the relationship structure.</p>"},{"location":"chapters/03-microsim-pattern-library/quiz/","title":"Quiz: The MicroSim Pattern Library","text":"<p>Test your understanding of visualization paradigms, from motion simulations to state machines, and learn to select the right pattern for any learning objective.</p>"},{"location":"chapters/03-microsim-pattern-library/quiz/#1-what-is-a-visualization-paradigm","title":"1. What is a visualization paradigm?","text":"<ol> <li>A specific JavaScript library for creating charts</li> <li>A fundamental approach to representing information visually\u2014the underlying structure that organizes information</li> <li>A color scheme used in educational materials</li> <li>A type of interactive button control</li> </ol> Show Answer <p>The correct answer is B. A visualization paradigm is a fundamental approach to representing information visually\u2014the \"shape\" of your visualization. It's not the specific colors or labels, but the underlying structure that organizes information. Choosing the right paradigm is like choosing the right vehicle for a journey; each type serves different purposes.</p> <p>Concept Tested: Visualization Paradigm</p> <p>See: Chapter Content</p>"},{"location":"chapters/03-microsim-pattern-library/quiz/#2-when-would-you-choose-a-motion-simulation-over-a-network-graph","title":"2. When would you choose a motion simulation over a network graph?","text":"<ol> <li>When showing relationships between entities</li> <li>When visualizing hierarchical organizational structures</li> <li>When teaching concepts involving continuous change over time, like projectile motion or pendulum oscillations</li> <li>When classifying items into categories</li> </ol> Show Answer <p>The correct answer is C. Motion simulations are ideal when teaching concepts involving continuous change over time, where objects move through space and physics plays a role. Examples include projectile motion, pendulums, and oscillations. Network graphs are better for showing relationships between entities, while classification displays are better for categorizing items.</p> <p>Concept Tested: Motion Simulation</p> <p>See: Chapter Content</p>"},{"location":"chapters/03-microsim-pattern-library/quiz/#3-what-distinguishes-a-dynamic-system-simulation-from-a-simple-motion-simulation","title":"3. What distinguishes a dynamic system simulation from a simple motion simulation?","text":"<ol> <li>Dynamic systems use more colors</li> <li>Dynamic systems involve multiple variables that interact and influence each other over time with feedback loops</li> <li>Dynamic systems run faster</li> <li>Dynamic systems only work on mobile devices</li> </ol> Show Answer <p>The correct answer is B. Dynamic systems represent simulations where multiple variables interact and influence each other over time, involving feedback loops, equilibrium states, and emergent behavior. Unlike simple motion where one object follows a trajectory, dynamic systems show how changes in one variable affect others\u2014like predator-prey ecosystems where rabbit and fox populations influence each other.</p> <p>Concept Tested: Dynamic Systems</p> <p>See: Chapter Content</p>"},{"location":"chapters/03-microsim-pattern-library/quiz/#4-a-network-graph-consists-of-which-two-fundamental-elements","title":"4. A network graph consists of which two fundamental elements?","text":"<ol> <li>Colors and labels</li> <li>Nodes (entities) and edges (relationships)</li> <li>X-axis and Y-axis</li> <li>Headers and footers</li> </ol> Show Answer <p>The correct answer is B. A network graph consists of nodes (vertices), which are the entities being related, and edges (links), which are the relationships between those entities. From this simple foundation of two components, we can represent complex relationships like social networks, concept dependencies, and organizational structures.</p> <p>Concept Tested: Network Graph</p> <p>See: Chapter Content</p>"},{"location":"chapters/03-microsim-pattern-library/quiz/#5-what-visualization-would-be-most-appropriate-for-showing-that-mammals-aquatic-animals-and-warm-blooded-creatures-share-some-overlapping-members","title":"5. What visualization would be most appropriate for showing that mammals, aquatic animals, and warm-blooded creatures share some overlapping members?","text":"<ol> <li>Timeline</li> <li>Venn Diagram</li> <li>Flowchart</li> <li>Trend Chart</li> </ol> Show Answer <p>The correct answer is B. Venn diagrams are perfect for showing set membership and overlapping categories. They visually represent which items belong to multiple groups by placing them in the overlapping regions of circles. The example of mammals, aquatic animals, and warm-blooded creatures is ideal for a Venn diagram because these categories have overlapping members (like whales, which are both mammals and aquatic).</p> <p>Concept Tested: Venn Diagram</p> <p>See: Chapter Content</p>"},{"location":"chapters/03-microsim-pattern-library/quiz/#6-when-would-you-choose-a-state-machine-diagram-over-a-flowchart","title":"6. When would you choose a state machine diagram over a flowchart?","text":"<ol> <li>When the process has a single start and end point</li> <li>When you want to show a linear sequence of steps</li> <li>When the current state of the system matters and affects what can happen next</li> <li>When you need to display geographic locations</li> </ol> Show Answer <p>The correct answer is C. State machine diagrams are preferred when the current state matters for understanding behavior\u2014when the system \"remembers\" where it is and that affects what can happen next. Flowcharts are better for showing linear processes that start, execute, and end. State machines focus on conditions the system can be in and transitions between them.</p> <p>Concept Tested: State Machine Diagram</p> <p>See: Chapter Content</p>"},{"location":"chapters/03-microsim-pattern-library/quiz/#7-what-do-concept-characteristics-help-you-determine","title":"7. What do concept characteristics help you determine?","text":"<ol> <li>The budget required for developing the MicroSim</li> <li>The appropriate visualization paradigm by analyzing properties like temporality, relationship structure, and causality</li> <li>The programming language to use</li> <li>The target audience's age</li> </ol> Show Answer <p>The correct answer is B. Concept characteristics are inherent properties of educational content that suggest appropriate visualization approaches. By analyzing characteristics like temporality (does time matter?), relationship structure (hierarchical or networked?), quantitative nature, spatial organization, and causality, you can systematically match concepts to the best paradigm.</p> <p>Concept Tested: Concept Characteristics</p> <p>See: Chapter Content</p>"},{"location":"chapters/03-microsim-pattern-library/quiz/#8-a-teacher-wants-to-visualize-how-stock-prices-changed-over-the-past-year-which-visualization-paradigm-is-most-appropriate","title":"8. A teacher wants to visualize how stock prices changed over the past year. Which visualization paradigm is most appropriate?","text":"<ol> <li>Venn Diagram</li> <li>Network Graph</li> <li>Trend Chart</li> <li>Classification Display</li> </ol> Show Answer <p>The correct answer is C. Trend charts (typically line charts) are designed to show how values change over continuous time or another continuous variable. They answer questions like \"Is it going up or down?\", \"How fast is it changing?\", and \"Are there cycles or patterns?\"\u2014exactly what's needed for stock price visualization.</p> <p>Concept Tested: Trend Chart</p> <p>See: Chapter Content</p>"},{"location":"chapters/03-microsim-pattern-library/quiz/#9-what-are-visual-affordances-in-the-context-of-microsim-design","title":"9. What are visual affordances in the context of MicroSim design?","text":"<ol> <li>The cost of visual elements</li> <li>Design properties that suggest how an element can be used or interacted with</li> <li>The number of colors used in a visualization</li> <li>The file size of images</li> </ol> Show Answer <p>The correct answer is B. Visual affordances are design properties that suggest how an element can be used or interacted with. A button looks \"clickable,\" a slider looks \"draggable,\" and a network looks \"explorable.\" Understanding affordances helps match visualization to learning activity\u2014if learners need to \"analyze relationships,\" give them an explorable network; if they need to \"classify items,\" give them a sorting activity.</p> <p>Concept Tested: Visual Affordances</p> <p>See: Chapter Content</p>"},{"location":"chapters/03-microsim-pattern-library/quiz/#10-which-visualization-paradigm-would-best-show-the-steps-involved-in-how-a-bill-becomes-a-law-with-decision-points-where-the-bill-can-be-rejected-or-amended","title":"10. Which visualization paradigm would best show the steps involved in how a bill becomes a law, with decision points where the bill can be rejected or amended?","text":"<ol> <li>Scatter Plot</li> <li>Flowchart</li> <li>Distribution Chart</li> <li>Spatial Map</li> </ol> Show Answer <p>The correct answer is B. Flowcharts are perfect for showing processes with decisions, branches, and different possible paths. The process of how a bill becomes a law involves sequential steps and decision points (like votes that can pass or fail), making a flowchart the ideal choice. Flowcharts use rectangles for processes, diamonds for decisions, and arrows for flow.</p> <p>Concept Tested: Flowchart</p> <p>See: Chapter Content</p>"},{"location":"chapters/04-visualization-libraries-tools/","title":"Visualization Libraries and Tools","text":""},{"location":"chapters/04-visualization-libraries-tools/#summary","title":"Summary","text":"<p>This chapter introduces the JavaScript libraries and tools used to implement MicroSims. You will learn about p5.js for animations, vis-network for relationship graphs, vis-timeline for temporal displays, Chart.js and Plotly for data visualizations, Leaflet for maps, and Mermaid for diagrams. The chapter also covers Claude Code skills, the MicroSim generator, template libraries, and code generation workflows. By understanding these tools, you will be able to select the right technology for implementing any visualization paradigm.</p>"},{"location":"chapters/04-visualization-libraries-tools/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 15 concepts from the learning graph:</p> <ol> <li>p5.js Animation</li> <li>vis-network Library</li> <li>Timeline Visualization</li> <li>vis-timeline Library</li> <li>Chart Visualization</li> <li>Chart.js Library</li> <li>Plotly Library</li> <li>Map Visualization</li> <li>Leaflet Library</li> <li>Diagram Visualization</li> <li>Mermaid Library</li> <li>Claude Code Skills</li> <li>MicroSim Generator</li> <li>Template Library</li> <li>Code Generation</li> </ol>"},{"location":"chapters/04-visualization-libraries-tools/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: The MicroSim Pattern Library</li> </ul>"},{"location":"chapters/04-visualization-libraries-tools/#welcome-to-the-visualization-toolbox","title":"Welcome to the Visualization Toolbox","text":"<p>Congratulations! You've made it to what we affectionately call the \"candy store\" chapter of this course. Here, we'll explore the delightful array of JavaScript libraries that transform abstract learning objectives into interactive, engaging visual experiences. Think of these libraries as your artistic palette, each color uniquely suited for painting different educational masterpieces.</p> <p>The beauty of modern web development is that you don't need to reinvent the wheel (or the slider, or the graph, or the map). Brilliant developers have created open-source libraries that handle the heavy lifting, allowing you to focus on what matters most: designing experiences that help people learn. As the saying goes, \"Why write a thousand lines of code when you can import one library and write ten?\"</p> <p>This chapter will equip you with the knowledge to select the right tool for any educational visualization challenge. By the end, you'll be like a well-stocked craftsperson, reaching confidently for p5.js when animation is needed, vis-network when relationships matter, or Chart.js when data needs to speak. Let's dive in!</p>"},{"location":"chapters/04-visualization-libraries-tools/#diagram-visualization-library-decision-tree","title":"Diagram: Visualization Library Decision Tree","text":"Visualization Library Decision Tree <p>Type: diagram</p> <p>Bloom Taxonomy: Understand</p> <p>Learning Objective: Help learners understand which visualization library to select based on their learning objective characteristics</p> <p>Purpose: Guide learners through the decision-making process of selecting the appropriate JavaScript library for their MicroSim based on the type of content being visualized</p> <p>Components to show: - Start node: \"What type of content?\" - Decision branches for: Animation/Physics, Data/Statistics, Relationships/Networks, Time-based, Geographic, Process/Workflow - Library endpoints: p5.js, Chart.js, Plotly, vis-network, vis-timeline, Leaflet, Mermaid</p> <p>Flow structure: 1. Start \u2192 \"Is it animated or physics-based?\"    - Yes \u2192 p5.js    - No \u2192 Continue 2. \"Is it data/statistics?\"    - Yes \u2192 \"Need mathematical functions?\" \u2192 Yes \u2192 Plotly, No \u2192 Chart.js    - No \u2192 Continue 3. \"Shows relationships/networks?\"    - Yes \u2192 vis-network    - No \u2192 Continue 4. \"Time-based/chronological?\"    - Yes \u2192 vis-timeline    - No \u2192 Continue 5. \"Geographic/location-based?\"    - Yes \u2192 Leaflet    - No \u2192 Mermaid (process/workflow default)</p> <p>Visual style: Top-down flowchart with colored decision diamonds and rectangular library nodes Color scheme: Decision nodes in light purple, library endpoints in distinct colors matching their brand identities (p5.js pink, Chart.js orange, Plotly blue, vis-network teal, vis-timeline green, Leaflet forest green, Mermaid purple)</p> <p>Implementation: Mermaid flowchart or vis-network with hierarchical layout</p>"},{"location":"chapters/04-visualization-libraries-tools/#p5js-the-swiss-army-knife-of-animation","title":"p5.js: The Swiss Army Knife of Animation","text":"<p>If JavaScript libraries were superheroes, p5.js would be the one that can fly, has super strength, AND makes excellent coffee. Originally created by Lauren McCarthy as a way to make coding accessible to artists and designers, p5.js has become the go-to library for creative, animated, and interactive visualizations on the web.</p>"},{"location":"chapters/04-visualization-libraries-tools/#what-makes-p5js-special","title":"What Makes p5.js Special?","text":"<p>p5.js excels at creating custom animations, physics simulations, and interactive visual experiences. Unlike libraries designed for specific chart types, p5.js gives you a blank canvas and says, \"Go wild!\" This flexibility makes it perfect for educational simulations where you need precise control over every pixel.</p> <p>Key characteristics of p5.js include:</p> <ul> <li>The setup() and draw() paradigm: Initialize once, then loop continuously</li> <li>Immediate mode rendering: Every frame, you draw everything from scratch</li> <li>Built-in functions: Shapes, colors, transformations, all at your fingertips</li> <li>Mouse and keyboard handling: Interactivity made simple</li> <li>Width-responsive design: Adapts to container size automatically</li> </ul>"},{"location":"chapters/04-visualization-libraries-tools/#the-anatomy-of-a-p5js-microsim","title":"The Anatomy of a p5.js MicroSim","text":"<p>Every p5.js MicroSim follows a consistent structure with distinct regions for drawing and controls. This separation keeps the educational content visually clean while providing interactive elements below.</p> <pre><code>// Canvas dimensions - the foundation of every MicroSim\nlet canvasWidth = 400;              // Initial width (responsive)\nlet drawHeight = 400;               // Drawing/simulation area height\nlet controlHeight = 50;             // Controls area height\nlet canvasHeight = drawHeight + controlHeight;\n\nfunction setup() {\n  updateCanvasSize();               // Get container width first!\n  const canvas = createCanvas(canvasWidth, canvasHeight);\n  canvas.parent(document.querySelector('main'));\n\n  // Create your sliders and controls here\n  describe('Educational description for screen readers', LABEL);\n}\n\nfunction draw() {\n  updateCanvasSize();\n\n  // Drawing area (light blue background)\n  fill('aliceblue');\n  stroke('silver');\n  rect(0, 0, canvasWidth, drawHeight);\n\n  // Control area (white background)\n  fill('white');\n  noStroke();\n  rect(0, drawHeight, canvasWidth, controlHeight);\n\n  // Your visualization code goes here\n}\n</code></pre> <p>Always Start with updateCanvasSize()</p> <p>The most common mistake new MicroSim creators make is forgetting to call <code>updateCanvasSize()</code> at the beginning of <code>setup()</code>. This function reads the container width and ensures your simulation adapts to any screen size.</p>"},{"location":"chapters/04-visualization-libraries-tools/#when-to-choose-p5js","title":"When to Choose p5.js","text":"<p>p5.js is your best friend when you need:</p> Use Case Example Physics simulations Bouncing balls, projectile motion, pendulums Custom animations Visualizing algorithms, state changes Interactive exploration Parameter spaces, function behavior Game-like experiences Educational games, gamified assessments Creative visualizations Generative art, data sonification <p>However, p5.js might be overkill for standard charts or diagrams. If you're just plotting bar charts, Chart.js will get you there faster. Think of it this way: you could use a chainsaw to cut butter, but a butter knife is probably more appropriate.</p>"},{"location":"chapters/04-visualization-libraries-tools/#diagram-p5js-microsim-architecture","title":"Diagram: p5.js MicroSim Architecture","text":"p5.js MicroSim Architecture <p>Type: diagram</p> <p>Bloom Taxonomy: Understand</p> <p>Learning Objective: Visualize the standard structure of a p5.js MicroSim with its distinct regions and responsive design</p> <p>Components to show: - Container (main element in HTML) - Canvas (created by p5.js) - Draw Region (top area with aliceblue background) - Control Region (bottom area with white background) - Slider elements in control region - Labels in control region</p> <p>Layout: - Vertical stack showing:   1. Browser window frame   2. Main container (full width)   3. Canvas divided into:      - Top: Draw region (labeled \"drawHeight\")      - Bottom: Control region (labeled \"controlHeight\")   4. Arrows showing responsive width behavior</p> <p>Annotations: - \"canvasWidth = container.offsetWidth\" pointing to width - \"Fixed heights, variable width\" annotation - \"updateCanvasSize() called each frame\" near draw loop indicator</p> <p>Color scheme: Draw region in aliceblue, control region in white, canvas border in silver</p> <p>Implementation: Mermaid block diagram or p5.js static rendering</p>"},{"location":"chapters/04-visualization-libraries-tools/#chart-visualization-telling-stories-with-data","title":"Chart Visualization: Telling Stories with Data","text":"<p>Data without visualization is like a joke without a punchline: technically complete, but missing the impact. Charts transform raw numbers into visual patterns that our pattern-recognizing brains can instantly comprehend. In this section, we'll explore two powerful charting libraries: Chart.js for simplicity and Plotly for sophistication.</p>"},{"location":"chapters/04-visualization-libraries-tools/#chartjs-simple-beautiful-and-fast","title":"Chart.js: Simple, Beautiful, and Fast","text":"<p>Chart.js is the library you reach for when you need a professional-looking chart in minutes, not hours. It supports all the standard chart types that business professionals and educators love:</p> <ul> <li>Line charts: Trends over time, continuous data</li> <li>Bar charts: Comparing categories, discrete values</li> <li>Pie and Doughnut charts: Parts of a whole (use sparingly!)</li> <li>Radar charts: Multi-dimensional comparisons</li> <li>Scatter plots: Correlations and distributions</li> <li>Bubble charts: Three-dimensional data visualization</li> </ul>"},{"location":"chapters/04-visualization-libraries-tools/#chartjs-in-action","title":"Chart.js in Action","text":"<p>Here's the basic pattern for creating a Chart.js visualization:</p> <pre><code>const ctx = document.getElementById('myChart').getContext('2d');\nconst myChart = new Chart(ctx, {\n    type: 'bar',  // or 'line', 'pie', 'doughnut', 'radar', 'scatter'\n    data: {\n        labels: ['Category A', 'Category B', 'Category C'],\n        datasets: [{\n            label: 'Dataset 1',\n            data: [65, 59, 80],\n            backgroundColor: 'rgba(54, 162, 235, 0.8)',\n            borderColor: 'rgb(54, 162, 235)',\n            borderWidth: 1\n        }]\n    },\n    options: {\n        responsive: true,\n        scales: {\n            y: { beginAtZero: true }\n        }\n    }\n});\n</code></pre> <p>Chart.js handles tooltips, legends, and animations automatically. It's like having a skilled graphic designer on call who works for free and never complains about deadlines.</p> Chart Type Best For Avoid When Line Trends over time Categorical comparisons Bar Comparing categories Continuous data Pie/Doughnut Parts of whole (3-6 slices) More than 6 categories Radar Multi-variable comparison Single variable analysis Scatter Correlations Categorical data"},{"location":"chapters/04-visualization-libraries-tools/#plotly-when-you-need-mathematical-muscle","title":"Plotly: When You Need Mathematical Muscle","text":"<p>While Chart.js excels at standard business charts, Plotly shines when you need to plot mathematical functions, create scientific visualizations, or provide advanced interactivity. Plotly is particularly powerful for:</p> <ul> <li>Mathematical function plots: $f(x) = \\sin(x)$, polynomial curves, exponential growth</li> <li>Interactive exploration: Hover tooltips showing precise coordinates</li> <li>Slider integration: Move points along curves to explore function behavior</li> <li>Scientific notation: Proper axis formatting for technical content</li> </ul> <pre><code>function f(x) {\n    return Math.sin(x);  // Your mathematical function\n}\n\n// Generate data points\nconst x = [];\nconst y = [];\nfor (let i = -6.28; i &lt;= 6.28; i += 0.01) {\n    x.push(i);\n    y.push(f(i));\n}\n\n// Create the plot\nPlotly.newPlot('plot', [{\n    x: x,\n    y: y,\n    type: 'scatter',\n    mode: 'lines',\n    name: 'y = sin(x)'\n}], {\n    responsive: true,\n    title: 'Sine Function Visualization'\n});\n</code></pre> <p>Choosing Between Chart.js and Plotly</p> <p>Use Chart.js when you're visualizing discrete data categories or need standard business charts. Use Plotly when you're plotting continuous mathematical functions or need advanced scientific visualization features.</p>"},{"location":"chapters/04-visualization-libraries-tools/#diagram-chart-type-selection-guide","title":"Diagram: Chart Type Selection Guide","text":"Chart Type Selection Guide <p>Type: infographic</p> <p>Bloom Taxonomy: Apply</p> <p>Learning Objective: Help learners quickly identify the appropriate chart type based on their data characteristics and communication goals</p> <p>Layout: 2x3 grid of chart type cards with decision criteria</p> <p>Cards (each with small icon, name, use case, and example): 1. Line Chart    - Icon: Simple line graph    - Use when: Showing trends over time    - Example: \"Stock prices over 12 months\"    - Library: Chart.js or Plotly</p> <ol> <li>Bar Chart</li> <li>Icon: Vertical bars</li> <li>Use when: Comparing categories</li> <li>Example: \"Sales by region\"</li> <li> <p>Library: Chart.js</p> </li> <li> <p>Pie/Doughnut</p> </li> <li>Icon: Pie segments</li> <li>Use when: Parts of a whole (max 6)</li> <li>Example: \"Market share distribution\"</li> <li> <p>Library: Chart.js</p> </li> <li> <p>Scatter Plot</p> </li> <li>Icon: Scattered dots</li> <li>Use when: Showing correlations</li> <li>Example: \"Height vs. weight\"</li> <li> <p>Library: Chart.js or Plotly</p> </li> <li> <p>Function Plot</p> </li> <li>Icon: Curved line (sine wave)</li> <li>Use when: Mathematical functions</li> <li>Example: \"y = sin(x)\"</li> <li> <p>Library: Plotly</p> </li> <li> <p>Radar Chart</p> </li> <li>Icon: Spider web shape</li> <li>Use when: Multi-variable comparison</li> <li>Example: \"Skill assessment profiles\"</li> <li>Library: Chart.js</li> </ol> <p>Color coding: Each card has a distinct background color Interactive features: Hover over each card to see expanded description</p> <p>Implementation: HTML/CSS grid with Chart.js mini-examples or static images</p>"},{"location":"chapters/04-visualization-libraries-tools/#vis-network-when-relationships-are-everything","title":"vis-network: When Relationships Are Everything","text":"<p>Some concepts are inherently relational. Consider learning graphs, organizational hierarchies, social networks, or dependency structures. These concepts don't fit neatly into rows and columns; they exist as webs of connections. Enter vis-network, a library designed specifically for visualizing nodes and edges.</p>"},{"location":"chapters/04-visualization-libraries-tools/#understanding-network-visualization","title":"Understanding Network Visualization","text":"<p>A network visualization consists of two fundamental elements:</p> <ul> <li>Nodes: The entities (concepts, people, systems, components)</li> <li>Edges: The relationships between entities (dependencies, connections, influences)</li> </ul> <p>vis-network brings these elements to life with physics-based layouts, smooth animations, and rich interactivity. Nodes can be dragged, clusters can be expanded, and hovering reveals details. It transforms static relationship data into an explorable landscape.</p>"},{"location":"chapters/04-visualization-libraries-tools/#the-vis-network-architecture-pattern","title":"The vis-network Architecture Pattern","text":"<p>When creating a vis-network MicroSim, we follow a standardized layout called \"vis-network-tutorial\":</p> <ul> <li>Graph on the left: The network visualization occupies most of the canvas</li> <li>Controls on the right: Interactive controls and status panels</li> <li>Title at top center: Clear identification</li> <li>Legend in upper left: Color and symbol key</li> </ul> <pre><code>// Define nodes with fixed positions for educational clarity\nconst nodeData = [\n    { id: 1, label: 'Learning Objective', x: -300, y: -100 },\n    { id: 2, label: 'Prerequisite 1', x: -450, y: 50 },\n    { id: 3, label: 'Prerequisite 2', x: -150, y: 50 },\n    { id: 4, label: 'Foundation Concept', x: -300, y: 200 }\n];\n\n// Define edges (relationships)\nconst edgeData = [\n    { from: 2, to: 1, label: 'enables' },\n    { from: 3, to: 1, label: 'enables' },\n    { from: 4, to: 2 },\n    { from: 4, to: 3 }\n];\n\n// Critical: Disable mouse zoom for iframe embedding!\nconst options = {\n    interaction: {\n        zoomView: false,        // Prevents scroll hijacking\n        dragView: false,        // Prevents accidental panning\n        navigationButtons: true // Provides explicit zoom controls\n    },\n    physics: { enabled: false } // Use fixed positions for clarity\n};\n</code></pre> <p>Always Disable Mouse Zoom in iframes</p> <p>When embedding vis-network in a textbook via iframe, you MUST disable <code>zoomView</code> and <code>dragView</code>. Otherwise, users scrolling through your textbook will accidentally zoom the diagram instead of scrolling the page. This is one of the most common usability mistakes in educational visualizations.</p>"},{"location":"chapters/04-visualization-libraries-tools/#when-vis-network-shines","title":"When vis-network Shines","text":"<p>vis-network is the perfect choice for:</p> <ul> <li>Learning graphs: Showing concept dependencies and prerequisites</li> <li>Knowledge maps: Visualizing how topics interconnect</li> <li>Organizational structures: Beyond simple hierarchies</li> <li>System architectures: Component relationships</li> <li>Social networks: Influence and connection patterns</li> <li>Causal diagrams: Cause-and-effect relationships</li> </ul>"},{"location":"chapters/04-visualization-libraries-tools/#diagram-learning-dependency-network-example","title":"Diagram: Learning Dependency Network Example","text":"Learning Dependency Network Example <p>Type: graph-model</p> <p>Bloom Taxonomy: Analyze</p> <p>Learning Objective: Demonstrate how vis-network can visualize learning dependencies between concepts, showing prerequisites flowing to target learning objectives</p> <p>Node types: 1. Foundation concepts (gray circles)    - Example: \"Basic Arithmetic\" 2. Prerequisite concepts (light blue rectangles)    - Example: \"Variables\", \"Functions\" 3. Target concept (green hexagon)    - Example: \"Calculus\" 4. Advanced concepts (purple diamonds)    - Example: \"Differential Equations\"</p> <p>Edge types: 1. PREREQUISITE_OF (solid arrows)    - Points from prerequisite to dependent concept 2. EXTENDS (dashed arrows)    - Points from concept to advanced application</p> <p>Sample data structure: - Basic Arithmetic \u2192 Variables - Basic Arithmetic \u2192 Functions - Variables \u2192 Algebra - Functions \u2192 Algebra - Algebra \u2192 Calculus - Calculus \u2192 Differential Equations</p> <p>Layout: Hierarchical with foundation concepts at bottom, target concepts at top</p> <p>Interactive features: - Click node to highlight all prerequisites - Hover to see concept definition - Step-through mode to show learning path - Legend explaining node types and colors</p> <p>Color scheme: - Foundation: #e0e0e0 (gray) - Prerequisites: #4facfe (blue) - Target: #4caf50 (green) - Advanced: #9c27b0 (purple)</p> <p>Implementation: vis-network JavaScript library with hierarchical layout</p>"},{"location":"chapters/04-visualization-libraries-tools/#timeline-visualization-history-comes-alive","title":"Timeline Visualization: History Comes Alive","text":"<p>Time is the fourth dimension, and vis-timeline helps you visualize it beautifully. Whether you're teaching historical events, project phases, or evolutionary processes, timeline visualizations place events in their temporal context, helping learners understand sequence, duration, and causality.</p>"},{"location":"chapters/04-visualization-libraries-tools/#the-power-of-temporal-context","title":"The Power of Temporal Context","text":"<p>Consider teaching the history of computing. You could list events:</p> <ul> <li>1943: ENIAC development begins</li> <li>1969: ARPANET goes live</li> <li>1989: Tim Berners-Lee proposes the World Wide Web</li> </ul> <p>Or you could show them on a timeline, letting learners see the gaps, the clusters of innovation, and the accelerating pace of change. Visual timelines answer questions like \"How long between X and Y?\" instantly, without calculation.</p>"},{"location":"chapters/04-visualization-libraries-tools/#vis-timeline-features","title":"vis-timeline Features","text":"<p>The vis-timeline library provides:</p> <ul> <li>Zoom and pan: Explore different time scales</li> <li>Category filtering: Show/hide event categories</li> <li>Hover tooltips: Additional context without clutter</li> <li>Click for details: Full event descriptions</li> <li>Date ranges: Events with duration, not just points</li> </ul> <pre><code>// Timeline data structure\nconst events = [\n    {\n        id: 1,\n        content: 'ARPANET Goes Live',\n        start: '1969-10-29',\n        group: 'networking',\n        title: 'First message sent over ARPANET: \"LO\" (the system crashed before \"LOGIN\")'\n    },\n    {\n        id: 2,\n        content: 'World Wide Web Proposed',\n        start: '1989-03-12',\n        group: 'web',\n        title: 'Tim Berners-Lee submits \"Information Management: A Proposal\"'\n    }\n];\n\n// Timeline options\nconst options = {\n    zoomable: false,  // Use buttons instead (prevents scroll hijacking)\n    moveable: true,   // Allow click-and-drag panning\n    showCurrentTime: false,\n    tooltip: { followMouse: true }\n};\n</code></pre>"},{"location":"chapters/04-visualization-libraries-tools/#timeline-best-practices-for-education","title":"Timeline Best Practices for Education","text":"<p>When creating educational timelines:</p> <ol> <li>Limit categories to 3-6: Too many colors become confusing</li> <li>Provide rich tooltips: Include \"why this matters\" context</li> <li>Use consistent date formatting: Choose one style and stick with it</li> <li>Add navigation buttons: Since scroll-zoom is disabled, provide explicit controls</li> <li>Include a \"Fit All\" button: Let users reset to see the full timeline</li> </ol>"},{"location":"chapters/04-visualization-libraries-tools/#map-visualization-geography-matters","title":"Map Visualization: Geography Matters","text":"<p>Some learning objectives are inherently spatial. Migration patterns, global events, regional comparisons, facility layouts: these concepts need maps. Leaflet is the leading open-source JavaScript library for interactive maps, and it integrates beautifully into educational MicroSims.</p>"},{"location":"chapters/04-visualization-libraries-tools/#leaflet-fundamentals","title":"Leaflet Fundamentals","text":"<p>Leaflet provides:</p> <ul> <li>Multiple tile layers: Street maps, satellite imagery, terrain</li> <li>Markers with popups: Click for information</li> <li>GeoJSON support: Complex regions and boundaries</li> <li>Layer controls: Toggle different data layers</li> <li>Responsive design: Works on mobile devices</li> </ul> <pre><code>// Initialize the map\nconst map = L.map('map').setView([37.7749, -122.4194], 10);\n\n// Add a tile layer (OpenStreetMap)\nL.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {\n    attribution: '\u00a9 OpenStreetMap contributors'\n}).addTo(map);\n\n// Add markers\nL.marker([37.7749, -122.4194])\n    .bindPopup('&lt;b&gt;San Francisco&lt;/b&gt;&lt;br&gt;The tech capital!')\n    .addTo(map);\n</code></pre>"},{"location":"chapters/04-visualization-libraries-tools/#educational-map-applications","title":"Educational Map Applications","text":"<p>Maps are essential for teaching:</p> Subject Area Map Application History Battle movements, empire expansions Geography Climate zones, population density Science Specimen locations, geological features Business Market regions, supply chains Environmental Pollution patterns, conservation areas <p>Always Include Attribution</p> <p>When using OpenStreetMap tiles, include proper attribution. It's not just polite; it's required by the license. Plus, acknowledging the work of open-source communities models good academic practice for your learners.</p>"},{"location":"chapters/04-visualization-libraries-tools/#mermaid-diagrams-from-text","title":"Mermaid: Diagrams from Text","text":"<p>Sometimes you need a diagram, but you don't need interactivity. You need a flowchart, a state diagram, or an entity-relationship diagram. Mermaid lets you create these from simple text descriptions, making them easy to version control, modify, and maintain.</p>"},{"location":"chapters/04-visualization-libraries-tools/#the-joy-of-text-based-diagrams","title":"The Joy of Text-Based Diagrams","text":"<p>Consider this Mermaid flowchart definition:</p> <pre><code>flowchart TD\n    Start(\"Identify Learning Objective\"):::startNode\n    Analyze[\"Analyze Concept Type\"]:::processNode\n    Decision{\"Animation Needed?\"}:::decisionNode\n    P5[\"Use p5.js\"]:::libraryNode\n    Static{\"Data-Driven?\"}:::decisionNode\n    Chart[\"Use Chart.js\"]:::libraryNode\n    Network{\"Relational?\"}:::decisionNode\n    VisNet[\"Use vis-network\"]:::libraryNode\n    Mermaid[\"Use Mermaid\"]:::libraryNode\n\n    Start --&gt; Analyze --&gt; Decision\n    Decision --&gt;|Yes| P5\n    Decision --&gt;|No| Static\n    Static --&gt;|Yes| Chart\n    Static --&gt;|No| Network\n    Network --&gt;|Yes| VisNet\n    Network --&gt;|No| Mermaid\n</code></pre> <p>From this simple text, Mermaid generates a professional flowchart. No drag-and-drop required. No pixel-pushing. Just describe the structure, and Mermaid renders it.</p>"},{"location":"chapters/04-visualization-libraries-tools/#mermaid-diagram-types","title":"Mermaid Diagram Types","text":"<p>Mermaid supports multiple diagram types:</p> <ul> <li>Flowcharts: Process flows and decision trees</li> <li>Sequence diagrams: Interactions over time</li> <li>State diagrams: State machines and lifecycles</li> <li>Entity-relationship diagrams: Database schemas</li> <li>Class diagrams: Object-oriented structures</li> <li>User journey maps: UX flows</li> </ul>"},{"location":"chapters/04-visualization-libraries-tools/#styling-mermaid-for-education","title":"Styling Mermaid for Education","text":"<p>For educational textbooks, we enhance Mermaid diagrams with:</p> <ul> <li>16-point fonts: Readable from the back of the classroom</li> <li>Colorful backgrounds: Visual distinction between node types</li> <li>Interactive tooltips: Educational context on hover</li> </ul> <pre><code>classDef startNode fill:#667eea,stroke:#333,stroke-width:2px,color:#fff,font-size:16px\nclassDef processNode fill:#764ba2,stroke:#333,stroke-width:2px,color:#fff,font-size:16px\nclassDef decisionNode fill:#f093fb,stroke:#333,stroke-width:2px,color:#333,font-size:16px\n</code></pre>"},{"location":"chapters/04-visualization-libraries-tools/#diagram-microsim-generation-workflow","title":"Diagram: MicroSim Generation Workflow","text":"MicroSim Generation Workflow <p>Type: workflow</p> <p>Bloom Taxonomy: Apply</p> <p>Learning Objective: Show the complete workflow from learning objective to deployed MicroSim, emphasizing the role of Claude Code skills</p> <p>Purpose: Illustrate the step-by-step process of generating a MicroSim using AI-assisted tools</p> <p>Visual style: Flowchart with swimlanes</p> <p>Swimlanes: 1. Instructional Designer 2. Claude Code 3. MicroSim Files</p> <p>Steps: 1. Start: \"Define Learning Objective\" (Designer)    Hover: \"What specific concept should students understand?\"</p> <ol> <li> <p>Process: \"Analyze Concept Type\" (Designer)    Hover: \"Is it animated? Data-driven? Relational? Geographic?\"</p> </li> <li> <p>Process: \"Select Library Type\" (Designer)    Hover: \"Based on concept type, choose p5.js, Chart.js, vis-network, etc.\"</p> </li> <li> <p>Process: \"Write Specification\" (Designer)    Hover: \"Describe the MicroSim in detail: visual elements, controls, data\"</p> </li> <li> <p>Process: \"Invoke MicroSim Generator Skill\" (Claude Code)    Hover: \"AI generates code following library-specific patterns\"</p> </li> <li> <p>Process: \"Generate Files\" (MicroSim Files)    Hover: \"main.html, script.js, style.css, index.md, metadata.json\"</p> </li> <li> <p>Decision: \"Test in Browser\" (Designer)    Hover: \"Does it work? Does it teach the concept effectively?\"</p> </li> </ol> <p>8a. Process: \"Iterate with Feedback\" (Claude Code) - if issues    Hover: \"Refine based on testing results\"</p> <p>8b. Process: \"Deploy to Textbook\" (MicroSim Files) - if success    Hover: \"Add to mkdocs.yml navigation, embed via iframe\"</p> <ol> <li>End: \"MicroSim Live!\" (All)</li> </ol> <p>Color coding: - Designer steps: Blue (#2196f3) - Claude Code steps: Purple (#9c27b0) - File operations: Green (#4caf50) - Decision: Yellow (#ffc107)</p> <p>Implementation: Mermaid flowchart with swimlane structure</p>"},{"location":"chapters/04-visualization-libraries-tools/#claude-code-skills-ai-powered-generation","title":"Claude Code Skills: AI-Powered Generation","text":"<p>Now we arrive at the magic that ties everything together: Claude Code skills. These are specialized AI capabilities that can generate complete, working MicroSims from natural language descriptions. Instead of writing hundreds of lines of JavaScript yourself, you describe what you want, and the skill produces production-ready code.</p>"},{"location":"chapters/04-visualization-libraries-tools/#what-are-claude-code-skills","title":"What Are Claude Code Skills?","text":"<p>A Claude Code skill is a set of instructions and templates that guide AI-assisted code generation. Each skill knows:</p> <ul> <li>The target library: p5.js, Chart.js, vis-network, etc.</li> <li>Required file structure: main.html, script.js, style.css, etc.</li> <li>Best practices: Responsive design, accessibility, educational patterns</li> <li>Common pitfalls: What to avoid and how to fix issues</li> </ul>"},{"location":"chapters/04-visualization-libraries-tools/#the-microsim-generator-ecosystem","title":"The MicroSim Generator Ecosystem","text":"<p>The microsim-generator skill is actually a router that directs requests to specialized sub-skills:</p> Sub-Skill Library Primary Use microsim-p5 p5.js Animations, physics simulations chartjs-generator Chart.js Standard data charts math-function-plotter-plotly Plotly.js Mathematical function plots vis-network vis-network Relationship graphs timeline-generator vis-timeline Chronological events map-generator Leaflet Geographic visualizations mermaid-generator Mermaid Flowcharts and diagrams"},{"location":"chapters/04-visualization-libraries-tools/#how-skills-match-to-requests","title":"How Skills Match to Requests","text":"<p>When you invoke the MicroSim generator, it analyzes your request for trigger words and data characteristics:</p> <ul> <li>\"bouncing ball simulation\" \u2192 microsim-p5 (animation, physics)</li> <li>\"bar chart comparing sales\" \u2192 chartjs-generator (categorical data)</li> <li>\"plot sine function\" \u2192 math-function-plotter-plotly (mathematical function)</li> <li>\"show concept dependencies\" \u2192 vis-network (nodes and edges)</li> <li>\"timeline of historical events\" \u2192 timeline-generator (dates)</li> <li>\"map of university locations\" \u2192 map-generator (coordinates)</li> <li>\"flowchart of the process\" \u2192 mermaid-generator (workflow)</li> </ul> <p>Routing Criteria</p> <p>The MicroSim generator uses a scoring system (0-100) to match requests to skills. A score of 90-100 means perfect match; 70-89 is strong match; 50-69 is moderate. The generator selects the highest-scoring skill for your request.</p>"},{"location":"chapters/04-visualization-libraries-tools/#template-libraries-standing-on-shoulders","title":"Template Libraries: Standing on Shoulders","text":"<p>Every MicroSim follows a standard file structure that makes maintenance, modification, and sharing straightforward. This modularity is not just good engineering; it's pedagogically sound. When learners (or other instructors) want to modify a MicroSim, they know exactly where to look.</p>"},{"location":"chapters/04-visualization-libraries-tools/#the-standard-microsim-structure","title":"The Standard MicroSim Structure","text":"<pre><code>docs/sims/[microsim-name]/\n\u251c\u2500\u2500 index.md          # Documentation with iframe embed\n\u251c\u2500\u2500 main.html         # HTML container with library CDN links\n\u251c\u2500\u2500 script.js         # All visualization logic\n\u251c\u2500\u2500 style.css         # Responsive styling\n\u251c\u2500\u2500 data.json         # Data separated from code (optional)\n\u2514\u2500\u2500 metadata.json     # Dublin Core metadata\n</code></pre> <p>This separation provides several benefits:</p> <ul> <li>Data independence: Update data without touching code</li> <li>Style customization: Change appearance without logic changes</li> <li>Documentation co-location: Explanation lives with the code</li> <li>Metadata for search: Faceted search engines can find MicroSims</li> <li>Version control friendly: Each file has a clear purpose</li> </ul>"},{"location":"chapters/04-visualization-libraries-tools/#template-components","title":"Template Components","text":"<p>Each template type includes:</p> <ol> <li>HTML boilerplate: Proper meta tags, CDN links, semantic structure</li> <li>CSS defaults: aliceblue background, responsive breakpoints, iframe-friendly margins</li> <li>JavaScript patterns: Standard initialization, event handling, responsive updates</li> <li>Documentation structure: Consistent sections for overview, features, customization</li> </ol>"},{"location":"chapters/04-visualization-libraries-tools/#url-parameters-for-extended-functionality","title":"URL Parameters for Extended Functionality","text":"<p>MicroSims can accept URL parameters for special modes:</p> <pre><code>&lt;!-- Normal mode --&gt;\n&lt;iframe src=\"main.html\" height=\"500\"&gt;&lt;/iframe&gt;\n\n&lt;!-- Quiz mode: Enable assessment features --&gt;\n&lt;iframe src=\"main.html?quiz-mode=true\" height=\"500\"&gt;&lt;/iframe&gt;\n\n&lt;!-- Editor mode: Allow dragging nodes to new positions --&gt;\n&lt;iframe src=\"main.html?editor-mode=true\" height=\"500\"&gt;&lt;/iframe&gt;\n</code></pre> <p>These parameters enable the same MicroSim to serve multiple purposes without code duplication.</p>"},{"location":"chapters/04-visualization-libraries-tools/#code-generation-from-specification-to-working-microsim","title":"Code Generation: From Specification to Working MicroSim","text":"<p>The final piece of our puzzle is understanding how specifications become working code. This is where the rubber meets the road, where learning objectives transform into interactive experiences.</p>"},{"location":"chapters/04-visualization-libraries-tools/#the-specification-to-code-pipeline","title":"The Specification-to-Code Pipeline","text":"<p>The code generation process follows these steps:</p> <ol> <li>Specification writing: Describe the MicroSim in detail</li> <li>Skill invocation: Call the appropriate generator skill</li> <li>Template selection: Match request to templates</li> <li>Placeholder replacement: Fill in specific values</li> <li>Code generation: Produce all required files</li> <li>Validation: Test and iterate</li> </ol>"},{"location":"chapters/04-visualization-libraries-tools/#what-makes-a-good-specification","title":"What Makes a Good Specification?","text":"<p>A specification should include:</p> <ul> <li>Learning objective: What concept will learners understand?</li> <li>Visual elements: What should appear on screen?</li> <li>Interactive controls: Sliders, buttons, checkboxes?</li> <li>Default values: Initial state of all parameters</li> <li>Behavior description: What happens when users interact?</li> <li>Edge cases: What should happen at extreme values?</li> </ul>"},{"location":"chapters/04-visualization-libraries-tools/#diagram-specification-quality-checklist","title":"Diagram: Specification Quality Checklist","text":"Specification Quality Checklist <p>Type: infographic</p> <p>Bloom Taxonomy: Evaluate</p> <p>Learning Objective: Help learners evaluate the completeness and quality of their MicroSim specifications before generation</p> <p>Layout: Checklist-style infographic with categories</p> <p>Categories with checkboxes:</p> <p>Educational Foundation - [ ] Learning objective clearly stated - [ ] Bloom's taxonomy level identified - [ ] Target audience specified - [ ] Prerequisites noted</p> <p>Visual Design - [ ] All visual elements described - [ ] Colors specified (or defaults accepted) - [ ] Layout structure defined - [ ] Responsive behavior noted</p> <p>Interactivity - [ ] All controls listed - [ ] Control ranges and defaults specified - [ ] User actions and responses described - [ ] Edge case behavior defined</p> <p>Technical Details - [ ] Library type identified - [ ] Data structure described - [ ] File naming convention followed - [ ] Integration method specified</p> <p>Visual style: Clean checklist with green checkmarks, organized in expandable sections</p> <p>Implementation: HTML/CSS interactive checklist or static infographic</p>"},{"location":"chapters/04-visualization-libraries-tools/#iterative-refinement","title":"Iterative Refinement","text":"<p>Code generation is rarely one-and-done. The typical workflow is:</p> <ol> <li>Generate initial version from specification</li> <li>Test in browser (both standalone and in iframe)</li> <li>Identify issues or improvements needed</li> <li>Refine specification or request changes</li> <li>Regenerate or modify existing code</li> <li>Repeat until satisfied</li> </ol> <p>This iterative approach mirrors the broader instructional design process. Just as learning objectives are refined through testing with actual learners, MicroSim specifications are refined through testing with actual browsers.</p> <p>Future Enhancement: xAPI Integration</p> <p>While not covered in this chapter, MicroSims can be extended with xAPI (Experience API) protocols to track learner interactions. Every slider movement, button click, and visualization state can be logged to a Learning Record Store (LRS) for analytics. This capability will be explored in a later chapter on learning analytics.</p>"},{"location":"chapters/04-visualization-libraries-tools/#putting-it-all-together-the-library-selection-matrix","title":"Putting It All Together: The Library Selection Matrix","text":"<p>After exploring all these libraries, you might feel like a kid in a candy store, unsure which treat to pick first. Here's a comprehensive matrix to guide your selection:</p> If Your Concept... Use This Library Key Strength Involves motion, physics, or custom animation p5.js Complete flexibility Compares categories with standard charts Chart.js Quick, professional results Plots mathematical functions Plotly Scientific precision Shows relationships between entities vis-network Interactive exploration Displays events over time vis-timeline Temporal navigation Involves geographic locations Leaflet Real-world mapping Describes processes or workflows Mermaid Text-based simplicity"},{"location":"chapters/04-visualization-libraries-tools/#the-decision-framework","title":"The Decision Framework","text":"<p>When selecting a library, ask these questions in order:</p> <ol> <li>Is animation or physics essential? \u2192 p5.js</li> <li>Is it primarily data/statistics? \u2192 Chart.js or Plotly</li> <li>Are there relationships/connections? \u2192 vis-network</li> <li>Is time the organizing principle? \u2192 vis-timeline</li> <li>Is geography involved? \u2192 Leaflet</li> <li>Is it a process or workflow? \u2192 Mermaid</li> </ol> <p>If you're still unsure, remember that p5.js can do almost anything (at the cost of more development time), while specialized libraries offer faster development for their specific domains.</p>"},{"location":"chapters/04-visualization-libraries-tools/#key-takeaways","title":"Key Takeaways","text":"<p>As we close this chapter, let's crystallize the essential knowledge:</p> <ol> <li> <p>p5.js is your creative powerhouse: When you need custom animations, physics simulations, or complete control, p5.js delivers. Just remember to call <code>updateCanvasSize()</code> first!</p> </li> <li> <p>Chart.js handles standard visualizations beautifully: Bar, line, pie, radar, scatter; if it's a standard chart type, Chart.js is your fastest path to professional results.</p> </li> <li> <p>Plotly excels at mathematical functions: When $f(x)$ needs visualization with precise coordinates and sliders, Plotly is the scientific choice.</p> </li> <li> <p>vis-network makes relationships visible: Nodes, edges, and the connections between concepts come alive with interactive graph visualization.</p> </li> <li> <p>vis-timeline puts events in context: Chronological data deserves chronological visualization; vis-timeline delivers with zoom, pan, and filtering.</p> </li> <li> <p>Leaflet brings maps to education: Geographic concepts need geographic visualization; Leaflet provides the foundation.</p> </li> <li> <p>Mermaid creates diagrams from text: When you need flowcharts or diagrams without interactivity overhead, Mermaid's text-based approach is unbeatable.</p> </li> <li> <p>Claude Code skills accelerate development: AI-assisted generation transforms specifications into working code, dramatically reducing development time.</p> </li> <li> <p>Modularity matters: Separate files for HTML, JavaScript, CSS, and data make MicroSims maintainable, modifiable, and shareable.</p> </li> <li> <p>Iteration is expected: Code generation is the beginning, not the end. Test, refine, and iterate to perfection.</p> </li> </ol> <p>The world becomes a better place when educators can focus on pedagogy while AI handles the implementation details. With these tools in your belt, you're equipped to transform any learning objective into an interactive, engaging MicroSim. Now go forth and visualize!</p>"},{"location":"chapters/04-visualization-libraries-tools/#references","title":"References","text":"<ul> <li>p5.js Documentation</li> <li>Chart.js Documentation</li> <li>Plotly.js Documentation</li> <li>vis-network Documentation</li> <li>vis-timeline Documentation</li> <li>Leaflet Documentation</li> <li>Mermaid Documentation</li> </ul>"},{"location":"chapters/04-visualization-libraries-tools/quiz/","title":"Quiz: Visualization Libraries and Tools","text":"<p>Test your understanding of JavaScript visualization libraries and tools for creating educational MicroSims.</p>"},{"location":"chapters/04-visualization-libraries-tools/quiz/#1-which-javascript-library-is-best-suited-for-creating-custom-animations-and-physics-simulations","title":"1. Which JavaScript library is best suited for creating custom animations and physics simulations?","text":"<ol> <li>Chart.js</li> <li>p5.js</li> <li>Mermaid</li> <li>vis-timeline</li> </ol> Show Answer <p>The correct answer is B. p5.js is the go-to library for creative, animated, and interactive visualizations. It excels at custom animations, physics simulations, and experiences requiring precise control over every pixel. The setup() and draw() paradigm allows for continuous animation loops, making it perfect for bouncing balls, projectile motion, pendulums, and other dynamic simulations.</p> <p>Concept Tested: p5.js Animation</p> <p>See: Chapter Content</p>"},{"location":"chapters/04-visualization-libraries-tools/quiz/#2-what-critical-function-must-be-called-at-the-beginning-of-setup-in-every-p5js-microsim","title":"2. What critical function must be called at the beginning of setup() in every p5.js MicroSim?","text":"<ol> <li>createCanvas()</li> <li>drawBackground()</li> <li>updateCanvasSize()</li> <li>initializeSliders()</li> </ol> Show Answer <p>The correct answer is C. The function updateCanvasSize() must be called at the beginning of setup() to ensure the simulation adapts to any screen size. This function reads the container width and enables responsive design. Forgetting this call is one of the most common mistakes new MicroSim creators make.</p> <p>Concept Tested: p5.js Animation</p> <p>See: Chapter Content</p>"},{"location":"chapters/04-visualization-libraries-tools/quiz/#3-when-should-you-choose-plotly-over-chartjs-for-data-visualization","title":"3. When should you choose Plotly over Chart.js for data visualization?","text":"<ol> <li>When you need simple bar charts</li> <li>When you need to plot mathematical functions with precise coordinates</li> <li>When you need pie charts</li> <li>When you only have categorical data</li> </ol> Show Answer <p>The correct answer is B. Plotly excels when you need to plot mathematical functions (like y = sin(x)), create scientific visualizations, or provide advanced interactivity with hover tooltips showing precise coordinates. Chart.js is better for standard business charts with discrete categorical data. Use Plotly for continuous mathematical functions; use Chart.js for standard data charts.</p> <p>Concept Tested: Plotly Library</p> <p>See: Chapter Content</p>"},{"location":"chapters/04-visualization-libraries-tools/quiz/#4-what-is-the-critical-setting-that-must-be-disabled-when-embedding-vis-network-in-a-textbook-via-iframe","title":"4. What is the critical setting that MUST be disabled when embedding vis-network in a textbook via iframe?","text":"<ol> <li>Node colors</li> <li>Edge labels</li> <li>zoomView (mouse zoom)</li> <li>Node physics</li> </ol> Show Answer <p>The correct answer is C. When embedding vis-network in an iframe, you MUST disable zoomView and dragView in the interaction options. Otherwise, users scrolling through the textbook will accidentally zoom the diagram instead of scrolling the page. This is one of the most common usability mistakes in educational visualizations.</p> <p>Concept Tested: vis-network Library</p> <p>See: Chapter Content</p>"},{"location":"chapters/04-visualization-libraries-tools/quiz/#5-what-is-the-primary-advantage-of-mermaid-for-creating-diagrams","title":"5. What is the primary advantage of Mermaid for creating diagrams?","text":"<ol> <li>It produces the most visually stunning graphics</li> <li>It allows creating diagrams from simple text descriptions without drag-and-drop</li> <li>It only works with geographical data</li> <li>It requires no browser to view</li> </ol> Show Answer <p>The correct answer is B. Mermaid's primary advantage is that it creates diagrams from simple text descriptions. You describe the structure (like flowchart steps and connections), and Mermaid renders it as a professional diagram. This makes diagrams easy to version control, modify, and maintain\u2014no pixel-pushing or drag-and-drop required.</p> <p>Concept Tested: Mermaid Library</p> <p>See: Chapter Content</p>"},{"location":"chapters/04-visualization-libraries-tools/quiz/#6-which-library-is-specifically-designed-for-visualizing-geographic-locations-with-interactive-maps","title":"6. Which library is specifically designed for visualizing geographic locations with interactive maps?","text":"<ol> <li>Chart.js</li> <li>vis-network</li> <li>Leaflet</li> <li>p5.js</li> </ol> Show Answer <p>The correct answer is C. Leaflet is the leading open-source JavaScript library for interactive maps. It provides multiple tile layers (street maps, satellite imagery, terrain), markers with popups, GeoJSON support for complex regions, layer controls, and responsive design for mobile devices. It's essential for teaching concepts with geographic components.</p> <p>Concept Tested: Leaflet Library</p> <p>See: Chapter Content</p>"},{"location":"chapters/04-visualization-libraries-tools/quiz/#7-what-does-the-microsim-generator-skill-actually-do","title":"7. What does the MicroSim Generator skill actually do?","text":"<ol> <li>It only generates p5.js code</li> <li>It routes requests to specialized sub-skills based on the type of visualization needed</li> <li>It creates physical simulations only</li> <li>It converts images to code</li> </ol> Show Answer <p>The correct answer is B. The MicroSim Generator skill is actually a router that analyzes requests for trigger words and data characteristics, then directs them to specialized sub-skills. It matches requests like \"bouncing ball simulation\" to microsim-p5, \"bar chart comparing sales\" to chartjs-generator, \"plot sine function\" to math-function-plotter-plotly, and so on.</p> <p>Concept Tested: MicroSim Generator</p> <p>See: Chapter Content</p>"},{"location":"chapters/04-visualization-libraries-tools/quiz/#8-what-is-the-standard-file-structure-for-a-microsim","title":"8. What is the standard file structure for a MicroSim?","text":"<ol> <li>A single HTML file with everything embedded</li> <li>Separate files: index.md, main.html, script.js, style.css, and optionally data.json and metadata.json</li> <li>Only JavaScript files with inline HTML</li> <li>PDF documents with embedded scripts</li> </ol> Show Answer <p>The correct answer is B. The standard MicroSim structure includes: index.md (documentation with iframe embed), main.html (HTML container with library CDN links), script.js (all visualization logic), style.css (responsive styling), and optionally data.json (data separated from code) and metadata.json (Dublin Core metadata). This separation provides data independence, style customization, and version control friendliness.</p> <p>Concept Tested: Template Library</p> <p>See: Chapter Content</p>"},{"location":"chapters/04-visualization-libraries-tools/quiz/#9-when-deciding-which-visualization-library-to-use-what-is-the-first-question-you-should-ask","title":"9. When deciding which visualization library to use, what is the first question you should ask?","text":"<ol> <li>What is the budget for development?</li> <li>Is animation or physics essential?</li> <li>How many users will view the MicroSim?</li> <li>What color scheme should be used?</li> </ol> Show Answer <p>The correct answer is B. The decision framework starts by asking \"Is animation or physics essential?\" If yes, choose p5.js. Then proceed through: Is it primarily data/statistics? (Chart.js or Plotly) \u2192 Are there relationships/connections? (vis-network) \u2192 Is time the organizing principle? (vis-timeline) \u2192 Is geography involved? (Leaflet) \u2192 Otherwise, Mermaid for process/workflow.</p> <p>Concept Tested: Code Generation</p> <p>See: Chapter Content</p>"},{"location":"chapters/04-visualization-libraries-tools/quiz/#10-what-type-of-learning-objective-is-chartjs-best-suited-for","title":"10. What type of learning objective is Chart.js best suited for?","text":"<ol> <li>Simulating projectile motion</li> <li>Comparing categorical data with standard charts like bar, line, and pie</li> <li>Showing concept dependencies in a network</li> <li>Animating sorting algorithms</li> </ol> Show Answer <p>The correct answer is B. Chart.js excels at visualizing discrete categorical data through standard chart types: bar charts for comparing categories, line charts for trends over time, pie/doughnut charts for parts of a whole, radar charts for multi-variable comparison, and scatter plots for correlations. It produces professional-looking charts quickly without the overhead needed for custom animations.</p> <p>Concept Tested: Chart.js Library</p> <p>See: Chapter Content</p>"},{"location":"chapters/05-writing-microsim-specifications/","title":"Writing Effective MicroSim Specifications","text":""},{"location":"chapters/05-writing-microsim-specifications/#summary","title":"Summary","text":"<p>This chapter teaches you how to write clear, complete specifications for MicroSim development. You will learn the anatomy of a specification document, how to describe visual elements without writing code, and how to specify interaction behaviors and constraints. The chapter covers defining success criteria, documenting edge cases, avoiding specification ambiguity, preserving pedagogical intent, and understanding how AI interprets your specifications. These skills ensure that generated MicroSims match your educational vision.</p>"},{"location":"chapters/05-writing-microsim-specifications/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 9 concepts from the learning graph:</p> <ol> <li>Specification Document</li> <li>Visual Description</li> <li>Interaction Behavior</li> <li>Behavior Constraints</li> <li>Success Criteria</li> <li>Edge Case Definition</li> <li>Specification Ambiguity</li> <li>Intent Preservation</li> <li>AI Interpretation</li> </ol>"},{"location":"chapters/05-writing-microsim-specifications/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Prerequisite Analysis and MicroSim Fundamentals</li> <li>Chapter 4: Visualization Libraries and Tools</li> </ul>"},{"location":"chapters/05-writing-microsim-specifications/#the-power-of-specification-driven-design","title":"The Power of Specification-Driven Design","text":"<p>Specification-Driven Design (SDD) is the single most important skill you'll develop as an instructional designer working with AI tools. It's the difference between getting a MicroSim that makes you say \"Yes! That's exactly what I imagined!\" versus one that makes you wonder if the AI was having an off day.</p> <p>Here's the revolutionary insight: you don't need to write code. You don't need to understand JavaScript, p5.js, or CSS. What you do need is the ability to describe precisely what you want\u2014the WHAT, not the HOW. Think of yourself as the architect, not the construction crew. Your blueprints need to be so clear that any builder (or AI) could construct your vision without asking a single clarifying question.</p> <p>Why does this matter for making the world a better place? Because every hour you spend fighting with vague specifications is an hour you could spend helping students learn. Every frustrating round of \"that's not what I meant\" is cognitive energy stolen from educational innovation. When you master SDD, you unlock the ability to rapidly prototype, iterate, and deploy educational experiences that genuinely transform how people understand difficult concepts.</p> <p>The spec is spec-tacular when done right. (Sorry, not sorry for the pun\u2014we promised fun!)</p>"},{"location":"chapters/05-writing-microsim-specifications/#what-is-a-specification-document","title":"What Is a Specification Document?","text":"<p>A specification document is your detailed blueprint for a MicroSim. It answers every question an implementer might have before they ask it. Think of it as a contract between your educational vision and the AI that will bring it to life.</p> <p>A well-crafted specification document contains these essential components:</p> <ul> <li>Title and Purpose: What is this MicroSim called, and why does it exist?</li> <li>Learning Objective: What specific concept or skill should students gain?</li> <li>Visual Description: What should the learner see on screen?</li> <li>Interaction Behavior: What can the learner do, and how does the system respond?</li> <li>Behavior Constraints: What are the limits and boundaries of the simulation?</li> <li>Success Criteria: How do we know the MicroSim is working correctly?</li> <li>Edge Cases: What happens at the boundaries of normal operation?</li> </ul> Component Purpose Example Question It Answers Title Identification \"What do we call this?\" Learning Objective Educational focus \"What will students learn?\" Visual Description Appearance \"What does it look like?\" Interaction Behavior User actions \"What can students do?\" Behavior Constraints Boundaries \"What can't happen?\" Success Criteria Validation \"How do we know it works?\" Edge Cases Robustness \"What about weird situations?\" <p>The Coffee Shop Test</p> <p>Imagine describing your MicroSim to a friend at a coffee shop who has never seen it. If they could sketch it on a napkin and describe how it works, your specification is clear enough. If they look confused and ask \"but what happens when...?\" you need more detail.</p>"},{"location":"chapters/05-writing-microsim-specifications/#visual-description-painting-with-words","title":"Visual Description: Painting with Words","text":"<p>A visual description specifies exactly what learners see on screen without writing any code. This is where many specifications fail\u2014being too vague or too technical. The sweet spot is detailed prose that creates a clear mental picture.</p>"},{"location":"chapters/05-writing-microsim-specifications/#the-anatomy-of-a-great-visual-description","title":"The Anatomy of a Great Visual Description","text":"<p>A strong visual description addresses these elements:</p> <ul> <li>Layout: Where are elements positioned relative to each other?</li> <li>Components: What objects, shapes, or UI elements exist?</li> <li>Colors: What color scheme communicates your message?</li> <li>Size relationships: What's big, what's small, what's emphasized?</li> <li>Text and labels: What words appear on screen?</li> <li>Dynamic elements: What moves or changes?</li> </ul>"},{"location":"chapters/05-writing-microsim-specifications/#example-the-vague-vs-the-vivid","title":"Example: The Vague vs. The Vivid","text":"<p>Consider this vague visual description:</p> <p>\"Show some balls bouncing around with gravity.\"</p> <p>Now compare it to this vivid specification:</p> <p>\"The canvas displays a 600-pixel wide by 400-pixel tall physics playground with a light gray background (#f0f0f0). Five circles of varying sizes (diameters: 20, 30, 40, 50, and 60 pixels) start at random horizontal positions along the top of the canvas. Each ball has a distinct color from a warm palette: red (#e74c3c), orange (#e67e22), yellow (#f1c40f), pink (#fd79a8), and coral (#ff7675). The balls fall under simulated gravity (acceleration of 0.5 pixels per frame squared), bounce off the bottom edge with 80% energy retention, and bounce off the left and right walls with 95% energy retention. A horizontal line marks the 'ground' at y=380, drawn as a 2-pixel thick dark gray line.\"</p> <p>The second version leaves nothing to imagination or interpretation. An AI\u2014or a human developer\u2014could implement this without asking any questions.</p>"},{"location":"chapters/05-writing-microsim-specifications/#diagram-visual-description-completeness-checklist","title":"Diagram: Visual Description Completeness Checklist","text":"Visual Description Completeness Checklist <p>Type: infographic</p> <p>Bloom Taxonomy Level: Remember/Understand</p> <p>Learning Objective: Students will be able to recall and verify that their visual descriptions include all essential components.</p> <p>Purpose: Provide an interactive checklist that instructional designers can use to verify their visual descriptions are complete.</p> <p>Layout: A vertical checklist with expandable sections for each category.</p> <p>Categories and items: 1. Canvas/Container    - [ ] Width specified (pixels or responsive)    - [ ] Height specified (pixels or responsive)    - [ ] Background color (hex code or named)    - [ ] Border or frame description</p> <ol> <li>Visual Elements</li> <li>[ ] Each element named/identified</li> <li>[ ] Shape or form described</li> <li>[ ] Size (absolute or relative)</li> <li>[ ] Position (absolute, relative, or algorithmic)</li> <li> <p>[ ] Color (hex codes preferred)</p> </li> <li> <p>Text Elements</p> </li> <li>[ ] Font size specified</li> <li>[ ] Text content defined</li> <li>[ ] Position described</li> <li> <p>[ ] Color specified</p> </li> <li> <p>Dynamic Elements</p> </li> <li>[ ] Initial state described</li> <li>[ ] Motion or change pattern specified</li> <li> <p>[ ] Speed or timing noted</p> </li> <li> <p>Layout Relationships</p> </li> <li>[ ] Spacing between elements</li> <li>[ ] Alignment rules</li> <li>[ ] Responsive behavior</li> </ol> <p>Interactive features: - Click checkbox to mark complete - Hover over items to see example specifications - Progress bar shows completion percentage - Export checklist as text for documentation</p> <p>Visual style: Clean, minimal design with green checkmarks for completed items Color scheme: White background, dark gray text, green (#27ae60) for completed items</p> <p>Implementation: HTML/CSS/JavaScript with localStorage for saving state</p>"},{"location":"chapters/05-writing-microsim-specifications/#interaction-behavior-the-dance-of-user-and-system","title":"Interaction Behavior: The Dance of User and System","text":"<p>Interaction behavior describes the dynamic relationship between what the learner does and how the MicroSim responds. This is where simulations come alive\u2014and where specifications often fall short.</p> <p>Every interaction has three parts:</p> <ol> <li>Trigger: What action does the user take?</li> <li>Response: What does the system do in reaction?</li> <li>Feedback: How does the user know something happened?</li> </ol>"},{"location":"chapters/05-writing-microsim-specifications/#types-of-interactions","title":"Types of Interactions","text":"Interaction Type User Action Common Responses Click Mouse click or tap Toggle state, select object, trigger animation Drag Click and move Reposition element, adjust value, draw path Slider Move handle Change parameter value continuously Hover Mouse over element Show tooltip, highlight, preview Input Type in field Update calculation, filter display Button Press button Start/stop, reset, submit"},{"location":"chapters/05-writing-microsim-specifications/#specifying-interactions-with-precision","title":"Specifying Interactions with Precision","text":"<p>A vague interaction specification might say:</p> <p>\"Users can adjust the speed.\"</p> <p>A precise specification says:</p> <p>\"A horizontal slider labeled 'Speed' appears below the animation canvas. The slider ranges from 0.1 (labeled 'Slow Motion') to 3.0 (labeled 'Fast Forward') with a default value of 1.0 (labeled 'Normal'). As the user drags the slider handle, the simulation speed multiplier updates in real-time\u2014no button press required. The current numeric value displays above the slider, updating as the handle moves. Moving the slider to 0.1 causes the animation to run at one-tenth speed; moving to 3.0 causes it to run at triple speed.\"</p> <p>The precise version answers every question: What's the range? What are the labels? What's the default? When does it take effect? What feedback does the user receive?</p>"},{"location":"chapters/05-writing-microsim-specifications/#diagram-interaction-specification-template","title":"Diagram: Interaction Specification Template","text":"Interaction Specification Template <p>Type: diagram</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Students will be able to structure interaction specifications using a consistent template that ensures completeness.</p> <p>Purpose: Show the three-part structure of a well-specified interaction.</p> <p>Components: 1. Box labeled \"TRIGGER\" (blue, left side)    - Contains: User action description    - Sub-box: Input device (mouse, keyboard, touch)    - Sub-box: Specific action (click, drag, type, etc.)    - Sub-box: Target element</p> <ol> <li> <p>Arrow from Trigger to Response (labeled \"causes\")</p> </li> <li> <p>Box labeled \"RESPONSE\" (orange, center)</p> </li> <li>Contains: System behavior</li> <li>Sub-box: What changes (visual, data, state)</li> <li>Sub-box: How fast (immediate, animated, delayed)</li> <li> <p>Sub-box: Affected elements</p> </li> <li> <p>Arrow from Response to Feedback (labeled \"produces\")</p> </li> <li> <p>Box labeled \"FEEDBACK\" (green, right side)</p> </li> <li>Contains: User perception</li> <li>Sub-box: Visual feedback (color change, animation)</li> <li>Sub-box: Audio feedback (click sound, completion tone)</li> <li>Sub-box: Text feedback (labels, tooltips, messages)</li> </ol> <p>Layout: Horizontal flow from left to right Connecting arrows: Curved arrows with labels</p> <p>Style: Rounded rectangles with drop shadows, icon in each box Color scheme: Blue (#3498db) for trigger, orange (#e67e22) for response, green (#2ecc71) for feedback</p> <p>Implementation: SVG or Mermaid diagram</p>"},{"location":"chapters/05-writing-microsim-specifications/#behavior-constraints-setting-the-rules-of-the-game","title":"Behavior Constraints: Setting the Rules of the Game","text":"<p>Behavior constraints define what can't happen in your MicroSim. They're the guardrails that prevent unexpected behavior and ensure the simulation stays educationally valid.</p> <p>Think of constraints as the rules of physics in your MicroSim universe. Without them, balls might fall through floors, sliders might accept negative numbers where they shouldn't, and simulations might enter states that confuse rather than educate.</p>"},{"location":"chapters/05-writing-microsim-specifications/#categories-of-constraints","title":"Categories of Constraints","text":"<ol> <li>Value Constraints: Limits on numerical inputs and outputs</li> <li>Minimum and maximum values</li> <li>Allowed increments (whole numbers only, multiples of 5, etc.)</li> <li> <p>Prohibited values (division by zero, negative quantities where inappropriate)</p> </li> <li> <p>State Constraints: Rules about what states are valid</p> </li> <li>Mutually exclusive options (can't be both running and paused)</li> <li>Required sequences (must click Start before Stop becomes available)</li> <li> <p>Valid combinations (certain parameters only valid together)</p> </li> <li> <p>Physical Constraints: Rules that mirror real-world physics</p> </li> <li>Objects can't overlap (collision detection)</li> <li>Values conserved (energy, mass, etc.)</li> <li> <p>Causality respected (effects follow causes)</p> </li> <li> <p>Pedagogical Constraints: Rules that ensure learning</p> </li> <li>Prevent skipping required steps</li> <li>Ensure students see consequences of actions</li> <li>Maintain appropriate difficulty progression</li> </ol>"},{"location":"chapters/05-writing-microsim-specifications/#example-constraint-specification","title":"Example: Constraint Specification","text":"<p>Consider a MicroSim teaching supply and demand curves:</p> <p>Value Constraints: - Price cannot be negative (minimum: $0.00) - Price cannot exceed $1000.00 - Quantity cannot be negative (minimum: 0 units) - Quantity cannot exceed 10,000 units</p> <p>State Constraints: - Supply and demand curves must always be visible (cannot be turned off) - Equilibrium point marker appears only when curves intersect within visible range</p> <p>Physical Constraints: - Demand curve must slope downward (higher price = lower quantity demanded) - Supply curve must slope upward (higher price = higher quantity supplied) - Curves cannot have the same slope (would never intersect or always overlap)</p> <p>Pedagogical Constraints: - Equilibrium price and quantity must be displayed numerically, not just graphically - When user adjusts demand shifters, the old demand curve remains visible as a dashed line for 3 seconds to show the shift</p>"},{"location":"chapters/05-writing-microsim-specifications/#success-criteria-how-do-we-know-it-works","title":"Success Criteria: How Do We Know It Works?","text":"<p>Success criteria are specific, testable conditions that prove your MicroSim functions correctly. They're not subjective assessments like \"looks good\"\u2014they're concrete checkpoints you can verify.</p> <p>Good success criteria follow the SMART framework:</p> <ul> <li>Specific: Exactly what should happen</li> <li>Measurable: Can be objectively verified</li> <li>Achievable: Technically possible</li> <li>Relevant: Connected to the learning objective</li> <li>Testable: Can be checked through observation</li> </ul>"},{"location":"chapters/05-writing-microsim-specifications/#writing-success-criteria","title":"Writing Success Criteria","text":"<p>Transform vague goals into testable criteria:</p> Vague Goal SMART Success Criterion \"Animation should be smooth\" \"Animation runs at 60 frames per second with no visible stuttering when tested on Chrome, Firefox, and Safari\" \"Slider should work\" \"Moving the slider from 0 to 100 updates the displayed value within 50ms, with the simulation reflecting the new value within the same animation frame\" \"Should handle bad input\" \"Typing non-numeric characters in the input field produces no change to the simulation; a red outline appears on the input field and a tooltip reads 'Please enter a number'\" \"Colors should be accessible\" \"All text maintains a minimum contrast ratio of 4.5:1 against background colors; simulation remains interpretable in grayscale mode\""},{"location":"chapters/05-writing-microsim-specifications/#a-complete-success-criteria-section","title":"A Complete Success Criteria Section","text":"<p>Here's what success criteria look like for a pendulum simulation:</p> <p>Functional Criteria: 1. Pendulum swings continuously until damping reduces amplitude below 1 pixel 2. Period changes inversely with the square root of gravity (doubling gravity reduces period by factor of \u221a2) 3. Period is independent of mass (changing mass has no effect on swing timing) 4. Amplitude affects maximum velocity but not period</p> <p>Technical Criteria: 1. Animation maintains 60fps during normal operation 2. All controls respond within 100ms of user input 3. Simulation remains stable for at least 10 minutes of continuous operation 4. Reset button returns all values to documented defaults within one frame</p> <p>Pedagogical Criteria: 1. Students can discover that period is independent of mass through experimentation 2. The relationship between length and period is visually apparent when adjusting the length slider 3. Energy display updates in real-time, showing conservation of energy principle</p>"},{"location":"chapters/05-writing-microsim-specifications/#diagram-success-criteria-flow","title":"Diagram: Success Criteria Flow","text":"Success Criteria Validation Workflow <p>Type: workflow</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Students will be able to systematically validate MicroSims against defined success criteria.</p> <p>Purpose: Show the process for validating a MicroSim against its success criteria.</p> <p>Visual style: Flowchart with decision diamonds and process rectangles</p> <p>Steps: 1. Start: \"MicroSim Generated\"    Hover text: \"AI has produced a working MicroSim from your specification\"</p> <ol> <li> <p>Process: \"Load Success Criteria Checklist\"    Hover text: \"Open the specification document to the success criteria section\"</p> </li> <li> <p>Process: \"Test Functional Criterion #1\"    Hover text: \"Manually verify the first functional requirement\"</p> </li> <li> <p>Decision: \"Criterion Met?\"    Hover text: \"Does the MicroSim behavior match the specification exactly?\"</p> </li> </ol> <p>5a. Process: \"Document Pass\" (if Yes)     Hover text: \"Check off the criterion and note any observations\"</p> <p>5b. Process: \"Document Failure Details\" (if No)     Hover text: \"Record exactly how behavior differs from specification\"</p> <ol> <li>Decision: \"More Criteria?\"    Hover text: \"Are there additional criteria to test?\"</li> </ol> <p>7a. Return to Step 3 (if Yes)</p> <p>7b. Decision: \"All Criteria Passed?\" (if No more criteria)     Hover text: \"Review overall results\"</p> <p>8a. End: \"MicroSim Approved\" (if all passed)     Hover text: \"MicroSim ready for deployment or next phase\"</p> <p>8b. Process: \"Create Refinement Request\" (if some failed)     Hover text: \"Document specific issues for AI to address in next iteration\"     Return to Step 1</p> <p>Color coding: - Blue: Testing steps - Yellow: Decision points - Green: Success outcomes - Orange: Iteration paths</p> <p>Implementation: Mermaid flowchart or custom SVG</p>"},{"location":"chapters/05-writing-microsim-specifications/#edge-case-definition-planning-for-the-unexpected","title":"Edge Case Definition: Planning for the Unexpected","text":"<p>Edge cases are the boundary conditions and unusual situations that test the robustness of your MicroSim. They're the \"what if?\" scenarios that students will inevitably discover\u2014often accidentally, sometimes mischievously.</p> <p>Edge cases aren't bugs waiting to happen; they're opportunities for learning when handled thoughtfully. A well-specified edge case defines both the unusual condition AND the desired response.</p>"},{"location":"chapters/05-writing-microsim-specifications/#common-categories-of-edge-cases","title":"Common Categories of Edge Cases","text":"<ol> <li>Extreme Values: Minimums, maximums, and values at boundaries</li> <li>Zero and Near-Zero: Empty sets, zero quantities, infinitesimally small values</li> <li>Rapid Actions: Quick repeated clicks, fast slider movements</li> <li>Invalid Inputs: Wrong data types, out-of-range values</li> <li>Timing Issues: Actions during animations, interrupted processes</li> <li>State Combinations: Unusual but valid combinations of settings</li> </ol>"},{"location":"chapters/05-writing-microsim-specifications/#specifying-edge-cases-properly","title":"Specifying Edge Cases Properly","text":"<p>For each edge case, specify:</p> <ol> <li>The Condition: What unusual situation might occur?</li> <li>The Trigger: How would this situation arise?</li> <li>The Expected Behavior: What should the MicroSim do?</li> <li>The Rationale: Why is this the correct response?</li> </ol> <p>Example Edge Case Specifications:</p> <p>Edge Case 1: Division by Zero - Condition: User adjusts denominator to zero in a fraction visualization - Trigger: Dragging the denominator slider to its minimum value of 0 - Expected Behavior: Slider stops at 1 (not 0); tooltip appears saying \"The denominator cannot be zero\u2014that would break mathematics! (And create a black hole in our server room.)\" - Rationale: Prevents undefined mathematical operations while using humor to make the constraint memorable</p> <p>Edge Case 2: Rapid Clicking - Condition: User clicks \"Next Step\" button faster than animation completes - Trigger: Multiple clicks within the 500ms animation duration - Expected Behavior: Button becomes disabled during animation (grayed out with cursor change); clicks during this period are ignored; button re-enables after animation completes - Rationale: Prevents animation queue buildup that could confuse learners or crash browser</p> <p>Edge Case 3: Extreme Parameter Combinations - Condition: User sets both damping to maximum and initial velocity to maximum - Trigger: Moving both sliders to their extreme values - Expected Behavior: Simulation runs normally but oscillation dies out within 2 seconds; informational tooltip appears: \"High damping absorbs energy quickly!\" - Rationale: Even unusual combinations should produce valid, educational results</p>"},{"location":"chapters/05-writing-microsim-specifications/#diagram-edge-case-discovery-microsim","title":"Diagram: Edge Case Discovery MicroSim","text":"Edge Case Discovery Simulator <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Students will be able to identify potential edge cases by experimenting with parameter boundaries and observing system responses.</p> <p>Canvas layout: - Full width responsive design - Drawing area (left 70%): Shows a simple physics simulation (bouncing ball) - Control panel (right 30%): Contains parameter sliders and edge case log</p> <p>Visual elements: - A single ball that bounces within a rectangular container - Container walls clearly visible - Ball shows velocity vector arrow - Current parameter values displayed numerically - Edge case detection log (scrollable text area)</p> <p>Interactive controls: 1. Slider: \"Gravity\" - Range: -10 to 10, Default: 9.8    - Note: Negative gravity is an edge case (ball falls upward) 2. Slider: \"Ball Radius\" - Range: 1 to 200, Default: 20    - Note: Radius larger than container is an edge case 3. Slider: \"Bounciness\" - Range: 0 to 2, Default: 0.8    - Note: Values &gt; 1 mean ball gains energy each bounce (edge case) 4. Slider: \"Initial Speed\" - Range: 0 to 1000, Default: 100    - Note: Extremely high speeds may cause tunneling (edge case) 5. Button: \"Log Current State\"    - Saves current parameters and observations to edge case log 6. Button: \"Reset to Defaults\"    - Returns all parameters to default values</p> <p>Edge case detection: - When ball exits container bounds, log \"EDGE CASE: Ball escaped container\" - When ball energy increases over time, log \"EDGE CASE: Energy not conserved\" - When ball overlaps walls, log \"EDGE CASE: Collision detection failed\" - When animation stutters (frame time &gt; 32ms), log \"EDGE CASE: Performance degradation\"</p> <p>Default parameters: - Gravity: 9.8 - Ball Radius: 20 - Bounciness: 0.8 - Initial Speed: 100</p> <p>Behavior: - Ball bounces continuously with physics simulation - Each parameter change takes effect immediately - Edge cases are automatically detected and logged - Log entries include timestamp and parameter values</p> <p>Educational purpose: - Students discover what constitutes edge cases through experimentation - The log helps students document their findings - Unusual behaviors become learning opportunities, not errors</p> <p>Implementation: p5.js with responsive canvas design</p>"},{"location":"chapters/05-writing-microsim-specifications/#specification-ambiguity-the-enemy-of-good-microsims","title":"Specification Ambiguity: The Enemy of Good MicroSims","text":"<p>Specification ambiguity occurs when your specification can be interpreted in multiple valid ways. It's the enemy of getting what you want, because AI tools will make some choice when faced with ambiguity\u2014just not necessarily your choice.</p> <p>Ambiguity is sneaky. What seems perfectly clear to you might be genuinely ambiguous to someone (or something) without your context.</p>"},{"location":"chapters/05-writing-microsim-specifications/#the-seven-deadly-sins-of-specification-ambiguity","title":"The Seven Deadly Sins of Specification Ambiguity","text":"<ol> <li>Vague Adjectives: \"large,\" \"small,\" \"fast,\" \"slow,\" \"nice colors\"</li> <li> <p>Fix: Use specific values: \"40 pixels,\" \"500ms animation duration,\" \"#3498db blue\"</p> </li> <li> <p>Undefined References: \"the button,\" \"that area,\" \"the main display\"</p> </li> <li> <p>Fix: Name everything: \"the Reset button,\" \"the Parameter Control Panel,\" \"the Simulation Canvas\"</p> </li> <li> <p>Assumed Context: \"like in the previous example,\" \"the usual way\"</p> </li> <li> <p>Fix: Be explicit every time; don't rely on shared context</p> </li> <li> <p>Ambiguous Pronouns: \"it should change when it's clicked\"</p> </li> <li> <p>Fix: \"The Start Button should change from green to red when the user clicks it\"</p> </li> <li> <p>Missing Defaults: \"the slider controls speed\"</p> </li> <li> <p>Fix: \"The Speed Slider ranges from 0.1x to 3.0x with a default of 1.0x\"</p> </li> <li> <p>Unspecified Timing: \"updates in real time\"</p> </li> <li> <p>Fix: \"Updates within 16ms (one frame at 60fps) of parameter change\"</p> </li> <li> <p>Implicit Behavior: Assuming something is \"obvious\"</p> </li> <li>Fix: State everything explicitly; nothing is obvious to an AI</li> </ol>"},{"location":"chapters/05-writing-microsim-specifications/#the-ambiguity-test","title":"The Ambiguity Test","text":"<p>Read your specification and ask: \"Could a reasonable person interpret this differently than I intend?\" If yes, add clarification.</p> Ambiguous Unambiguous \"Make the important elements stand out\" \"Important elements (node labels and axis titles) use 16pt bold font and #2c3e50 dark blue color\" \"The simulation should be responsive\" \"The canvas resizes to fill its container width while maintaining a 16:9 aspect ratio, with minimum width of 320px\" \"Include appropriate labels\" \"Each axis has a label in 12pt Arial: X-axis reads 'Time (seconds)', Y-axis reads 'Distance (meters)'\" \"Animation should feel natural\" \"Objects accelerate at 9.8 px/frame\u00b2 and decelerate with 0.98 velocity damping per frame\" <p>The Telephone Game</p> <p>Remember playing telephone as a kid? The message gets distorted with each person. Your specification goes through a similar process: You \u2192 Specification Document \u2192 AI Interpretation \u2192 Generated Code \u2192 Running MicroSim. Each step can introduce drift. The clearer your initial message, the more faithful the final result.</p>"},{"location":"chapters/05-writing-microsim-specifications/#intent-preservation-keeping-the-why-alive","title":"Intent Preservation: Keeping the \"Why\" Alive","text":"<p>Intent preservation ensures that your pedagogical purpose survives the journey from idea to implementation. It's not enough for a MicroSim to be technically correct\u2014it must be educationally correct.</p>"},{"location":"chapters/05-writing-microsim-specifications/#the-intent-statement","title":"The Intent Statement","text":"<p>Every specification should include an explicit intent statement that captures:</p> <ul> <li>What concept are students learning?</li> <li>What misconception might this address?</li> <li>What discovery should students make?</li> <li>What feeling should students have?</li> </ul> <p>Example Intent Statement:</p> <p>Pedagogical Intent: This MicroSim teaches the concept of exponential growth vs. linear growth. Students often underestimate how quickly exponential functions overtake linear ones. By allowing students to adjust growth rates and visually see the curves diverge, they will discover that even small exponential rates eventually dominate large linear rates. Students should feel surprised at how quickly the exponential curve \"takes off\" and internalize the phrase \"the power of exponential growth.\"</p>"},{"location":"chapters/05-writing-microsim-specifications/#how-intent-gets-lost","title":"How Intent Gets Lost","text":"<p>Intent can be lost at several stages:</p> <ol> <li>Specification Stage: Focusing on mechanics over purpose</li> <li>Generation Stage: AI optimizing for technical correctness, not learning</li> <li>Iteration Stage: Fixing bugs without checking educational impact</li> <li>Deployment Stage: Context stripped away from the simulation</li> </ol>"},{"location":"chapters/05-writing-microsim-specifications/#preserving-intent-throughout-the-process","title":"Preserving Intent Throughout the Process","text":"<p>Protect your intent by:</p> <ol> <li>State it explicitly in the specification (the Intent Statement)</li> <li>Connect every feature back to the learning objective</li> <li>Test with learners, not just technically</li> <li>Review generated results against the Intent Statement</li> <li>Resist scope creep that dilutes the core message</li> </ol>"},{"location":"chapters/05-writing-microsim-specifications/#diagram-intent-preservation-matrix","title":"Diagram: Intent Preservation Matrix","text":"Intent Preservation Matrix <p>Type: diagram</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Students will be able to trace specification elements back to learning objectives to verify intent preservation.</p> <p>Purpose: Show how each specification element should connect to the overarching pedagogical intent.</p> <p>Layout: Matrix/table diagram with connecting lines</p> <p>Structure: - Top row: Learning Objective (single box spanning width) - Second row: Key Concepts (3-5 boxes) - Third row: Features/Elements (multiple boxes) - Bottom row: Specification Details (many boxes) - Lines connect each lower element to its parent(s)</p> <p>Example content for a \"Supply and Demand\" MicroSim: - Learning Objective: \"Understand how price equilibrium emerges from supply and demand interaction\"</p> <ul> <li>Key Concepts:</li> <li>\"Supply curve slopes upward\"</li> <li>\"Demand curve slopes downward\"</li> <li>\"Equilibrium occurs at intersection\"</li> <li> <p>\"Shifts affect equilibrium\"</p> </li> <li> <p>Features:</p> </li> <li>\"Draggable demand curve\"</li> <li>\"Draggable supply curve\"</li> <li>\"Equilibrium point marker\"</li> <li>\"Price/quantity readout\"</li> <li> <p>\"Shift buttons\"</p> </li> <li> <p>Specification Details:</p> </li> <li>\"Demand curve: blue, dashed, endpoints draggable\"</li> <li>\"Supply curve: red, solid, endpoints draggable\"</li> <li>\"Intersection marked with yellow circle, 10px radius\"</li> <li>\"Numeric display updates in real-time\"</li> <li>\"Shift buttons move entire curve left/right by 20 units\"</li> </ul> <p>Visual style: - Hierarchical layout from top to bottom - Color coding by level (dark blue \u2192 medium blue \u2192 light blue \u2192 gray) - Lines show traceability (every detail connects to a concept, every concept connects to objective) - Orphan elements (unconnected) highlighted in red as warnings</p> <p>Interactive features: - Hover over any element to highlight its connections - Click element to see rationale in side panel - Toggle \"show orphans\" to find unconnected elements</p> <p>Implementation: vis-network or custom SVG with JavaScript</p>"},{"location":"chapters/05-writing-microsim-specifications/#ai-interpretation-how-machines-read-your-words","title":"AI Interpretation: How Machines Read Your Words","text":"<p>AI interpretation is the process by which an AI system transforms your natural language specification into executable code. Understanding how this works helps you write specifications that produce better results.</p>"},{"location":"chapters/05-writing-microsim-specifications/#how-ai-understands-specifications","title":"How AI \"Understands\" Specifications","text":"<p>AI language models don't understand specifications the way humans do. They:</p> <ol> <li>Pattern match against training data (similar specifications \u2192 similar outputs)</li> <li>Resolve ambiguity by picking the statistically most likely interpretation</li> <li>Fill gaps with reasonable defaults (which may not match your expectations)</li> <li>Follow conventions from the libraries and frameworks they've seen most often</li> </ol> <p>This means:</p> <ul> <li>Common patterns work better: If your specification matches patterns the AI has seen before, results are more predictable</li> <li>Unusual requests need more detail: Novel requirements need explicit specification</li> <li>Library conventions matter: Specifying \"p5.js style\" vs \"D3.js style\" activates different patterns</li> <li>Order matters: Information presented earlier in the specification may be weighted more heavily</li> </ul>"},{"location":"chapters/05-writing-microsim-specifications/#writing-for-ai-comprehension","title":"Writing for AI Comprehension","text":"<p>Optimize your specifications for AI interpretation by:</p> <ol> <li>Use declarative language: \"The canvas IS 800 pixels wide\" not \"Make the canvas about 800 pixels\"</li> <li>Front-load important information: Put critical requirements early</li> <li>Use consistent terminology: Pick one term and stick with it (don't alternate between \"button\" and \"control\")</li> <li>Be redundant strategically: State important constraints in multiple ways</li> <li>Use structured formats: Lists and tables are parsed more reliably than flowing prose</li> <li>Reference known patterns: \"Like a standard p5.js setup with setup() and draw()\" activates reliable patterns</li> </ol>"},{"location":"chapters/05-writing-microsim-specifications/#example-specification-optimized-for-ai","title":"Example: Specification Optimized for AI","text":"<p>Here's a specification structured for optimal AI interpretation:</p> <p>MicroSim: Simple Harmonic Motion Pendulum</p> <p>Implementation Framework: p5.js</p> <p>Canvas: - Width: Container width (responsive) - Height: 500 pixels - Background: #f8f9fa (light gray)</p> <p>Visual Elements: 1. Pivot Point    - Position: center-top of canvas (width/2, 50)    - Appearance: black circle, 10px diameter</p> <ol> <li>Pendulum Rod</li> <li>Start: pivot point</li> <li>End: bob center</li> <li>Appearance: black line, 2px thick</li> <li> <p>Length: controlled by Length Slider (100-300px, default 200px)</p> </li> <li> <p>Pendulum Bob</p> </li> <li>Shape: circle</li> <li>Diameter: 40px</li> <li>Color: #e74c3c (red)</li> <li>Position: calculated from rod length and current angle</li> </ol> <p>Controls (below canvas): - Slider: \"Length\" (100-300px, default 200px) - Slider: \"Gravity\" (1-20 m/s\u00b2, default 9.8) - Slider: \"Damping\" (0-0.1, default 0.01) - Button: \"Release\" - starts pendulum from current angle - Button: \"Reset\" - returns to default values and stops motion</p> <p>Physics: - Angular acceleration: (-gravity / length) * sin(angle) - damping * angularVelocity - Update angle and angular velocity each frame - Initial angle: 45 degrees (\u03c0/4 radians)</p> <p>Behavior: - Pendulum swings when \"Release\" is clicked - Parameter changes take effect immediately if pendulum is stationary - If pendulum is moving when parameter changed, it continues with new parameters - Motion stops when angular velocity drops below 0.001 radians/frame</p>"},{"location":"chapters/05-writing-microsim-specifications/#diagram-ai-specification-interpretation-microsim","title":"Diagram: AI Specification Interpretation MicroSim","text":"Specification Clarity Analyzer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Students will be able to evaluate the clarity of specification text by seeing how AI confidence varies with specificity.</p> <p>Canvas layout: - Full width responsive design - Left panel (60%): Text input area for specification text - Right panel (40%): Analysis results and confidence meters</p> <p>Visual elements: - Large text area for entering specification text - Clarity score display (0-100 scale) - Category breakdown bars:   - Visual Description clarity (0-100)   - Interaction clarity (0-100)   - Constraint clarity (0-100)   - Edge case clarity (0-100) - Issue list highlighting specific problems - Suggestion panel with improvement tips</p> <p>Interactive controls: 1. Text area: Enter or paste specification text 2. Button: \"Analyze Specification\"    - Triggers analysis and updates all meters 3. Dropdown: \"Show Issues\"    - Filters: All, Critical, Warnings, Suggestions 4. Toggle: \"Show Examples\"    - Displays example fixes inline with the text 5. Button: \"Load Sample Specs\"    - Dropdown with pre-loaded examples (good, bad, ugly)</p> <p>Analysis criteria (simplified heuristic for demo): - Detects vague adjectives (\"large\", \"small\", \"fast\", \"slow\", \"nice\", \"good\") - Flags undefined references (\"it\", \"that\", \"the button\" without prior definition) - Checks for numeric specificity in dimensions and timing - Identifies missing defaults in control specifications - Flags ambiguous color descriptions (\"blue\" vs \"#3498db\") - Rewards structured formatting (lists, tables)</p> <p>Default state: - Empty text area with placeholder: \"Paste your specification here...\" - All meters at 0 - Sample specs available in dropdown</p> <p>Behavior: - Analysis runs when button clicked (not on every keystroke) - Issues highlighted in text with colored underlines - Clicking an issue scrolls to improvement suggestion - Improvement suggestions are actionable (not just \"be more specific\")</p> <p>Educational purpose: - Students learn to recognize ambiguity in their own writing - Immediate feedback helps develop specification instincts - Comparison between good and bad samples builds pattern recognition</p> <p>Implementation: p5.js for meters/visuals, HTML for text area, JavaScript for text analysis</p>"},{"location":"chapters/05-writing-microsim-specifications/#sdd-and-the-agile-connection","title":"SDD and the Agile Connection","text":"<p>If you've worked in software development, Specification-Driven Design might feel familiar\u2014and that's no accident. SDD shares DNA with Agile methodologies, particularly user stories and acceptance test plans. Understanding this connection helps you leverage decades of battle-tested practices while adapting them for educational contexts.</p> <p>Think of it this way: Agile gave us tools for building software that users actually want. SDD gives us tools for building simulations that learners actually learn from. Same spirit, different superpowers.</p>"},{"location":"chapters/05-writing-microsim-specifications/#user-stories-and-microsim-specs-solve-the-same-problem","title":"User Stories and MicroSim Specs Solve the Same Problem","text":"<p>In Agile, a user story captures intent in a compact, testable form:</p> <p>As a [learner], I want to [manipulate X] so that I can [understand Y].</p> <p>A MicroSim specification is essentially a pedagogically enriched user story\u2014it takes the same core structure and adds the educational context that makes learning happen.</p> Agile Story Element MicroSim SDD Equivalent Actor Learner (with grade level and prior knowledge) Goal Learning objective (Bloom-classified) Motivation Conceptual understanding or skill transfer Constraints UI layout rules, cognitive load limits Acceptance Criteria Learning outcomes and interaction behaviors"},{"location":"chapters/05-writing-microsim-specifications/#example-story-to-specification","title":"Example: Story to Specification","text":"<p>Here's an Agile-style story:</p> <p>As a physics student, I want to change gravity so I can see how it affects projectile motion.</p> <p>Nice and compact! But watch what happens when we enrich it into a MicroSim specification:</p> <p>Learning Objective: Apply Newtonian mechanics to predict trajectory changes (Bloom: Apply)</p> <p>Control: Gravity slider (range: 0\u201320 m/s\u00b2, default: 9.8 m/s\u00b2, step: 0.1)</p> <p>Visual: Trajectory path updates immediately as slider moves; previous trajectory shown as faded dotted line for comparison</p> <p>Constraint: Canvas maintains 16:9 aspect ratio; width-responsive; minimum width 320px</p> <p>Misconception Addressed: \"Heavier objects fall faster\" \u2014 simulation shows identical trajectories regardless of mass setting</p> <p>Success Criterion: Student can predict landing position before releasing projectile after 5 practice throws</p> <p>The story tells us what the learner wants. The specification tells us exactly how to deliver it\u2014and how we'll know if we succeeded.</p> <p>The Key Difference</p> <p>Agile stories are planning artifacts\u2014they guide development conversations. MicroSim specifications must be executable\u2014they're contracts that AI (or developers) can implement without asking clarifying questions.</p>"},{"location":"chapters/05-writing-microsim-specifications/#acceptance-test-plans-the-bridge-between-intent-and-implementation","title":"Acceptance Test Plans: The Bridge Between Intent and Implementation","text":"<p>In Agile, acceptance criteria define when a story is \"done.\" In SDD, acceptance tests define when a MicroSim is:</p> <ul> <li>Pedagogically correct \u2014 It teaches what it claims to teach</li> <li>Technically compliant \u2014 It works as specified</li> <li>Learner-effective \u2014 Students actually learn from it</li> </ul> <p>Think of acceptance tests as the bridge where specification, implementation, and learning outcomes meet for coffee and verify they're all on the same page.</p>"},{"location":"chapters/05-writing-microsim-specifications/#the-four-categories-of-microsim-acceptance-tests","title":"The Four Categories of MicroSim Acceptance Tests","text":"<p>A complete acceptance test plan covers four distinct areas:</p> <p>1. Functional Tests</p> <p>These verify the MicroSim does what it's supposed to do mechanically:</p> <ul> <li>Slider changes immediately affect the simulation (within 16ms)</li> <li>Reset button returns all parameters to documented default values</li> <li>No controls overlap at any container width from 320px to 1920px</li> <li>All buttons respond to both click and keyboard activation</li> <li>Animation maintains 60fps under normal operation</li> </ul> <p>2. Pedagogical Tests</p> <p>These verify the MicroSim teaches what it's supposed to teach:</p> <ul> <li>Learner can observe the intended cause-and-effect relationship</li> <li>Target misconception is visibly contradicted by simulation behavior</li> <li>Learning objective is achievable within reasonable time (typically \u226415 minutes)</li> <li>Progressive complexity allows scaffolded discovery</li> <li>Key concepts are reinforced through multiple interaction patterns</li> </ul> <p>3. Technical Tests</p> <p>These verify the MicroSim meets implementation standards:</p> <ul> <li>Runs unmodified in standard p5.js web editor</li> <li>Passes responsive resize test (works at mobile, tablet, and desktop widths)</li> <li>Includes accessibility description for screen readers</li> <li>Loads within 3 seconds on standard broadband connection</li> <li>No console errors or warnings during normal operation</li> </ul> <p>4. Assessment-Ready Tests</p> <p>These verify the MicroSim can support learning analytics:</p> <ul> <li>User interactions are observable and loggable</li> <li>Completion or mastery conditions are inferable from interaction patterns</li> <li>Events can be logged deterministically (same inputs \u2192 same event sequence)</li> <li>Timestamps available for time-on-task analysis</li> <li>Error states and recovery actions are distinguishable in logs</li> </ul>"},{"location":"chapters/05-writing-microsim-specifications/#diagram-acceptance-test-coverage-matrix","title":"Diagram: Acceptance Test Coverage Matrix","text":"MicroSim Acceptance Test Coverage Matrix <p>Type: infographic</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Students will be able to evaluate whether a MicroSim meets all acceptance criteria across functional, pedagogical, technical, and assessment dimensions.</p> <p>Purpose: Interactive matrix showing test coverage across all four categories with pass/fail tracking.</p> <p>Layout: 4-column grid with expandable test items</p> <p>Structure: - Header row with four category labels (Functional, Pedagogical, Technical, Assessment-Ready) - Each column contains 5-7 specific test criteria - Each criterion has checkbox and status indicator - Bottom row shows category completion percentage - Overall coverage score displayed prominently</p> <p>Test items per category:</p> <p>Functional (green theme): - [ ] Controls respond within 16ms - [ ] Reset returns to defaults - [ ] No UI overlap at any width - [ ] Keyboard navigation works - [ ] Animation maintains 60fps - [ ] All specified states reachable</p> <p>Pedagogical (blue theme): - [ ] Cause-effect visible - [ ] Misconception addressed - [ ] Objective achievable in \u226415min - [ ] Scaffolding appropriate - [ ] Feedback immediate and clear</p> <p>Technical (orange theme): - [ ] Runs in p5.js editor - [ ] Responsive design works - [ ] Accessibility included - [ ] Loads in &lt;3 seconds - [ ] No console errors</p> <p>Assessment-Ready (purple theme): - [ ] Interactions loggable - [ ] Completion detectable - [ ] Events deterministic - [ ] Timestamps available - [ ] Errors distinguishable</p> <p>Interactive features: - Click checkbox to toggle pass/fail - Hover for detailed test description - Color coding: green=pass, red=fail, gray=untested - Export results as markdown checklist - Summary score updates in real-time</p> <p>Visual style: Clean grid with rounded corners, subtle shadows Color scheme: Category-specific colors with white backgrounds</p> <p>Implementation: HTML/CSS/JavaScript with localStorage for persistence</p>"},{"location":"chapters/05-writing-microsim-specifications/#why-this-matters-for-ai-generated-microsims","title":"Why This Matters for AI-Generated MicroSims","text":"<p>Here's where SDD becomes genuinely transformative: it's what makes generative AI reliable instead of lucky.</p> <p>Without SDD:</p> <ul> <li>Each MicroSim is a one-off creative act</li> <li>Quality varies wildly between generations</li> <li>Reuse and search are nearly impossible</li> <li>\"It worked last time\" is your best debugging strategy</li> </ul> <p>With SDD:</p> <ul> <li>AI can select, adapt, and compose MicroSims systematically</li> <li>Learning objectives map cleanly to simulation patterns</li> <li>Automated quality scoring becomes possible</li> <li>MicroSims become composable units in a larger learning graph</li> <li>Iteration is targeted rather than random</li> </ul> <p>The magic formula is: Stories \u2192 Specifications \u2192 Acceptance Tests \u2192 Reproducible MicroSims</p> <p>This pipeline transforms educational design from an artisanal craft (beautiful but unscalable) into an engineering discipline (consistent, measurable, improvable). And yes, it's still creative\u2014just like architecture is creative while still following building codes.</p> <p>The Executable Contract</p> <p>Specification-driven design turns a learning objective into an executable contract. Agile stories express learner intent. Acceptance test plans verify that the resulting MicroSim faithfully delivers both the pedagogy and the behavior promised by the specification. Together, they form a complete quality assurance chain from concept to classroom.</p>"},{"location":"chapters/05-writing-microsim-specifications/#putting-it-all-together-complete-specification-examples","title":"Putting It All Together: Complete Specification Examples","text":"<p>Now let's see how all these concepts combine into complete, production-ready specifications. Each example demonstrates how to avoid ambiguity, preserve intent, and write for AI comprehension.</p>"},{"location":"chapters/05-writing-microsim-specifications/#example-1-ohms-law-interactive-demonstration","title":"Example 1: Ohm's Law Interactive Demonstration","text":"<p>This specification teaches the relationship between voltage, current, and resistance.</p>"},{"location":"chapters/05-writing-microsim-specifications/#diagram-ohms-law-circuit-simulator","title":"Diagram: Ohm's Law Circuit Simulator","text":"Ohm's Law Circuit Simulator <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Students will be able to apply Ohm's Law (V = IR) by manipulating circuit parameters and observing how changes in one variable affect the others.</p> <p>PEDAGOGICAL INTENT: Students often memorize V = IR without understanding the relationships. This MicroSim allows them to discover that increasing voltage increases current (direct relationship) while increasing resistance decreases current (inverse relationship). The \"aha moment\" occurs when students realize they can predict outcomes before seeing them.</p> <p>CANVAS SPECIFICATION: - Layout: Full width responsive, minimum width 320px - Height: 450 pixels - Background: #ffffff (white) - Organized into three horizontal regions:   1. Circuit display area (top, 300px height)   2. Control panel (middle, 100px height)   3. Equation display (bottom, 50px height)</p> <p>CIRCUIT VISUAL ELEMENTS: 1. Battery (left side)    - Position: x=50, y=150    - Symbol: Standard battery symbol (long and short parallel lines)    - Size: 60px tall    - Color: Black lines, 2px stroke    - Label: \"V\" with current voltage value below (e.g., \"V = 12V\")</p> <ol> <li>Resistor (top of circuit)</li> <li>Position: centered horizontally, y=50</li> <li>Symbol: Zigzag line (standard resistor symbol)</li> <li>Size: 100px wide, 30px tall</li> <li>Color: Black lines, 2px stroke</li> <li>Label: \"R\" with current resistance value (e.g., \"R = 4\u03a9\")</li> <li> <p>Fill color changes based on current flow: cold (#3498db blue) at low current, hot (#e74c3c red) at high current</p> </li> <li> <p>Ammeter (right side)</p> </li> <li>Position: x=canvas width - 80, y=150</li> <li>Symbol: Circle with \"A\" inside</li> <li>Size: 40px diameter</li> <li>Color: Black outline, white fill</li> <li> <p>Display: Current value in center (e.g., \"3.0 A\")</p> </li> <li> <p>Wires</p> </li> <li>Color: Black, 3px stroke</li> <li>Connect battery (+) \u2192 top wire \u2192 resistor \u2192 right wire \u2192 ammeter \u2192 bottom wire \u2192 battery (-)</li> <li>Current flow animation: Small yellow dots move along wires in direction of conventional current</li> <li> <p>Animation speed proportional to current magnitude</p> </li> <li> <p>Electron flow visualization (optional toggle)</p> </li> <li>Blue dots moving opposite to conventional current</li> <li>Default: OFF</li> </ol> <p>CONTROL PANEL: Position: Below circuit, full width, light gray background (#f0f0f0)</p> <ol> <li>Voltage Slider</li> <li>Label: \"Voltage (V)\"</li> <li>Range: 1V to 24V</li> <li>Default: 12V</li> <li>Step: 1V</li> <li>Width: 200px</li> <li>Position: Left third of control panel</li> <li> <p>Numeric display above slider shows current value</p> </li> <li> <p>Resistance Slider</p> </li> <li>Label: \"Resistance (\u03a9)\"</li> <li>Range: 1\u03a9 to 20\u03a9</li> <li>Default: 4\u03a9</li> <li>Step: 0.5\u03a9</li> <li>Width: 200px</li> <li>Position: Center of control panel</li> <li> <p>Numeric display above slider shows current value</p> </li> <li> <p>Control Buttons</p> </li> <li>Position: Right third of control panel</li> <li>\"Reset\" button: Returns all values to defaults (12V, 4\u03a9)</li> <li>\"Show Equation\" toggle: Reveals/hides the calculation at bottom</li> </ol> <p>EQUATION DISPLAY: Position: Bottom region, centered - Shows: \"V = I \u00d7 R  \u2192  [Voltage Value] = [Current Value] \u00d7 [Resistance Value]\" - Example: \"V = I \u00d7 R  \u2192  12V = 3.0A \u00d7 4\u03a9\" - Font: 18px monospace - Updates in real-time with slider changes - Hidden by default, shown when toggle is ON</p> <p>BEHAVIOR SPECIFICATION: 1. When Voltage slider moves:    - Current recalculates immediately: I = V/R    - Wire animation speed updates    - Ammeter display updates    - Equation display updates    - Battery label updates</p> <ol> <li>When Resistance slider moves:</li> <li>Current recalculates immediately: I = V/R</li> <li>Wire animation speed updates</li> <li>Ammeter display updates</li> <li>Resistor heat color updates (lerp from blue to red based on power dissipation)</li> <li>Equation display updates</li> <li> <p>Resistor label updates</p> </li> <li> <p>Current flow animation:</p> </li> <li>Yellow dots appear on wires</li> <li>10 dots evenly spaced around circuit</li> <li>Speed: pixels per frame = current \u00d7 2</li> <li>When current is 0.5A, dots move at 1 px/frame</li> <li>When current is 12A, dots move at 24 px/frame</li> </ol> <p>BEHAVIOR CONSTRAINTS: - Current cannot exceed 24A (would occur at V=24, R=1) - Current cannot be negative - If calculated current exceeds 20A, ammeter flashes red and displays \"OVERLOAD!\" - Minimum current display: 0.1A (if calculated is less, show \"&lt;0.1A\")</p> <p>SUCCESS CRITERIA: 1. Moving voltage slider from 12V to 24V with R=4\u03a9 doubles the current from 3A to 6A 2. Moving resistance slider from 4\u03a9 to 8\u03a9 with V=12V halves the current from 3A to 1.5A 3. All three values (V, I, R) are always mathematically consistent with V = IR 4. Animation speed visibly changes when current changes 5. Resistor color shifts from blue (1A) to red (20A+) based on current 6. Reset button returns to exactly: V=12V, R=4\u03a9, I=3A</p> <p>EDGE CASES: 1. Minimum resistance (1\u03a9) with maximum voltage (24V):    - Current = 24A, displays \"OVERLOAD!\" with flashing    - Resistor color: bright red    - Dots move very quickly    - Simulation continues (does not stop)</p> <ol> <li>Maximum resistance (20\u03a9) with minimum voltage (1V):</li> <li>Current = 0.05A, displays \"&lt;0.1A\"</li> <li>Resistor color: blue</li> <li>Dots move very slowly (barely perceptible)</li> <li> <p>Simulation continues normally</p> </li> <li> <p>Rapid slider movement:</p> </li> <li>All values update every frame</li> <li>No queuing or lag in updates</li> </ol> <p>ACCESSIBILITY: - All text minimum 14px - Contrast ratio 4.5:1 minimum - Keyboard navigation for sliders (left/right arrows) - Current value announced to screen readers on change</p> <p>Implementation: p5.js with responsive canvas design</p>"},{"location":"chapters/05-writing-microsim-specifications/#example-2-probability-tree-diagram-explorer","title":"Example 2: Probability Tree Diagram Explorer","text":"<p>This specification teaches conditional probability through interactive tree diagrams.</p>"},{"location":"chapters/05-writing-microsim-specifications/#diagram-probability-tree-explorer","title":"Diagram: Probability Tree Explorer","text":"Probability Tree Explorer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Students will be able to calculate compound probabilities by tracing paths through a probability tree and multiplying branch probabilities.</p> <p>PEDAGOGICAL INTENT: Students struggle to understand why we multiply probabilities along branches. This MicroSim makes the multiplication visible by showing how the path width narrows with each branch. When students trace a path to \"draw two red balls,\" they SEE the probability shrinking, not just calculate it. The target \"aha moment\" is: \"Oh! The path gets narrower because we're keeping less of the total each time!\"</p> <p>CANVAS SPECIFICATION: - Layout: Full width responsive, minimum width 400px - Height: 600 pixels - Background: #fafafa (off-white) - Regions:   1. Tree diagram area (top, 400px)   2. Calculation display (middle, 100px)   3. Control panel (bottom, 100px)</p> <p>SCENARIO: A bag contains red and blue balls. Draw two balls without replacement. The tree shows all possible outcomes.</p> <p>TREE VISUAL ELEMENTS: 1. Root Node (start)    - Position: Left side, vertically centered in tree area    - Label: \"Start\" in 14px font    - Visual: Circle, 30px diameter, white fill, black outline    - Represents: The bag before any draws</p> <ol> <li>First Level Branches (from root)</li> <li>Branch to \"First Red\": Probability = (red balls / total balls)</li> <li>Branch to \"First Blue\": Probability = (blue balls / total balls)</li> <li>Branch width: Proportional to probability (probability \u00d7 50 pixels)</li> <li>Red branch color: #e74c3c (red), Blue branch color: #3498db (blue)</li> <li>Label on each branch: Shows probability as fraction and decimal</li> <li> <p>Example: \"5/8 = 0.625\"</p> </li> <li> <p>First Level Nodes</p> </li> <li>\"First Red\" node: Red circle, 25px, positioned right of root</li> <li>\"First Blue\" node: Blue circle, 25px, positioned right of root</li> <li> <p>Vertical spacing: 150px between nodes</p> </li> <li> <p>Second Level Branches (from each first level node)</p> </li> <li>From \"First Red\":<ul> <li>To \"Red then Red\": Probability = (red-1)/(total-1)</li> <li>To \"Red then Blue\": Probability = blue/(total-1)</li> </ul> </li> <li>From \"First Blue\":<ul> <li>To \"Blue then Red\": Probability = red/(total-1)</li> <li>To \"Blue then Blue\": Probability = (blue-1)/(total-1)</li> </ul> </li> <li>Branch width: Proportional to CUMULATIVE probability from root</li> <li> <p>Labels: Show conditional probability for this branch</p> </li> <li> <p>Final Outcome Nodes (rightmost)</p> </li> <li>Four outcomes: RR, RB, BR, BB</li> <li>Each shows final probability (product of path)</li> <li>Node size proportional to final probability</li> <li> <p>Color: Gradient of path colors (red-red = dark red, blue-blue = dark blue, mixed = purple)</p> </li> <li> <p>Path highlighting</p> </li> <li>When user hovers over final node, entire path highlights</li> <li>Path glows yellow</li> <li>Calculation appears in calculation display area</li> </ol> <p>CONTROL PANEL: 1. Slider: \"Red Balls\"    - Range: 1 to 10    - Default: 5    - Position: Left side of control panel</p> <ol> <li>Slider: \"Blue Balls\"</li> <li>Range: 1 to 10</li> <li>Default: 3</li> <li> <p>Position: Center of control panel</p> </li> <li> <p>Button: \"Reset Defaults\"</p> </li> <li>Returns to 5 red, 3 blue</li> <li> <p>Position: Right side of control panel</p> </li> <li> <p>Toggle: \"Show Calculations\"</p> </li> <li>When ON: Branch labels show full calculations</li> <li>When OFF: Labels show only final probabilities</li> <li>Default: ON</li> </ol> <p>CALCULATION DISPLAY: - Shows the multiplication for the currently hovered path - Example when hovering RR path:   \"P(Red then Red) = P(1st Red) \u00d7 P(2nd Red | 1st Red)\"   \"P(RR) = 5/8 \u00d7 4/7 = 20/56 = 5/14 \u2248 0.357\" - Font: 16px monospace - Updates instantly on hover - When nothing hovered: \"Hover over an outcome to see the calculation\"</p> <p>BEHAVIOR SPECIFICATION: 1. When ball counts change:    - Tree redraws with new probabilities    - Branch widths recalculate    - All labels update    - Animation: 300ms transition for branch width changes</p> <ol> <li>When hovering outcome node:</li> <li>Path from root to that node highlights (yellow glow)</li> <li>Calculation display shows step-by-step multiplication</li> <li> <p>Other paths dim (opacity 0.3)</p> </li> <li> <p>When not hovering:</p> </li> <li>All paths at normal opacity</li> <li>Calculation display shows instruction text</li> </ol> <p>BEHAVIOR CONSTRAINTS: - Total balls must be at least 2 (minimum 1 red + 1 blue) - Red slider cannot go to 0 if blue is 1 (and vice versa) - Probabilities always sum to 1 at each level (validate visually by total branch widths)</p> <p>SUCCESS CRITERIA: 1. With 5 red and 3 blue balls:    - P(RR) = 5/8 \u00d7 4/7 = 20/56 \u2248 0.357    - P(RB) = 5/8 \u00d7 3/7 = 15/56 \u2248 0.268    - P(BR) = 3/8 \u00d7 5/7 = 15/56 \u2248 0.268    - P(BB) = 3/8 \u00d7 2/7 = 6/56 \u2248 0.107    - Total: 56/56 = 1.000</p> <ol> <li>Branch widths visually represent probabilities:</li> <li>First Red branch visibly wider than First Blue branch (5/8 vs 3/8)</li> <li> <p>RR path visibly narrower than First Red branch alone</p> </li> <li> <p>Changing red balls from 5 to 8 immediately updates all probabilities</p> </li> <li> <p>All four outcome probabilities sum to 1.00 (displayed to 3 decimal places)</p> </li> </ol> <p>EDGE CASES: 1. Equal balls (5 red, 5 blue):    - First branches equal width    - All conditional probabilities symmetric</p> <ol> <li>Extreme ratio (10 red, 1 blue):</li> <li>Blue branch very narrow but still visible (minimum 3px width)</li> <li> <p>P(BB) = 1/11 \u00d7 0/10 = 0 \u2192 Display as \"0.000 (impossible)\"</p> </li> <li> <p>Minimum balls (1 red, 1 blue):</p> </li> <li>Only two outcomes possible: RB and BR</li> <li>Both with probability 0.500</li> <li>Note: RR and BB paths show \"0.000 (impossible)\"</li> </ol> <p>ACCESSIBILITY: - Color is not the only distinguisher (branches also have labels) - Keyboard navigation: Tab through outcome nodes - Screen reader: Announces probability calculations when node focused</p> <p>Implementation: p5.js with responsive canvas design</p>"},{"location":"chapters/05-writing-microsim-specifications/#example-3-sorting-algorithm-race","title":"Example 3: Sorting Algorithm Race","text":"<p>This specification compares different sorting algorithms visually.</p>"},{"location":"chapters/05-writing-microsim-specifications/#diagram-sorting-algorithm-racing-microsim","title":"Diagram: Sorting Algorithm Racing MicroSim","text":"Sorting Algorithm Racing MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze/Evaluate</p> <p>Learning Objective: Students will be able to compare the efficiency of different sorting algorithms by observing their operation counts and completion times on identical datasets.</p> <p>PEDAGOGICAL INTENT: Students often memorize Big-O notation without understanding what it means in practice. This MicroSim creates a race between algorithms so students can SEE that O(n\u00b2) algorithms start fast but lose badly on large datasets, while O(n log n) algorithms have more overhead but scale better. The \"aha moment\" is watching Bubble Sort lead initially, then get demolished by Quick Sort as the array grows.</p> <p>CANVAS SPECIFICATION: - Layout: Full width responsive, minimum width 600px - Height: 650 pixels - Background: #1a1a2e (dark blue) - Regions:   1. Race lanes (top, 500px) - Four horizontal lanes for algorithms   2. Stats display (middle, 100px) - Comparison counts, swap counts, time   3. Control panel (bottom, 50px) - Array size, speed, start/reset</p> <p>RACE LANE VISUAL ELEMENTS: Each algorithm gets its own lane (4 lanes total, stacked vertically):</p> <ol> <li>Lane Structure:</li> <li>Width: 90% of canvas width, centered</li> <li>Height: 100px per lane</li> <li>Background: #16213e (slightly lighter blue)</li> <li>Border: 1px solid #0f3460</li> <li>Left side: Algorithm name and current status</li> <li> <p>Right side: Bar visualization of array being sorted</p> </li> <li> <p>Bar Visualization (per lane):</p> </li> <li>Array represented as vertical bars</li> <li>Each bar width: lane width / array size</li> <li>Bar height: Proportional to value (value/max \u00d7 80px)</li> <li>Bar color: Gradient based on value (low=#0f3460 to high=#e94560)</li> <li>Currently compared bars: Yellow (#ffc857) highlight</li> <li> <p>Currently swapping bars: White (#ffffff) highlight with swap animation</p> </li> <li> <p>Algorithm Labels:</p> </li> <li>Position: Left side of each lane, 15px from edge</li> <li>Font: 14px bold, white</li> <li>Shows: Algorithm name + current comparison count</li> <li> <p>Example: \"Bubble Sort: 1,247 comparisons\"</p> </li> <li> <p>Progress Indicator:</p> </li> <li>Below each lane</li> <li>Horizontal progress bar showing % sorted</li> <li>Color: Green (#4ecca3) fill</li> </ol> <p>ALGORITHMS TO RACE: Lane 1: Bubble Sort (red theme) Lane 2: Selection Sort (orange theme) Lane 3: Insertion Sort (yellow theme) Lane 4: Quick Sort (green theme)</p> <p>STATS DISPLAY: Columns showing real-time statistics: | Algorithm | Comparisons | Swaps | Time (ms) | Status | | Each cell updates in real-time during race |</p> <p>CONTROL PANEL: 1. Slider: \"Array Size\"    - Range: 10 to 200    - Default: 50    - Step: 10    - Position: Left third</p> <ol> <li>Slider: \"Animation Speed\"</li> <li>Range: 1 (slow, 500ms per step) to 100 (fast, 5ms per step)</li> <li>Default: 50 (50ms per step)</li> <li> <p>Position: Center</p> </li> <li> <p>Buttons:</p> </li> <li>\"Generate New Array\": Creates new random array</li> <li>\"Start Race\": Begins all four sorts simultaneously</li> <li>\"Pause\": Pauses all algorithms (becomes \"Resume\")</li> <li>\"Reset\": Clears stats, regenerates array</li> <li>Position: Right third</li> </ol> <p>BEHAVIOR SPECIFICATION: 1. Array Generation:    - All four lanes receive IDENTICAL copy of the random array    - Values range from 1 to array size (no duplicates for clarity)    - Array is shuffled using Fisher-Yates shuffle</p> <ol> <li>Race Execution:</li> <li>All four algorithms run in parallel (using setTimeout for visualization)</li> <li>Each algorithm proceeds at same visualization speed</li> <li>Algorithm completes when array is sorted (verified)</li> <li> <p>Completion order is the race result</p> </li> <li> <p>Visualization Updates:</p> </li> <li>Each comparison highlights the two compared bars in yellow</li> <li>Each swap shows bars moving (quick animation: 100ms)</li> <li>Bars in final position don't change color again</li> <li> <p>Stats update after each operation</p> </li> <li> <p>Race Completion:</p> </li> <li>Winning algorithm lane flashes green</li> <li>Final stats frozen for comparison</li> <li>\"Race Complete!\" message appears</li> <li>Ranking displayed: 1st, 2nd, 3rd, 4th with times</li> </ol> <p>BEHAVIOR CONSTRAINTS: - Cannot start race while one is in progress - Cannot change array size during race - Animation speed CAN be adjusted during race - Pause affects all algorithms equally (no cheating!) - Each algorithm must perform identical logical operations regardless of speed</p> <p>SUCCESS CRITERIA: 1. With 50-element array, Quick Sort finishes first in &gt;90% of races 2. Comparison counts for Bubble Sort: approximately n\u00b2 (2500 for n=50) 3. Comparison counts for Quick Sort: approximately n log n (280 for n=50) 4. All four lanes show identical starting arrays 5. Speed slider makes visible difference in animation pace 6. Stats are accurate (manually verifiable for small arrays)</p> <p>EDGE CASES: 1. Already sorted array:    - Generate option includes \"Already Sorted\" preset    - Insertion Sort should complete with only n-1 comparisons    - Bubble Sort optimized version detects and exits early</p> <ol> <li>Reverse sorted array:</li> <li>Generate option includes \"Reverse Sorted\" preset</li> <li>This is worst case for some algorithms</li> <li> <p>Quick Sort (with poor pivot) may struggle</p> </li> <li> <p>Very small array (10 elements):</p> </li> <li>All algorithms complete almost instantly</li> <li> <p>Slows down minimum time to ensure visible difference</p> </li> <li> <p>Very large array (200 elements):</p> </li> <li>May need to cap animation frames</li> <li>Bubble Sort could take very long (40,000 operations)</li> <li>Show warning if estimated time &gt; 60 seconds</li> </ol> <p>EDUCATIONAL ANNOTATIONS: - Hover over algorithm name to see Big-O notation and brief explanation - \"Why is X winning?\" tooltip explains current leader's advantage - After race: Pop-up comparing expected vs actual operation counts</p> <p>ACCESSIBILITY: - Colors not sole distinguisher (algorithm names always visible) - Pause allows time to observe state - Screen reader announces comparison counts periodically - Keyboard: Space = start/pause, R = reset</p> <p>Implementation: p5.js with responsive canvas design</p>"},{"location":"chapters/05-writing-microsim-specifications/#summary-your-specification-checklist","title":"Summary: Your Specification Checklist","text":"<p>Writing effective MicroSim specifications is both an art and a science. Like any skill, it improves with practice. As you write more specifications, you'll develop an intuition for what needs to be explicit and what can be left to sensible defaults.</p> <p>Remember these key principles:</p> <ol> <li>Be specific, not vague: Use numbers, hex colors, and precise descriptions</li> <li>State the WHAT, not the HOW: Focus on behavior and appearance, not implementation</li> <li>Preserve your intent: Always connect features back to learning objectives</li> <li>Plan for edge cases: Think about what happens at boundaries and extremes</li> <li>Write for AI comprehension: Use structured formats and consistent terminology</li> <li>Test your criteria: Ensure success criteria are measurable and testable</li> </ol> <p>The investment you make in writing clear specifications pays dividends throughout the development process. A well-specified MicroSim can be generated correctly on the first try, iterated quickly, and maintained easily. A poorly specified one will frustrate you with endless rounds of \"that's not what I meant.\"</p> <p>You now have the tools to spec-ify your vision with crystal clarity. Go forth and make educational simulations that change how people learn!</p> Quick Check: Can you identify the ambiguity? <p>Consider this specification snippet: \"The slider should update the display quickly when moved.\"</p> <p>What's ambiguous here?</p> <ul> <li>\"quickly\" - How quickly? 50ms? 500ms? Within the same frame?</li> <li>\"the display\" - Which display? There might be multiple displays</li> <li>\"when moved\" - While moving? After releasing? Both?</li> </ul> <p>A better version: \"The Speed Slider updates the Velocity Display within 16ms (one animation frame) as the user drags the handle, providing real-time visual feedback during interaction.\"</p>"},{"location":"chapters/05-writing-microsim-specifications/#whats-next","title":"What's Next?","text":"<p>In the next chapter, we'll explore how to adapt MicroSim complexity for different audience levels\u2014from kindergarteners to PhD students. You'll learn how the same core concept can be presented at wildly different complexity levels, and how your specifications need to change accordingly.</p> <p>Until then, practice writing specifications for simple MicroSims. Start with something you know well, like a basic physics concept or a math visualization. Write the specification first, without thinking about code. Then imagine handing that specification to someone who has never seen what you're picturing. Could they build it?</p> <p>If not, add more detail. If so, you're ready to let AI bring your vision to life!</p> <p>\"A specification is a promise to your future self that you know what you want.\" \u2014 Every instructional designer who's ever watched an AI interpret their vague request creatively</p>"},{"location":"chapters/05-writing-microsim-specifications/quiz/","title":"Quiz: Writing Effective MicroSim Specifications","text":"<p>Test your understanding of Specification-Driven Design and how to write clear, complete specifications that AI tools can implement faithfully.</p>"},{"location":"chapters/05-writing-microsim-specifications/quiz/#1-what-is-specification-driven-design-sdd","title":"1. What is Specification-Driven Design (SDD)?","text":"<ol> <li>A programming methodology focused on writing efficient code</li> <li>The skill of describing precisely what you want\u2014the WHAT, not the HOW\u2014so that AI can implement your vision</li> <li>A design pattern for database schemas</li> <li>A method for testing software after development</li> </ol> Show Answer <p>The correct answer is B. Specification-Driven Design is the skill of describing precisely what you want\u2014the WHAT, not the HOW. You don't need to write code; you need to describe behavior and appearance so clearly that any builder (or AI) could construct your vision without asking clarifying questions. Think of yourself as the architect, not the construction crew.</p> <p>Concept Tested: Specification Document</p> <p>See: Chapter Content</p>"},{"location":"chapters/05-writing-microsim-specifications/quiz/#2-which-of-the-following-is-an-example-of-a-vague-visual-description-that-should-be-avoided","title":"2. Which of the following is an example of a vague visual description that should be avoided?","text":"<ol> <li>\"Five circles of varying sizes (diameters: 20, 30, 40, 50, and 60 pixels) with colors #e74c3c, #e67e22, #f1c40f, #fd79a8, and #ff7675\"</li> <li>\"A 600-pixel wide by 400-pixel tall physics playground with a light gray background (#f0f0f0)\"</li> <li>\"Show some balls bouncing around with gravity\"</li> <li>\"The balls fall under simulated gravity (acceleration of 0.5 pixels per frame squared) and bounce off the bottom edge with 80% energy retention\"</li> </ol> Show Answer <p>The correct answer is C. \"Show some balls bouncing around with gravity\" is vague because it doesn't specify how many balls, what sizes, what colors, how strong the gravity is, or how bouncing works. The other options provide specific, measurable values that an AI or developer could implement without asking questions.</p> <p>Concept Tested: Visual Description</p> <p>See: Chapter Content</p>"},{"location":"chapters/05-writing-microsim-specifications/quiz/#3-every-interaction-specification-should-include-which-three-parts","title":"3. Every interaction specification should include which three parts?","text":"<ol> <li>Color, size, and position</li> <li>Trigger (user action), Response (system behavior), and Feedback (user perception)</li> <li>Input, output, and storage</li> <li>Start, middle, and end</li> </ol> Show Answer <p>The correct answer is B. Every interaction has three parts: Trigger (what action the user takes), Response (what the system does in reaction), and Feedback (how the user knows something happened). A complete interaction specification describes all three parts precisely, like \"The Start Button (trigger) changes from green to red (response) when the user clicks it, and the simulation begins playing (feedback).\"</p> <p>Concept Tested: Interaction Behavior</p> <p>See: Chapter Content</p>"},{"location":"chapters/05-writing-microsim-specifications/quiz/#4-what-are-behavior-constraints-in-a-microsim-specification","title":"4. What are behavior constraints in a MicroSim specification?","text":"<ol> <li>The maximum file size of the MicroSim</li> <li>Rules defining what can't happen\u2014the guardrails preventing unexpected behavior</li> <li>The number of users who can view the MicroSim simultaneously</li> <li>The programming languages that can be used</li> </ol> Show Answer <p>The correct answer is B. Behavior constraints define what can't happen in your MicroSim\u2014they're the guardrails that prevent unexpected behavior and ensure the simulation stays educationally valid. Examples include: values can't be negative, objects can't overlap, sliders can't exceed certain ranges, and mutually exclusive options can't both be selected.</p> <p>Concept Tested: Behavior Constraints</p> <p>See: Chapter Content</p>"},{"location":"chapters/05-writing-microsim-specifications/quiz/#5-what-does-the-smart-framework-stand-for-in-the-context-of-success-criteria","title":"5. What does the SMART framework stand for in the context of success criteria?","text":"<ol> <li>Simple, Memorable, Attractive, Responsive, Technical</li> <li>Specific, Measurable, Achievable, Relevant, Testable</li> <li>Speed, Memory, Accuracy, Reliability, Testing</li> <li>Start, Middle, Action, Result, Timeline</li> </ol> Show Answer <p>The correct answer is B. SMART success criteria are: Specific (exactly what should happen), Measurable (can be objectively verified), Achievable (technically possible), Relevant (connected to the learning objective), and Testable (can be checked through observation). This transforms vague goals like \"animation should be smooth\" into testable criteria like \"animation runs at 60 fps with no visible stuttering.\"</p> <p>Concept Tested: Success Criteria</p> <p>See: Chapter Content</p>"},{"location":"chapters/05-writing-microsim-specifications/quiz/#6-which-of-the-following-is-an-example-of-a-properly-specified-edge-case","title":"6. Which of the following is an example of a properly specified edge case?","text":"<ol> <li>\"Handle errors gracefully\"</li> <li>\"The system should work properly with unusual inputs\"</li> <li>\"When user adjusts denominator to zero: slider stops at 1, tooltip appears saying 'The denominator cannot be zero'\"</li> <li>\"Edge cases should not crash the simulation\"</li> </ol> Show Answer <p>The correct answer is C. A properly specified edge case includes: the condition (denominator adjusted to zero), the trigger (dragging the slider to minimum), the expected behavior (slider stops at 1, tooltip appears), and ideally a rationale. The other options are vague statements without specific behaviors.</p> <p>Concept Tested: Edge Case Definition</p> <p>See: Chapter Content</p>"},{"location":"chapters/05-writing-microsim-specifications/quiz/#7-what-is-specification-ambiguity-and-why-is-it-problematic","title":"7. What is \"specification ambiguity\" and why is it problematic?","text":"<ol> <li>When specifications are too detailed, causing confusion</li> <li>When specifications can be interpreted in multiple valid ways, leading AI to make choices that may not match your intent</li> <li>When specifications use too many technical terms</li> <li>When specifications are written in multiple languages</li> </ol> Show Answer <p>The correct answer is B. Specification ambiguity occurs when your specification can be interpreted in multiple valid ways. It's problematic because AI tools will make some choice when faced with ambiguity\u2014just not necessarily your choice. Examples include vague adjectives (\"large,\" \"fast\"), undefined references (\"the button\"), and missing defaults.</p> <p>Concept Tested: Specification Ambiguity</p> <p>See: Chapter Content</p>"},{"location":"chapters/05-writing-microsim-specifications/quiz/#8-what-should-an-intent-statement-in-a-specification-capture","title":"8. What should an intent statement in a specification capture?","text":"<ol> <li>The programming language and libraries to use</li> <li>What concept students are learning, what misconception it addresses, what discovery they should make</li> <li>The file naming conventions</li> <li>The deployment schedule</li> </ol> Show Answer <p>The correct answer is B. An intent statement captures the pedagogical purpose: what concept students are learning, what misconception this might address, what discovery students should make, and even what feeling students should have. This ensures that the educational purpose survives the journey from idea to implementation.</p> <p>Concept Tested: Intent Preservation</p> <p>See: Chapter Content</p>"},{"location":"chapters/05-writing-microsim-specifications/quiz/#9-how-do-ai-language-models-interpret-specifications","title":"9. How do AI language models interpret specifications?","text":"<ol> <li>They understand specifications exactly like humans do</li> <li>They pattern match against training data, resolve ambiguity by picking the most likely interpretation, and fill gaps with defaults</li> <li>They ask clarifying questions before proceeding</li> <li>They ignore natural language and only process code</li> </ol> Show Answer <p>The correct answer is B. AI language models pattern match against training data (similar specifications lead to similar outputs), resolve ambiguity by picking statistically most likely interpretations, fill gaps with reasonable defaults (which may not match your expectations), and follow conventions from libraries they've seen most often. This means common patterns work better, and unusual requests need more detail.</p> <p>Concept Tested: AI Interpretation</p> <p>See: Chapter Content</p>"},{"location":"chapters/05-writing-microsim-specifications/quiz/#10-which-of-the-following-specification-strategies-helps-optimize-for-ai-comprehension","title":"10. Which of the following specification strategies helps optimize for AI comprehension?","text":"<ol> <li>Using flowing prose with creative metaphors</li> <li>Varying terminology to keep the specification interesting</li> <li>Using declarative language, front-loading important information, and structured formats like lists and tables</li> <li>Keeping specifications as brief as possible</li> </ol> Show Answer <p>The correct answer is C. To optimize for AI comprehension: use declarative language (\"The canvas IS 800 pixels wide\" not \"Make the canvas about 800 pixels\"), front-load important information, use consistent terminology, use structured formats (lists and tables are parsed more reliably than prose), and reference known patterns. These strategies reduce ambiguity and improve the accuracy of AI-generated implementations.</p> <p>Concept Tested: AI Interpretation</p> <p>See: Chapter Content</p>"},{"location":"chapters/06-adapting-audience-levels/","title":"Adapting for Audience Levels","text":""},{"location":"chapters/06-adapting-audience-levels/#summary","title":"Summary","text":"<p>This chapter explores how to adapt MicroSim designs for different audiences based on cognitive development theory. You will learn about Piaget's stages and Vygotsky's theories, then apply this knowledge to design for specific audiences: early childhood (touch targets, simple cause-effect), elementary (guided exploration, scaffolded complexity), middle school (abstract concepts, multiple variables, hypothesis testing), high school (real-world applications, data interpretation), undergraduate (theoretical foundations, mathematical relationships), graduate (research applications, parameter spaces), and corporate training (job-relevant scenarios, time-efficient design).</p>"},{"location":"chapters/06-adapting-audience-levels/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 26 concepts from the learning graph:</p> <ol> <li>Cognitive Development</li> <li>Piaget Stages</li> <li>Vygotsky Theory</li> <li>Early Childhood Design</li> <li>Elementary Design</li> <li>Middle School Design</li> <li>High School Design</li> <li>Undergraduate Design</li> <li>Graduate Design</li> <li>Corporate Training Design</li> <li>Touch Target Size</li> <li>Simple Cause-Effect</li> <li>Guided Exploration</li> <li>Scaffolded Complexity</li> <li>Reading Support</li> <li>Abstract Concepts</li> <li>Multiple Variables</li> <li>Hypothesis Testing</li> <li>Real-World Application</li> <li>Data Interpretation</li> <li>Theoretical Foundations</li> <li>Mathematical Relations</li> <li>Research Applications</li> <li>Parameter Space</li> <li>Job-Relevant Scenarios</li> <li>Time-Efficient Design</li> </ol>"},{"location":"chapters/06-adapting-audience-levels/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Prerequisite Analysis and MicroSim Fundamentals</li> <li>Chapter 3: The MicroSim Pattern Library</li> </ul>"},{"location":"chapters/06-adapting-audience-levels/#one-size-does-not-fit-all-and-thats-a-good-thing","title":"One Size Does Not Fit All (And That's a Good Thing!)","text":"<p>Here's a wild thought: the same concept\u2014whether it's fractions, gravity, or data structures\u2014can be taught to a five-year-old and a PhD candidate. The magic isn't in dumbing down or smartening up; it's in adapting the experience to meet learners where they are. And when you master this skill, you become an instructional design superhero capable of making knowledge accessible to everyone on the planet.</p> <p>No pressure, right? \ud83e\uddb8\u200d\u2640\ufe0f</p> <p>But seriously, this is where the democratization of education gets real. MicroSims have been successfully deployed in books ranging from \"Reading for Kindergarten\" (where colorful celebration animations make learning letters feel like a party) all the way to graduate-level Signal Processing courses (where Fourier transforms dance across frequency domains). The underlying principles? Exactly the same. The implementation? Wildly different.</p> <p>Let's explore how to become fluent in this translation process.</p>"},{"location":"chapters/06-adapting-audience-levels/#the-science-behind-audience-adaptation","title":"The Science Behind Audience Adaptation","text":"<p>Before we dive into specific age groups, we need to understand why different audiences need different designs. This isn't about making things \"easier\" or \"harder\"\u2014it's about aligning with how the human brain develops and learns at different stages.</p>"},{"location":"chapters/06-adapting-audience-levels/#cognitive-development-the-foundation","title":"Cognitive Development: The Foundation","text":"<p>Cognitive development refers to the progression of mental processes including thinking, reasoning, problem-solving, and understanding. This development isn't random\u2014it follows predictable patterns that researchers have mapped extensively over the past century.</p> <p>Understanding these patterns gives you superpowers as an instructional designer. Instead of guessing what will work, you can make principled decisions based on decades of research. Think of it as having a roadmap for the human mind.</p>"},{"location":"chapters/06-adapting-audience-levels/#diagram-cognitive-development-progression","title":"Diagram: Cognitive Development Progression","text":"Cognitive Development Progression <p>Type: diagram</p> <p>Purpose: Visualize how cognitive capabilities develop from childhood through adulthood</p> <p>Bloom Taxonomy: Understand</p> <p>Learning Objective: Students will be able to describe how cognitive capabilities progress through developmental stages</p> <p>Components to show: - Vertical axis representing age (0-25+ years) - Horizontal axis representing cognitive capability types - Curved progression lines for different capabilities:   - Concrete thinking (plateaus around age 12)   - Abstract reasoning (begins around age 11, continues developing)   - Metacognition (emerges around age 8, matures in adulthood)   - Working memory capacity (gradual increase, peaks in 20s)</p> <p>Visual style: Area chart with overlapping colored regions</p> <p>Color scheme: - Light blue for concrete thinking - Purple for abstract reasoning - Orange for metacognition - Green for working memory</p> <p>Annotations: - Key developmental milestones marked on timeline - \"Zone of Proximal Development\" highlighted as a band around each curve</p> <p>Implementation: Chart.js area chart with annotations</p>"},{"location":"chapters/06-adapting-audience-levels/#piagets-stages-a-classic-framework","title":"Piaget's Stages: A Classic Framework","text":"<p>Jean Piaget's theory of cognitive development remains one of the most influential frameworks in education. While modern research has refined some of his claims, the core insights are golden for MicroSim design.</p> <p>Piaget identified four main developmental stages, each characterized by distinct ways of thinking:</p> Stage Age Range Key Characteristics MicroSim Implications Sensorimotor 0-2 years Learning through senses and actions Beyond our typical scope Preoperational 2-7 years Symbolic thinking, egocentric view Simple cause-effect, bright visuals Concrete Operational 7-11 years Logical thinking about concrete events Guided exploration, multiple representations Formal Operational 11+ years Abstract and hypothetical reasoning Variables, hypothesis testing, theory <p>Piaget's Gift to MicroSim Design</p> <p>Piaget showed us that children aren't just \"small adults\" with less knowledge\u2014they literally think differently. A seven-year-old struggles with hypothetical scenarios not because they're not smart, but because their brain hasn't developed that capability yet. Design accordingly!</p>"},{"location":"chapters/06-adapting-audience-levels/#vygotskys-zone-of-proximal-development","title":"Vygotsky's Zone of Proximal Development","text":"<p>While Piaget focused on what learners can do alone at each stage, Lev Vygotsky asked a more interesting question: What can learners do with help?</p> <p>Vygotsky's theory centers on the Zone of Proximal Development (ZPD)\u2014the sweet spot between what a learner can do independently and what they can't do even with assistance. This is where the magic happens.</p> <p>For MicroSim designers, Vygotsky's insights translate directly into scaffolding strategies:</p> <ul> <li>Current ability: What the learner can do without help</li> <li>ZPD: What the learner can do with guidance, hints, or scaffolding</li> <li>Beyond reach: What's too advanced even with maximum support</li> </ul>"},{"location":"chapters/06-adapting-audience-levels/#diagram-zone-of-proximal-development","title":"Diagram: Zone of Proximal Development","text":"Zone of Proximal Development Visualization <p>Type: infographic</p> <p>Purpose: Show the relationship between independent capability, scaffolded learning, and unreachable content</p> <p>Bloom Taxonomy: Understand</p> <p>Learning Objective: Students will be able to identify the Zone of Proximal Development and understand its implications for scaffolding</p> <p>Layout: Three concentric circles (target/bullseye style)</p> <p>Circles: - Inner circle (green): \"Can Do Independently\" - fully solid - Middle ring (yellow): \"Zone of Proximal Development\" - gradient from solid to hatched - Outer ring (red): \"Cannot Do Yet\" - hatched pattern</p> <p>Interactive elements: - Hover over each zone to see examples - Slider to adjust \"scaffolding level\" which visually expands the middle zone - Click to see how different MicroSim features map to each zone</p> <p>Annotations: - Arrow pointing to middle zone: \"This is where learning happens!\" - Example tasks floating in each zone - Quote from Vygotsky at bottom</p> <p>Visual style: Clean, modern concentric circles with gradient transitions</p> <p>Color scheme: Green \u2192 Yellow \u2192 Red (traffic light metaphor)</p> <p>Implementation: p5.js with hover interactions and slider control</p> <p>The beautiful thing about MicroSims is that they're inherently scaffolding tools. A well-designed simulation provides exactly the kind of support Vygotsky envisioned\u2014guidance that helps learners reach beyond their current abilities without overwhelming them.</p>"},{"location":"chapters/06-adapting-audience-levels/#from-theory-to-practice-seven-audience-levels","title":"From Theory to Practice: Seven Audience Levels","text":"<p>Now let's get practical. We'll examine seven distinct audience levels, each with specific design considerations. Remember: these aren't rigid categories but rather points along a continuum. Your actual learners will vary, so use these as starting points for your designs.</p>"},{"location":"chapters/06-adapting-audience-levels/#early-childhood-design-ages-3-6","title":"Early Childhood Design (Ages 3-6)","text":"<p>Welcome to the land of wonder, where everything is new and exciting! Designing for early childhood means embracing simplicity, joy, and lots of positive reinforcement.</p> <p>The \"Reading for Kindergarten\" project (dmccreary.github.io/reading-for-kindergarten) provides an excellent model for this age group. Its MicroSims include:</p> <ul> <li>Talking Letters: Click letters to hear pronunciation with speech synthesis</li> <li>Letter Matching Game: Match uppercase to lowercase with drag-and-drop</li> <li>Letter Hunt: Find and click target letters in a search-and-find game</li> <li>First Sound Finder: Listen to a word and identify the first sound</li> </ul> <p>Notice a pattern? These are all about simple cause-effect relationships. Click \u2192 something happens. Drag \u2192 something moves. The cognitive load is minimal, the feedback is immediate, and the celebrations are spectacular.</p>"},{"location":"chapters/06-adapting-audience-levels/#touch-target-size-bigger-is-better","title":"Touch Target Size: Bigger Is Better","text":"<p>When designing for young children (and honestly, for mobile users of any age), touch target size matters enormously. Small fingers have limited precision, and frustration from missed taps can derail an entire learning session.</p> <p>Recommended minimum touch targets by age:</p> Age Group Minimum Target Size Recommended Size 3-4 years 48x48 pixels 64x64 pixels 5-6 years 44x44 pixels 56x56 pixels 7-8 years 40x40 pixels 48x48 pixels Adults 44x44 pixels 48x48 pixels <p>The 48-Pixel Rule</p> <p>Apple and Google both recommend 44-48 pixel minimum touch targets for adult users. For young children, go bigger! A frustrated child is a child who stops learning.</p>"},{"location":"chapters/06-adapting-audience-levels/#simple-cause-effect-the-magic-of-immediate-feedback","title":"Simple Cause-Effect: The Magic of Immediate Feedback","text":"<p>Young children are building their mental models of how the world works. Every interaction is a tiny experiment: \"What happens if I do this?\"</p> <p>Simple cause-effect relationships in MicroSims should:</p> <ul> <li>Have clear, visible connections between action and outcome</li> <li>Provide immediate feedback (within 100-200ms)</li> <li>Use consistent patterns (same action always produces same result)</li> <li>Include celebratory animations for correct responses</li> </ul> <p>Those colorful celebration animations in the Reading for Kindergarten project? They're not just fun\u2014they're essential. Positive reinforcement through sparkles, stars, and cheerful sounds creates dopamine responses that reinforce learning. It's neuroscience in action!</p>"},{"location":"chapters/06-adapting-audience-levels/#diagram-early-childhood-microsim-pattern","title":"Diagram: Early Childhood MicroSim Pattern","text":"Early Childhood MicroSim Pattern <p>Type: diagram</p> <p>Purpose: Show the key components of an effective early childhood MicroSim</p> <p>Bloom Taxonomy: Remember</p> <p>Learning Objective: Students will be able to identify the essential components of early childhood MicroSim design</p> <p>Components to show: - Large, colorful interactive elements (letters, shapes, characters) - Oversized touch targets with visual affordances - Celebration animation area - Simple instruction area (with audio option) - Progress indicator (stars, stickers)</p> <p>Layout: Single-screen view with labeled regions</p> <p>Visual style: Mockup/wireframe style showing component placement</p> <p>Color scheme: Bright, primary colors with high contrast</p> <p>Labels: - \"Touch Target: 64px minimum\" - \"Immediate Audio Feedback\" - \"Celebration Animation Zone\" - \"Simple Visual Instructions\" - \"Progress Rewards\"</p> <p>Implementation: Static diagram with callout annotations</p>"},{"location":"chapters/06-adapting-audience-levels/#elementary-design-ages-7-11","title":"Elementary Design (Ages 7-11)","text":"<p>Elementary students are entering Piaget's concrete operational stage. They can think logically about concrete situations but still struggle with purely abstract concepts. This opens up exciting possibilities!</p>"},{"location":"chapters/06-adapting-audience-levels/#guided-exploration-structured-freedom","title":"Guided Exploration: Structured Freedom","text":"<p>Guided exploration is the sweet spot for this age group. Students want to explore and experiment, but they need guardrails to prevent frustration and ensure learning happens.</p> <p>Effective guided exploration includes:</p> <ul> <li>Clear goals that students understand</li> <li>Limited option spaces (don't overwhelm with choices)</li> <li>Hints or suggestions when students get stuck</li> <li>Multiple valid paths to success</li> <li>Celebration of both correct answers and good attempts</li> </ul> <p>Think of it like a museum with a suggested tour route but permission to wander. The path guides without constraining.</p>"},{"location":"chapters/06-adapting-audience-levels/#scaffolded-complexity-leveling-up","title":"Scaffolded Complexity: Leveling Up","text":"<p>Scaffolded complexity means gradually introducing more sophisticated elements as learners demonstrate mastery. This is video game design 101, and it works brilliantly in educational contexts.</p> <p>A well-scaffolded MicroSim might:</p> <ol> <li>Start with a single variable to manipulate</li> <li>Add a second variable after initial success</li> <li>Introduce interactions between variables</li> <li>Eventually reveal the full complexity of the system</li> </ol> <p>Each level builds on previous understanding, and learners feel accomplished rather than overwhelmed.</p>"},{"location":"chapters/06-adapting-audience-levels/#reading-support-meeting-literacy-where-it-is","title":"Reading Support: Meeting Literacy Where It Is","text":"<p>Elementary students have wildly varying reading abilities. A third-grader might read at a first-grade or sixth-grade level. Reading support strategies include:</p> <ul> <li>Audio narration for all instructions</li> <li>Simple vocabulary with complex words defined in context</li> <li>Short sentences (12-18 words maximum)</li> <li>Visual icons accompanying text</li> <li>Optional text-to-speech for any on-screen text</li> </ul> <p>Never let reading ability become a barrier to learning content! If a student understands fractions but struggles with reading, the fraction MicroSim shouldn't punish them.</p>"},{"location":"chapters/06-adapting-audience-levels/#diagram-elementary-scaffolding-progression","title":"Diagram: Elementary Scaffolding Progression","text":"Elementary Scaffolding Progression <p>Type: microsim</p> <p>Purpose: Demonstrate scaffolded complexity through an interactive example</p> <p>Bloom Taxonomy: Apply</p> <p>Learning Objective: Students will be able to design scaffolded learning sequences that progressively introduce complexity</p> <p>Canvas layout (800x500px): - Main simulation area (600x500): Shows a simple physics simulation - Control panel (200x500): Scaffolding controls</p> <p>Visual elements: - Ball that falls with gravity - Ground surface - Variable controls that unlock progressively - Level indicator (1-5)</p> <p>Interactive controls: - Level 1: Just drop the ball (single button) - Level 2: Adjust drop height (slider unlocks) - Level 3: Adjust ball mass (second slider unlocks) - Level 4: Toggle air resistance (checkbox unlocks) - Level 5: Multiple balls simultaneously (unlock multiple instances)</p> <p>Default parameters: - Level: 1 - Height: 100 pixels - Mass: 1 unit - Air resistance: off</p> <p>Behavior: - Completing each level (dropping and observing 3 times) unlocks next level - Locked controls are visible but grayed out with \"Coming soon!\" tooltip - Celebration animation when each level unlocks - Running score of successful experiments</p> <p>Implementation notes: - Use p5.js for physics simulation - Simple gravity model: y_velocity += gravity * deltaTime - Air resistance model: drag = 0.5 * density * velocity^2 * area * drag_coefficient - Store progress in localStorage for persistence</p> <p>Responsive design: - Canvas width adjusts to container - Controls stack vertically on narrow screens</p>"},{"location":"chapters/06-adapting-audience-levels/#middle-school-design-ages-12-14","title":"Middle School Design (Ages 12-14)","text":"<p>Welcome to the formal operational stage! Middle schoolers are developing the capacity for abstract thinking, and this transforms what's possible in MicroSim design.</p>"},{"location":"chapters/06-adapting-audience-levels/#abstract-concepts-beyond-the-concrete","title":"Abstract Concepts: Beyond the Concrete","text":"<p>For the first time, learners can manipulate abstract concepts without needing physical referents. They can think about \"fairness\" without a specific unfair situation, or \"velocity\" without a specific moving object.</p> <p>This doesn't mean abandoning visuals\u2014it means visuals can represent abstract relationships. A middle school MicroSim might show:</p> <ul> <li>Variables as movable points on axes</li> <li>Relationships as connecting lines or curves</li> <li>Categories as color-coded regions</li> <li>Hypothetical scenarios as \"what if\" explorations</li> </ul> <p>The key is providing concrete representations of abstract ideas, not replacing abstraction with concreteness.</p>"},{"location":"chapters/06-adapting-audience-levels/#multiple-variables-juggling-complexity","title":"Multiple Variables: Juggling Complexity","text":"<p>Middle schoolers can handle multiple variables interacting simultaneously. This is huge for scientific thinking! A simulation might include:</p> <ul> <li>Independent variables they control (temperature, concentration)</li> <li>Dependent variables that respond (reaction rate, color change)</li> <li>Controlled variables they set once (container size, catalyst presence)</li> </ul> <p>The cognitive load increases, but so does the learning potential. Students begin understanding systems rather than isolated facts.</p>"},{"location":"chapters/06-adapting-audience-levels/#hypothesis-testing-the-scientific-method-in-action","title":"Hypothesis Testing: The Scientific Method in Action","text":"<p>Hypothesis testing becomes possible at this stage. Students can:</p> <ol> <li>Observe a phenomenon</li> <li>Form a prediction about what will happen under different conditions</li> <li>Test that prediction systematically</li> <li>Revise their understanding based on results</li> </ol> <p>MicroSims are perfect for this because they allow safe, rapid experimentation. A student might test 50 hypotheses in a simulation that would take weeks in a physical lab.</p>"},{"location":"chapters/06-adapting-audience-levels/#diagram-hypothesis-testing-interface","title":"Diagram: Hypothesis Testing Interface","text":"Hypothesis Testing Interface Pattern <p>Type: microsim</p> <p>Purpose: Demonstrate a MicroSim pattern that supports the scientific method</p> <p>Bloom Taxonomy: Analyze</p> <p>Learning Objective: Students will be able to design MicroSims that support hypothesis formation and testing</p> <p>Canvas layout (900x600px): - Simulation area (500x600): Visual representation of system - Hypothesis panel (200x300): Where students record predictions - Results panel (200x300): Where outcomes are displayed</p> <p>Visual elements: - System visualization (example: pendulum) - Variable controls with labeled axes - Hypothesis input area with structured prompts - Results graph showing actual vs. predicted</p> <p>Interactive controls: - \"I predict that...\" dropdown/fill-in - Variable adjustment sliders - \"Test my hypothesis\" button - \"New hypothesis\" button - \"View my experiment history\" toggle</p> <p>Hypothesis workflow: 1. Student observes initial state 2. Student records prediction using structured format 3. Student sets variables 4. Student runs simulation 5. Results displayed alongside prediction 6. Student marks prediction as confirmed/rejected 7. Optional: Student writes reflection</p> <p>Default parameters: - System: simple pendulum - Variable 1: length (10-100cm) - Variable 2: starting angle (5-45 degrees) - Variable 3: mass (10-500g)</p> <p>Behavior: - Cannot run simulation until hypothesis is recorded - Results persist across sessions - Incorrect predictions are celebrated as \"good science!\" - Patterns in experiment history are highlighted</p> <p>Implementation: p5.js with form elements and localStorage</p>"},{"location":"chapters/06-adapting-audience-levels/#high-school-design-ages-15-18","title":"High School Design (Ages 15-18)","text":"<p>High schoolers have (mostly) mature cognitive capabilities. The design challenge shifts from cognitive development to motivation, relevance, and depth.</p>"},{"location":"chapters/06-adapting-audience-levels/#real-world-application-making-it-matter","title":"Real-World Application: Making It Matter","text":"<p>Real-world application is crucial for high school learners who constantly ask \"When will I ever use this?\" Your MicroSims need compelling answers.</p> <p>Effective real-world connections:</p> <ul> <li>Use actual data from real phenomena</li> <li>Reference current events and technologies</li> <li>Show career applications</li> <li>Connect to students' interests and experiences</li> <li>Demonstrate practical problem-solving</li> </ul> <p>A physics MicroSim shouldn't just show abstract forces\u2014it should simulate car crashes, roller coasters, or smartphone accelerometers. Context drives engagement.</p>"},{"location":"chapters/06-adapting-audience-levels/#data-interpretation-reading-the-story-in-numbers","title":"Data Interpretation: Reading the Story in Numbers","text":"<p>Data interpretation skills become central at this level. High schoolers should be able to:</p> <ul> <li>Read graphs and extract meaning</li> <li>Identify trends and anomalies</li> <li>Distinguish correlation from causation</li> <li>Recognize limitations in data</li> <li>Make predictions based on patterns</li> </ul> <p>MicroSims that generate data for analysis are incredibly powerful here. Students experience being scientists rather than just learning about science.</p>"},{"location":"chapters/06-adapting-audience-levels/#diagram-real-world-data-microsim","title":"Diagram: Real-World Data MicroSim","text":"Real-World Data Integration Pattern <p>Type: microsim</p> <p>Purpose: Show how to integrate real-world data into high school MicroSims</p> <p>Bloom Taxonomy: Evaluate</p> <p>Learning Objective: Students will be able to design MicroSims that incorporate authentic data and support data interpretation skills</p> <p>Canvas layout (1000x600px): - Data visualization area (700x400): Main chart/graph - Real data overlay (same area): Toggleable actual data - Control panel (300x600): Data selection and parameters - Interpretation prompts (700x200): Guided analysis questions</p> <p>Visual elements: - Simulation-generated data (line graph) - Real-world comparison data (overlay points) - Residual/error visualization - Trend line with equation</p> <p>Interactive controls: - Model parameter sliders - \"Show real data\" toggle - Time range selector - \"Add noise\" slider (for realism) - Data source selector (multiple real datasets) - Export data button (CSV)</p> <p>Example implementation: Climate modeling - Simulate temperature based on CO2 levels - Compare with actual temperature records - Adjust model parameters to improve fit - Analyze where model succeeds and fails</p> <p>Guided interpretation prompts: - \"What pattern do you observe in the real data?\" - \"How well does your model match reality?\" - \"What factors might explain the differences?\" - \"What does this suggest about future trends?\"</p> <p>Data sources: - Link to actual data repositories - Citation of sources - Discussion of data collection methods</p> <p>Implementation: Chart.js or Plotly with CSV data import</p>"},{"location":"chapters/06-adapting-audience-levels/#undergraduate-design-ages-18-22","title":"Undergraduate Design (Ages 18-22)","text":"<p>College students can handle sophisticated material and are (hopefully) developing intrinsic motivation for learning. The design focus shifts toward theoretical depth and professional preparation.</p>"},{"location":"chapters/06-adapting-audience-levels/#theoretical-foundations-understanding-the-why","title":"Theoretical Foundations: Understanding the \"Why\"","text":"<p>Theoretical foundations matter at this level. Undergraduates shouldn't just know that something works\u2014they should understand why it works.</p> <p>MicroSims for undergraduates might:</p> <ul> <li>Derive equations before simulating them</li> <li>Show assumptions underlying models</li> <li>Explore edge cases where theories break down</li> <li>Compare competing theoretical frameworks</li> <li>Connect empirical observations to theoretical predictions</li> </ul> <p>The goal is building robust mental models that transfer to new situations, not just memorizing procedures.</p>"},{"location":"chapters/06-adapting-audience-levels/#mathematical-relations-equations-in-action","title":"Mathematical Relations: Equations in Action","text":"<p>Mathematical relations can be central rather than hidden. Undergraduates are ready to see the math and understand how it governs simulation behavior.</p> <p>Consider showing:</p> <ul> <li>Differential equations alongside their visual solutions</li> <li>Parameter effects on mathematical expressions</li> <li>Numerical methods and their accuracy</li> <li>Mathematical derivations as interactive explorations</li> <li>Error analysis and uncertainty propagation</li> </ul> <p>The Signal Processing course (github.com/dmccreary/signal-processing) exemplifies this approach with MicroSims including:</p> <ul> <li>Basic FFT: Time domain and frequency domain side-by-side</li> <li>FFT Butterfly: Visualization of the FFT algorithm's computational breakdown</li> <li>Euler's Formula Explorer: Unit circle rotation with synchronized wave trace</li> <li>Complex Plane: Interactive visualization of complex numbers</li> <li>Convolution: Demonstrating convolution as overlap between functions</li> </ul> <p>These simulations don't hide the mathematics\u2014they illuminate it through interaction.</p>"},{"location":"chapters/06-adapting-audience-levels/#diagram-mathematical-relationship-explorer","title":"Diagram: Mathematical Relationship Explorer","text":"Mathematical Relationship Explorer Pattern <p>Type: microsim</p> <p>Purpose: Demonstrate how to make mathematical relationships interactive and visual</p> <p>Bloom Taxonomy: Analyze</p> <p>Learning Objective: Students will be able to design MicroSims that reveal mathematical relationships through parameter manipulation</p> <p>Canvas layout (1000x700px): - Equation display area (1000x100): Shows current equation with live parameters - Visualization area (700x500): Graph of function - Parameter controls (300x500): Sliders for each parameter - Derivative/integral panel (1000x100): Related mathematical expressions</p> <p>Visual elements: - Function plot with smooth curve - Tangent line at movable point (for derivatives) - Shaded area under curve (for integrals) - Critical points marked and labeled - Asymptotes shown as dashed lines</p> <p>Interactive controls: - Parameter sliders with real-time equation update - Draggable point for tangent line position - Integration bounds sliders - Checkbox to show/hide mathematical annotations - \"Random parameters\" button for exploration</p> <p>Example: Logistic function exploration - Equation: $f(x) = \\frac{L}{1 + e^{-k(x-x_0)}}$ - L: carrying capacity (affects height) - k: steepness (affects slope at midpoint) - x\u2080: midpoint (horizontal shift) - Show derivative: $f'(x) = k \\cdot f(x) \\cdot (1 - \\frac{f(x)}{L})$</p> <p>Mathematical annotations: - LaTeX-rendered equations update with parameter values - Calculated values (max, min, inflection points) displayed - Rate of change shown numerically and visually</p> <p>Behavior: - Smooth animation between parameter changes - Highlight regions of interest based on parameters - Show warning when parameters create undefined behavior - Link between symbolic and graphical representations</p> <p>Implementation: p5.js with MathJax or KaTeX for equation rendering</p>"},{"location":"chapters/06-adapting-audience-levels/#graduate-design-ages-22","title":"Graduate Design (Ages 22+)","text":"<p>Graduate students are becoming experts. They need tools for research, exploration of edge cases, and understanding of limitations.</p>"},{"location":"chapters/06-adapting-audience-levels/#research-applications-tools-for-discovery","title":"Research Applications: Tools for Discovery","text":"<p>Research applications transform MicroSims from learning tools into research tools. Graduate-level simulations might:</p> <ul> <li>Allow parameter ranges beyond \"safe\" educational bounds</li> <li>Support export of data for further analysis</li> <li>Include literature references and theoretical context</li> <li>Enable comparison between models</li> <li>Facilitate hypothesis generation for original research</li> </ul> <p>The line between learning and discovery blurs at this level. A well-designed MicroSim might actually help a graduate student generate publishable insights.</p>"},{"location":"chapters/06-adapting-audience-levels/#parameter-space-exploring-the-infinite","title":"Parameter Space: Exploring the Infinite","text":"<p>Parameter space exploration becomes central. Graduate students need to understand:</p> <ul> <li>How systems behave across the full range of possible parameters</li> <li>Where phase transitions and bifurcations occur</li> <li>Which parameters are most sensitive</li> <li>How parameters interact in complex ways</li> <li>Where models break down or become unrealistic</li> </ul> <p>MicroSims that map parameter spaces\u2014showing stability regions, chaotic zones, and interesting boundaries\u2014are incredibly valuable for developing expert intuition.</p>"},{"location":"chapters/06-adapting-audience-levels/#diagram-parameter-space-explorer","title":"Diagram: Parameter Space Explorer","text":"Parameter Space Explorer Pattern <p>Type: microsim</p> <p>Purpose: Enable systematic exploration of parameter spaces for graduate-level understanding</p> <p>Bloom Taxonomy: Create</p> <p>Learning Objective: Students will be able to design MicroSims that support parameter space exploration and research-level investigation</p> <p>Canvas layout (1200x800px): - 2D parameter space map (600x600): Heat map or contour plot - System visualization (400x400): Current state at selected parameters - Parameter traces (200x400): History of exploration - Analysis panel (400x200): Quantitative metrics</p> <p>Visual elements: - 2D heat map colored by system behavior metric - Crosshairs showing current parameter location - Trail of previously explored points - Stability boundaries marked - Bifurcation points highlighted - Basins of attraction colored differently</p> <p>Interactive controls: - Click anywhere on parameter space to set values - Drag to create parameter sweep - Dropdown for behavior metric (Lyapunov exponent, period, stability, etc.) - Resolution slider for parameter space calculation - \"Scan region\" button for systematic exploration - Export button for data and images</p> <p>Example: Lorenz system exploration - Parameters: \u03c3 (sigma), \u03c1 (rho), \u03b2 (beta) - Fixed one parameter, vary other two - Color by: chaotic vs. periodic behavior - Show attractors in 3D projection</p> <p>Advanced features: - Automatic detection of interesting boundaries - Hover to preview behavior at any point - Save \"interesting points\" for later study - Compare multiple systems side-by-side - Parameter sensitivity analysis</p> <p>Research support: - Export publication-quality figures - Include parameter values in exports - Log all exploration for reproducibility - Link to relevant literature</p> <p>Implementation: p5.js with WebGL for complex visualizations</p>"},{"location":"chapters/06-adapting-audience-levels/#corporate-training-design","title":"Corporate Training Design","text":"<p>Corporate learners are a unique audience. They're adults with limited time, specific job responsibilities, and immediate application needs.</p>"},{"location":"chapters/06-adapting-audience-levels/#job-relevant-scenarios-instant-applicability","title":"Job-Relevant Scenarios: Instant Applicability","text":"<p>Job-relevant scenarios are non-negotiable for corporate training. Every minute spent learning must connect to job performance.</p> <p>Effective corporate MicroSims:</p> <ul> <li>Use scenarios from the actual work environment</li> <li>Feature job-specific vocabulary and tools</li> <li>Show consequences in business terms (revenue, efficiency, risk)</li> <li>Include decision-making practice with realistic constraints</li> <li>Provide immediate applicability (\"Use this Monday morning\")</li> </ul> <p>Abstract concepts need concrete workplace translations. A simulation about decision trees isn't about decision trees\u2014it's about which customers to prioritize.</p>"},{"location":"chapters/06-adapting-audience-levels/#time-efficient-design-respect-the-clock","title":"Time-Efficient Design: Respect the Clock","text":"<p>Time-efficient design respects that corporate learners have meetings in 10 minutes. Effective strategies:</p> <ul> <li>Chunk learning into 5-15 minute modules</li> <li>Provide clear objectives upfront</li> <li>Include \"quick reference\" modes for review</li> <li>Support mobile and interrupted learning</li> <li>Offer pre-tests to skip known material</li> <li>Show progress and estimated completion time</li> </ul> <p>The best corporate MicroSim is one that teaches a valuable skill during a coffee break.</p>"},{"location":"chapters/06-adapting-audience-levels/#diagram-corporate-learning-module-pattern","title":"Diagram: Corporate Learning Module Pattern","text":"Corporate Learning Module Pattern <p>Type: workflow</p> <p>Purpose: Show the structure of an effective corporate training MicroSim</p> <p>Bloom Taxonomy: Apply</p> <p>Learning Objective: Students will be able to design time-efficient corporate training MicroSims</p> <p>Visual style: Flowchart with timing annotations</p> <p>Steps: 1. Start: \"Module Launch\"    Hover text: \"Module loads immediately - no waiting\"    Time: 0 seconds</p> <ol> <li> <p>Process: \"Pre-Assessment Check\"    Hover text: \"3-question check if learner already knows material\"    Time: 30 seconds    Branch: If passed \u2192 Skip to Practice</p> </li> <li> <p>Process: \"Concept Introduction\"    Hover text: \"Brief explanation with workplace context\"    Time: 2 minutes</p> </li> <li> <p>Process: \"Interactive Demonstration\"    Hover text: \"MicroSim showing concept in action\"    Time: 3 minutes</p> </li> <li> <p>Process: \"Guided Practice\"    Hover text: \"Learner tries with hints available\"    Time: 3 minutes</p> </li> <li> <p>Process: \"Independent Practice\"    Hover text: \"Realistic scenario without scaffolding\"    Time: 3 minutes</p> </li> <li> <p>Process: \"Job Aid Delivery\"    Hover text: \"Downloadable reference for actual work use\"    Time: 30 seconds</p> </li> <li> <p>End: \"Module Complete\"    Hover text: \"Total time: ~12 minutes\"</p> </li> </ol> <p>Swimlanes: - Learner Actions - System Responses - Progress Tracking</p> <p>Color coding: - Blue: Assessment activities - Green: Learning activities - Yellow: Practice activities - Orange: Resource delivery</p> <p>Annotations: - Total module time: 12-15 minutes - \"Skip available\" markers on optional sections - Mobile-friendly indicators</p> <p>Implementation: Mermaid flowchart or custom SVG</p>"},{"location":"chapters/06-adapting-audience-levels/#putting-it-all-together-the-audience-adaptation-framework","title":"Putting It All Together: The Audience Adaptation Framework","text":"<p>Now that we've explored each audience level, let's synthesize this into a practical framework you can apply to any MicroSim design.</p>"},{"location":"chapters/06-adapting-audience-levels/#the-adaptation-checklist","title":"The Adaptation Checklist","text":"<p>When adapting a MicroSim concept for a specific audience, work through these questions:</p> <ol> <li>Cognitive stage: Where is this audience on the Piaget/Vygotsky continuum?</li> <li>Abstraction level: Can they handle pure abstraction, or do they need concrete representations?</li> <li>Variable capacity: How many interacting elements can they juggle?</li> <li>Scaffolding needs: What support do they need to operate in their ZPD?</li> <li>Reading/literacy: What text complexity is appropriate?</li> <li>Motivation drivers: What makes this audience care?</li> <li>Time constraints: How long will they engage?</li> <li>Application context: Where will they use this knowledge?</li> </ol>"},{"location":"chapters/06-adapting-audience-levels/#comparison-table-design-elements-across-levels","title":"Comparison Table: Design Elements Across Levels","text":"Design Element Early Childhood Elementary Middle School High School Undergraduate Graduate Corporate Touch targets 64px+ 48px+ 44px 44px 44px 44px 44px+ Variables 1 2-3 3-5 5+ Unlimited Unlimited Job-relevant Text level Minimal Simple Moderate Advanced Academic Expert Professional Audio support Essential Important Optional Optional Rare Rare Optional Celebration Extensive Moderate Light Light None None Achievement Real-world Fantasy OK Some Important Essential Professional Research Required Scaffolding Heavy Moderate Light Minimal On-demand None Efficient Time per session 2-5 min 5-15 min 10-20 min 15-30 min 30-60 min Variable 5-15 min"},{"location":"chapters/06-adapting-audience-levels/#the-same-concept-at-seven-levels","title":"The Same Concept at Seven Levels","text":"<p>Let's see how a single concept\u2014probability\u2014might be designed differently for each audience:</p>"},{"location":"chapters/06-adapting-audience-levels/#diagram-probability-across-audience-levels","title":"Diagram: Probability Across Audience Levels","text":"Probability Concept Adaptation <p>Type: infographic</p> <p>Purpose: Demonstrate how the same concept adapts across all seven audience levels</p> <p>Bloom Taxonomy: Analyze</p> <p>Learning Objective: Students will be able to design audience-appropriate versions of a single concept</p> <p>Layout: Horizontal comparison with seven columns, each showing a MicroSim sketch</p> <p>Columns:</p> <ol> <li>Early Childhood</li> <li>Title: \"Lucky Duck Pond\"</li> <li>Visual: Colorful ducks on a pond</li> <li>Interaction: Tap to pick a duck, see what's underneath</li> <li>Outcome: Celebration when correct color found</li> <li> <p>No numbers or fractions</p> </li> <li> <p>Elementary</p> </li> <li>Title: \"Marble Jar Predictor\"</li> <li>Visual: Jar with colored marbles</li> <li>Interaction: Guess most common color, then draw marbles</li> <li>Outcome: Track predictions vs. reality with tally marks</li> <li> <p>Simple fractions introduced</p> </li> <li> <p>Middle School</p> </li> <li>Title: \"Probability Explorer\"</li> <li>Visual: Multiple probability scenarios</li> <li>Interaction: Adjust proportions, make hypotheses</li> <li>Outcome: Compare expected vs. observed frequencies</li> <li> <p>Introduce probability notation</p> </li> <li> <p>High School</p> </li> <li>Title: \"Insurance Risk Calculator\"</li> <li>Visual: Real insurance scenario</li> <li>Interaction: Adjust risk factors, see premium changes</li> <li>Outcome: Connect to actual actuarial calculations</li> <li> <p>Full probability math</p> </li> <li> <p>Undergraduate</p> </li> <li>Title: \"Bayesian Reasoning Lab\"</li> <li>Visual: Prior, likelihood, posterior distributions</li> <li>Interaction: Adjust priors, add evidence</li> <li>Outcome: See Bayes' theorem in action</li> <li> <p>Full mathematical notation</p> </li> <li> <p>Graduate</p> </li> <li>Title: \"MCMC Sampler\"</li> <li>Visual: Parameter space with sampling chains</li> <li>Interaction: Configure sampler, run diagnostics</li> <li>Outcome: Research-ready probability tool</li> <li> <p>Advanced statistical methods</p> </li> <li> <p>Corporate</p> </li> <li>Title: \"Risk Decision Tool\"</li> <li>Visual: Business scenario with uncertainty</li> <li>Interaction: Input estimates, see decision recommendations</li> <li>Outcome: Immediate workplace application</li> <li>Focus on decision-making</li> </ol> <p>Interactive features: - Slider to morph between levels - Side-by-side comparison mode - \"What changes\" callouts highlighting differences</p> <p>Color scheme: Rainbow gradient across levels (red to violet)</p> <p>Implementation: p5.js with tabbed interface or slider control</p>"},{"location":"chapters/06-adapting-audience-levels/#case-studies-microsims-across-the-spectrum","title":"Case Studies: MicroSims Across the Spectrum","text":"<p>Let's examine two real-world examples that demonstrate excellence at opposite ends of the audience spectrum.</p>"},{"location":"chapters/06-adapting-audience-levels/#case-study-1-reading-for-kindergarten","title":"Case Study 1: Reading for Kindergarten","text":"<p>The Reading for Kindergarten project shows what early childhood MicroSim design looks like when done right.</p> <p>Key design elements observed:</p> <ul> <li>Celebration animations: Sparkle effects and star rewards create dopamine-driven positive reinforcement</li> <li>Audio-first interaction: Speech synthesis lets pre-readers engage fully</li> <li>Oversized touch targets: Large letters and buttons accommodate developing motor skills</li> <li>Immediate feedback: Every action produces instant, predictable response</li> <li>Simple cause-effect: Click letter \u2192 hear sound, no complex mechanics</li> <li>Joy-centered design: Learning should feel like play</li> </ul> <p>The project includes a dedicated \"Celebration Animation Tester\" MicroSim, demonstrating how seriously the creators take positive reinforcement. This isn't frivolous\u2014it's pedagogically sound design for the developmental stage.</p>"},{"location":"chapters/06-adapting-audience-levels/#case-study-2-signal-processing-course","title":"Case Study 2: Signal Processing Course","text":"<p>The Signal Processing course demonstrates graduate/undergraduate MicroSim design.</p> <p>Key design elements:</p> <ul> <li>Mathematical rigor: FFT algorithms, Euler's formula, complex plane operations</li> <li>Multiple representations: Time domain, frequency domain, complex plane views</li> <li>Real data integration: FFT from microphone, live audio analysis</li> <li>Professional tools: Bode plots, convolution demonstrations, modulation visualizations</li> <li>Parameter exploration: Adjustable transform sizes, waveform types, filter parameters</li> <li>No scaffolding: Assumes mathematical maturity and self-directed exploration</li> </ul> <p>The MicroSims include sophisticated concepts like the FFT Butterfly algorithm visualization and M\u00f6bius transformations\u2014content that would be meaningless to younger audiences but is precisely what advanced students need.</p>"},{"location":"chapters/06-adapting-audience-levels/#the-common-thread","title":"The Common Thread","text":"<p>Despite their vast differences, both projects share essential qualities:</p> <ol> <li>Clear learning objectives drive every design decision</li> <li>Appropriate complexity matches the audience's cognitive capabilities</li> <li>Interactive exploration replaces passive consumption</li> <li>Visual representation makes abstract concepts concrete</li> <li>Immediate feedback supports learning from experience</li> </ol> <p>The underlying pedagogical principles are identical. Only the implementation differs.</p>"},{"location":"chapters/06-adapting-audience-levels/#practical-guidelines-for-audience-adaptation","title":"Practical Guidelines for Audience Adaptation","text":""},{"location":"chapters/06-adapting-audience-levels/#start-with-the-concept-not-the-code","title":"Start with the Concept, Not the Code","text":"<p>When adapting for a new audience, begin by asking:</p> <ol> <li>What is the core insight this concept teaches?</li> <li>What misconceptions are common at this level?</li> <li>What prior knowledge can I build on?</li> <li>What metaphors or analogies resonate with this audience?</li> <li>What would success look like in their world?</li> </ol> <p>Only after answering these questions should you think about sliders, buttons, and animations.</p>"},{"location":"chapters/06-adapting-audience-levels/#prototype-at-the-extremes","title":"Prototype at the Extremes","text":"<p>A useful exercise: design the same concept for your target audience AND for audiences two levels above and below. This forces you to understand what's truly essential versus what's audience-specific scaffolding.</p>"},{"location":"chapters/06-adapting-audience-levels/#test-with-real-learners","title":"Test with Real Learners","text":"<p>No amount of theory substitutes for watching actual learners interact with your MicroSim. Observe:</p> <ul> <li>Where do they get confused?</li> <li>Where do they get bored?</li> <li>What questions do they ask?</li> <li>What do they try that doesn't work?</li> <li>When do they smile?</li> </ul> <p>These observations reveal whether your audience adaptation is working.</p>"},{"location":"chapters/06-adapting-audience-levels/#iterate-based-on-evidence","title":"Iterate Based on Evidence","text":"<p>Your first design will be wrong. That's not failure\u2014that's science! Use learner feedback to refine:</p> <ul> <li>Too hard? Add scaffolding or reduce variables</li> <li>Too easy? Remove scaffolding or add complexity</li> <li>Confusing? Clarify language or add visual cues</li> <li>Boring? Add interactivity or real-world connections</li> <li>Too long? Chunk into smaller modules</li> </ul>"},{"location":"chapters/06-adapting-audience-levels/#the-global-opportunity","title":"The Global Opportunity","text":"<p>Here's the optimistic truth that makes all this work worthwhile: MicroSims can democratize education worldwide.</p> <p>A child in a rural village can access the same high-quality physics simulations as a student at an elite preparatory school. A working professional can learn signal processing from interactive demonstrations originally designed for MIT students. A kindergartener in any country can learn to read through joyful, celebrated interactions.</p> <p>The barrier isn't intelligence or ability\u2014it's access and appropriate design. When we create MicroSims thoughtfully adapted for different audiences, we remove barriers and open doors.</p> <p>This is why audience adaptation matters. It's not just good pedagogy\u2014it's educational justice.</p>"},{"location":"chapters/06-adapting-audience-levels/#summary-key-takeaways","title":"Summary: Key Takeaways","text":"<p>Let's recap what we've learned about adapting MicroSims for different audiences:</p> <ol> <li> <p>Cognitive development theory provides the foundation\u2014Piaget's stages and Vygotsky's ZPD guide our design decisions</p> </li> <li> <p>Seven audience levels have distinct needs:</p> </li> <li>Early childhood: Simple cause-effect, large targets, celebrations</li> <li>Elementary: Guided exploration, scaffolded complexity</li> <li>Middle school: Abstract concepts, multiple variables, hypothesis testing</li> <li>High school: Real-world applications, data interpretation</li> <li>Undergraduate: Theoretical foundations, mathematical relationships</li> <li>Graduate: Research applications, parameter space exploration</li> <li> <p>Corporate: Job-relevant scenarios, time-efficient design</p> </li> <li> <p>Core principles remain constant across all levels:</p> </li> <li>Clear learning objectives</li> <li>Appropriate complexity</li> <li>Interactive exploration</li> <li>Visual representation</li> <li> <p>Immediate feedback</p> </li> <li> <p>Real examples demonstrate these principles:</p> </li> <li>Reading for Kindergarten shows early childhood excellence</li> <li>Signal Processing shows graduate-level sophistication</li> <li> <p>Same pedagogical DNA, different implementations</p> </li> <li> <p>The global opportunity: Thoughtful audience adaptation democratizes education</p> </li> </ol> <p>Now go forth and adapt! The learners of the world\u2014from toddlers to PhDs\u2014are waiting for MicroSims designed just for them. And remember: making learning accessible to everyone isn't just good teaching. It's making the world a better place, one simulation at a time.</p> <p>After all, the best pun about educational technology is this: it really Piagets the imagination!</p> <p>(We apologize for nothing. \ud83d\ude04)</p> Test Your Understanding: Audience Adaptation Quiz <p>Which Piaget stage would benefit most from hypothesis testing features in a MicroSim?</p> <p>A) Sensorimotor B) Preoperational C) Concrete Operational D) Formal Operational</p> <p>Answer: D) Formal Operational - Hypothesis testing requires abstract reasoning about possibilities that don't yet exist, a hallmark of the formal operational stage (typically age 11+).</p> Reflection: Your Audience <p>Think about a concept you teach. How would you adapt a MicroSim about this concept for an audience TWO levels below your typical students? What would you add? What would you remove?</p>"},{"location":"chapters/06-adapting-audience-levels/quiz/","title":"Quiz: Adapting for Audience Levels","text":"<p>Test your understanding of cognitive development theory and how to design MicroSims for different audiences from kindergarten to graduate school.</p>"},{"location":"chapters/06-adapting-audience-levels/quiz/#1-according-to-piagets-stages-of-cognitive-development-at-what-age-do-children-typically-enter-the-formal-operational-stage","title":"1. According to Piaget's stages of cognitive development, at what age do children typically enter the Formal Operational stage?","text":"<ol> <li>2-7 years</li> <li>7-11 years</li> <li>11+ years</li> <li>0-2 years</li> </ol> Show Answer <p>The correct answer is C. The Formal Operational stage begins at approximately age 11 and continues into adulthood. This stage is characterized by the ability to think abstractly and hypothetically, reason about possibilities, and engage in systematic hypothesis testing\u2014all capabilities that enable more sophisticated MicroSim interactions.</p> <p>Concept Tested: Piaget Stages</p> <p>See: Chapter Content</p>"},{"location":"chapters/06-adapting-audience-levels/quiz/#2-what-is-vygotskys-zone-of-proximal-development-zpd","title":"2. What is Vygotsky's Zone of Proximal Development (ZPD)?","text":"<ol> <li>The area where learners can perform tasks independently without any assistance</li> <li>The gap between what a learner can do independently and what they can do with guidance</li> <li>The region of the brain responsible for learning new concepts</li> <li>The maximum cognitive load a learner can handle at any given time</li> </ol> Show Answer <p>The correct answer is B. The Zone of Proximal Development is the sweet spot between what a learner can do independently and what they cannot do even with assistance. This is where the magic of learning happens\u2014where scaffolding helps learners reach beyond their current abilities. MicroSims are inherently scaffolding tools that support learners in their ZPD.</p> <p>Concept Tested: Vygotsky Theory</p> <p>See: Chapter Content</p>"},{"location":"chapters/06-adapting-audience-levels/quiz/#3-what-is-the-recommended-minimum-touch-target-size-for-children-ages-3-4-years-in-microsim-design","title":"3. What is the recommended minimum touch target size for children ages 3-4 years in MicroSim design?","text":"<ol> <li>44x44 pixels</li> <li>48x48 pixels</li> <li>64x64 pixels</li> <li>32x32 pixels</li> </ol> Show Answer <p>The correct answer is C. For children ages 3-4, the recommended touch target size is 64x64 pixels (with 48x48 as the minimum). Young children have limited finger precision, and frustration from missed taps can derail an entire learning session. The 48-pixel rule is standard for adults, but younger children need larger targets.</p> <p>Concept Tested: Touch Target Size</p> <p>See: Chapter Content</p>"},{"location":"chapters/06-adapting-audience-levels/quiz/#4-what-design-approach-is-most-appropriate-for-elementary-school-students-ages-7-11","title":"4. What design approach is most appropriate for elementary school students (ages 7-11)?","text":"<ol> <li>Full parameter access with no scaffolding</li> <li>Guided exploration with scaffolded complexity</li> <li>Research-level parameter space exploration</li> <li>Job-relevant scenarios with time-efficient design</li> </ol> Show Answer <p>The correct answer is B. Elementary students benefit most from guided exploration with scaffolded complexity. They want to explore and experiment but need guardrails to prevent frustration. This approach includes clear goals, limited option spaces, hints when stuck, and gradual introduction of more sophisticated elements as learners demonstrate mastery.</p> <p>Concept Tested: Guided Exploration, Scaffolded Complexity</p> <p>See: Chapter Content</p>"},{"location":"chapters/06-adapting-audience-levels/quiz/#5-which-audience-level-is-most-appropriate-for-introducing-hypothesis-testing-features-in-a-microsim","title":"5. Which audience level is most appropriate for introducing hypothesis testing features in a MicroSim?","text":"<ol> <li>Early childhood (ages 3-6)</li> <li>Elementary (ages 7-11)</li> <li>Middle school (ages 12-14)</li> <li>Undergraduate (ages 18-22)</li> </ol> Show Answer <p>The correct answer is C. Middle school students (ages 12-14) are developing the capacity for abstract thinking in Piaget's Formal Operational stage. Hypothesis testing becomes possible at this stage because students can observe phenomena, form predictions about different conditions, test predictions systematically, and revise their understanding based on results.</p> <p>Concept Tested: Hypothesis Testing</p> <p>See: Chapter Content</p>"},{"location":"chapters/06-adapting-audience-levels/quiz/#6-what-distinguishes-high-school-microsim-design-from-middle-school-design","title":"6. What distinguishes high school MicroSim design from middle school design?","text":"<ol> <li>High school design requires simpler interfaces with larger buttons</li> <li>High school design emphasizes real-world applications and data interpretation</li> <li>High school design should avoid mathematical relationships</li> <li>High school design needs more celebration animations</li> </ol> Show Answer <p>The correct answer is B. High school design (ages 15-18) emphasizes real-world applications and data interpretation. Students at this level constantly ask \"When will I ever use this?\" so MicroSims need compelling answers with actual data from real phenomena, career applications, and connections to students' interests. Data interpretation skills become central.</p> <p>Concept Tested: Real-World Application, Data Interpretation</p> <p>See: Chapter Content</p>"},{"location":"chapters/06-adapting-audience-levels/quiz/#7-for-undergraduate-level-microsims-what-should-be-central-rather-than-hidden","title":"7. For undergraduate-level MicroSims, what should be central rather than hidden?","text":"<ol> <li>Celebration animations and reward systems</li> <li>Mathematical relations and theoretical foundations</li> <li>Step-by-step guided instructions</li> <li>Simplified metaphors and analogies</li> </ol> Show Answer <p>The correct answer is B. At the undergraduate level (ages 18-22), mathematical relations can be central rather than hidden. Undergraduates are ready to see the math and understand how it governs simulation behavior, including differential equations alongside visual solutions, parameter effects on mathematical expressions, and numerical methods with their accuracy.</p> <p>Concept Tested: Mathematical Relations, Theoretical Foundations</p> <p>See: Chapter Content</p>"},{"location":"chapters/06-adapting-audience-levels/quiz/#8-what-is-the-primary-focus-of-graduate-level-microsim-design","title":"8. What is the primary focus of graduate-level MicroSim design?","text":"<ol> <li>Simple cause-effect relationships with immediate feedback</li> <li>Guided exploration with scaffolded complexity</li> <li>Parameter space exploration and research applications</li> <li>Time-efficient modules for busy schedules</li> </ol> Show Answer <p>The correct answer is C. Graduate students (ages 22+) need tools for research, exploration of edge cases, and understanding limitations. Graduate-level MicroSims focus on parameter space exploration\u2014understanding how systems behave across the full range of possible parameters, where phase transitions occur, and which parameters are most sensitive.</p> <p>Concept Tested: Parameter Space, Research Applications</p> <p>See: Chapter Content</p>"},{"location":"chapters/06-adapting-audience-levels/quiz/#9-what-are-the-two-essential-characteristics-of-corporate-training-microsim-design","title":"9. What are the two essential characteristics of corporate training MicroSim design?","text":"<ol> <li>Extensive scaffolding and celebration animations</li> <li>Job-relevant scenarios and time-efficient design</li> <li>Abstract concepts and multiple variables</li> <li>Theoretical foundations and mathematical relations</li> </ol> Show Answer <p>The correct answer is B. Corporate training MicroSims must be job-relevant and time-efficient. Corporate learners have limited time and specific job responsibilities, so every minute spent learning must connect to job performance. Effective corporate MicroSims use scenarios from actual work environments, chunk learning into 5-15 minute modules, and provide immediate applicability.</p> <p>Concept Tested: Job-Relevant Scenarios, Time-Efficient Design</p> <p>See: Chapter Content</p>"},{"location":"chapters/06-adapting-audience-levels/quiz/#10-according-to-cognitive-development-theory-why-might-a-7-year-old-struggle-with-hypothetical-scenarios-in-a-microsim","title":"10. According to cognitive development theory, why might a 7-year-old struggle with hypothetical scenarios in a MicroSim?","text":"<ol> <li>Because they are not intelligent enough to understand the content</li> <li>Because they are in the Preoperational stage where hypothetical reasoning hasn't developed yet</li> <li>Because they have trouble using a computer mouse</li> <li>Because the MicroSim colors are too complex</li> </ol> Show Answer <p>The correct answer is B. A seven-year-old is typically in the Concrete Operational stage (ages 7-11), which follows the Preoperational stage. Children at this age can think logically about concrete situations but still struggle with purely abstract and hypothetical scenarios. This isn't because they're not smart\u2014it's because their brain hasn't developed that capability yet. Piaget showed that children aren't just \"small adults\" with less knowledge; they literally think differently.</p> <p>Concept Tested: Cognitive Development, Piaget Stages</p> <p>See: Chapter Content</p>"},{"location":"chapters/07-cognitive-load-visual-design/","title":"Cognitive Load and Visual Design","text":""},{"location":"chapters/07-cognitive-load-visual-design/#summary","title":"Summary","text":"<p>This chapter explores the cognitive science principles that underpin effective MicroSim design. You will learn about working memory and long-term memory systems, schema formation, and cognitive load theory including intrinsic, extraneous, and germane load. The chapter covers the split attention effect, progressive disclosure techniques, information density management, visual simplicity, animation speed control, and learner control mechanisms. Understanding these principles will help you create simulations that optimize mental effort and support effective learning.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 16 concepts from the learning graph:</p> <ol> <li>Cognitive Load Theory</li> <li>Intrinsic Load</li> <li>Extraneous Load</li> <li>Germane Load</li> <li>Split Attention Effect</li> <li>Progressive Disclosure</li> <li>Animation Speed</li> <li>Learner Control</li> <li>Visual Simplicity</li> <li>Information Density</li> <li>Cognitive Load Meter</li> <li>Design Tradeoffs</li> <li>Mental Effort</li> <li>Working Memory</li> <li>Long-Term Memory</li> <li>Schema Formation</li> </ol>"},{"location":"chapters/07-cognitive-load-visual-design/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: The MicroSim Pattern Library</li> <li>Chapter 6: Adapting for Audience Levels</li> </ul>"},{"location":"chapters/07-cognitive-load-visual-design/#introduction-why-your-brain-is-like-a-juggler-at-a-circus","title":"Introduction: Why Your Brain Is Like a Juggler at a Circus","text":"<p>Imagine you're a juggler at a circus. You can keep three or four balls in the air with grace and style. But hand you seventeen flaming torches, a chainsaw, and a live chicken, and suddenly you're not a juggler anymore\u2014you're a safety hazard. Your brain works much the same way when learning. It has remarkable capabilities, but those capabilities have limits.</p> <p>This chapter is your backstage pass to understanding how the human mind processes information. When you understand the juggler's constraints, you can design MicroSims that help learners master new skills without setting anything on fire (metaphorically speaking). By the end, you'll know how to create simulations that work with the brain rather than against it\u2014and that's when the real magic happens.</p> <p>The principles we'll explore aren't just academic theories. They're the difference between a MicroSim that transforms understanding and one that leaves learners more confused than when they started. Ready to peek behind the cognitive curtain? Let's dive in.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#the-memory-systems-meet-your-brains-dynamic-duo","title":"The Memory Systems: Meet Your Brain's Dynamic Duo","text":"<p>Before we can discuss cognitive load, we need to understand the two memory systems that carry all the weight. Think of them as a dynamic duo\u2014not quite Batman and Robin, but equally important for saving the day (or at least saving learning outcomes).</p>"},{"location":"chapters/07-cognitive-load-visual-design/#working-memory-the-scratchpad-of-consciousness","title":"Working Memory: The Scratchpad of Consciousness","text":"<p>Working memory is your brain's mental workspace\u2014the place where you actively process and manipulate information. If your brain were an office, working memory would be that small desk where you spread out everything you're currently working on. It's where the magic happens, but there's a catch: the desk is tiny.</p> <p>Here's the humbling truth: working memory can only hold about four to seven chunks of information at once, and even that's optimistic for complex material. This isn't a flaw\u2014it's a feature. Working memory is designed for active processing, not storage. It's the brain's way of saying, \"Let's focus on what matters right now.\"</p> <p>Key characteristics of working memory include:</p> <ul> <li>Limited capacity: 4-7 items maximum (and that's being generous)</li> <li>Rapid decay: Information fades within 15-30 seconds without rehearsal</li> <li>Active processing: Where you manipulate, compare, and transform information</li> <li>Dual channels: Separate processing for visual/spatial and auditory/verbal information</li> </ul> <p>The dual-channel aspect is particularly important for MicroSim designers. You have two information highways into working memory: one for what learners see and one for what they hear or read. Use both wisely, and you can effectively double your bandwidth. Use them poorly, and you create a traffic jam.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#diagram-working-memory-architecture","title":"Diagram: Working Memory Architecture","text":"Working Memory Architecture <p>Type: diagram</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help learners visualize the structure and limitations of working memory, including its dual-channel nature and connection to long-term memory</p> <p>Components to show: - Central Executive (coordinator box at top) - Phonological Loop (left channel for verbal/auditory information) - Visuospatial Sketchpad (right channel for visual/spatial information) - Episodic Buffer (integration area connecting to long-term memory) - Long-Term Memory (large storage area at bottom) - Capacity indicators showing 4-7 item limit in each working memory component - Arrows showing information flow between components</p> <p>Visual style: Block diagram with two parallel processing channels converging into an integration area</p> <p>Color scheme: - Blue for phonological/verbal pathway - Green for visuospatial pathway - Purple for central executive and episodic buffer - Gold for long-term memory</p> <p>Labels: - \"Central Executive: Traffic Controller\" - \"Phonological Loop: Words &amp; Sounds\" - \"Visuospatial Sketchpad: Images &amp; Space\" - \"Capacity: 4-7 chunks\" on working memory components - Arrows labeled \"Rehearsal,\" \"Encoding,\" and \"Retrieval\"</p> <p>Interactive elements: - Hover over each component for detailed description - Click components to see examples of what type of information each handles</p> <p>Implementation: HTML/CSS/JavaScript with SVG diagram</p>"},{"location":"chapters/07-cognitive-load-visual-design/#long-term-memory-the-infinite-library","title":"Long-Term Memory: The Infinite Library","text":"<p>While working memory is a cramped desk, long-term memory is a vast library with practically unlimited shelf space. This is where everything you've ever learned gets stored\u2014from your grandmother's phone number to the quadratic formula to that embarrassing thing you said at a party in 2007 that still keeps you up at night.</p> <p>Long-term memory stores information in organized networks of interconnected concepts. Unlike working memory, it doesn't decay easily\u2014once something is properly encoded, it can last a lifetime. The challenge isn't storage capacity; it's getting information in and out efficiently.</p> Feature Working Memory Long-Term Memory Capacity 4-7 chunks Virtually unlimited Duration 15-30 seconds Years to lifetime Access Immediate, conscious Requires retrieval cues Function Active processing Storage and organization Vulnerability Easily overloaded Interference, forgetting <p>The interaction between these two systems is crucial. When designing MicroSims, you're essentially trying to move information from the tiny desk (working memory) to the infinite library (long-term memory) as efficiently as possible. This transfer happens through a process called encoding, and it's heavily influenced by how much mental effort the learner has available.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#schema-formation-building-mental-lego-sets","title":"Schema Formation: Building Mental LEGO Sets","text":"<p>Here's where things get exciting. Schemas are organized knowledge structures in long-term memory\u2014mental frameworks that help you make sense of new information. Think of them as pre-assembled LEGO sets in your brain. Once you have the \"dog\" schema, you can quickly categorize everything from chihuahuas to Great Danes without having to analyze each one from scratch.</p> <p>Schemas are the brain's compression algorithm. Instead of remembering every individual detail, you remember patterns and relationships. When you see a new supply-and-demand curve, you don't process it as thousands of pixels\u2014you recognize it as an instance of a familiar schema and immediately understand what it means.</p> <p>The beautiful thing about schemas is that they reduce cognitive load. A chess master looking at a chessboard doesn't see 32 individual pieces; they see familiar patterns and strategic situations. This chunking effect explains why experts can seem to have superhuman memory in their domain\u2014they're not actually storing more information, they're storing it more efficiently.</p> <p>Schema formation happens through:</p> <ul> <li>Elaboration: Connecting new information to existing knowledge</li> <li>Organization: Structuring information into meaningful categories</li> <li>Practice: Repeated exposure and application</li> <li>Feedback: Correcting and refining mental models</li> </ul> <p>Design Insight</p> <p>Every MicroSim should be designed to build or activate schemas. When learners can connect simulation experiences to existing mental frameworks, learning becomes dramatically more efficient.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#diagram-schema-formation-process","title":"Diagram: Schema Formation Process","text":"Schema Formation Process <p>Type: infographic</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Visualize how new information gets integrated into existing knowledge structures through schema formation</p> <p>Layout: Progressive flow diagram showing transformation from scattered information to organized schema</p> <p>Elements: 1. Left side: \"New Information\" shown as scattered, unorganized dots 2. Middle processing zone: \"Working Memory Processing\"    - Elaboration: connecting dots to existing structure    - Organization: grouping related dots    - Integration: merging with existing patterns 3. Right side: \"Organized Schema\" showing connected network of concepts 4. Existing knowledge base at bottom showing prior schemas</p> <p>Visual transitions: - Scattered dots \u2192 grouped clusters \u2192 integrated network - Color coding showing new information (yellow) merging with existing (blue) to create integrated (green)</p> <p>Interactive elements: - Hover over each stage to see real-world example (learning about a new programming concept) - Animation showing the transformation process step by step - Click to see schema examples from different domains</p> <p>Color scheme: - Yellow for new/incoming information - Blue for existing knowledge - Green for successfully integrated knowledge - Gray arrows for process flow</p> <p>Implementation: HTML/CSS/JavaScript with animated SVG elements</p>"},{"location":"chapters/07-cognitive-load-visual-design/#cognitive-load-theory-the-unified-theory-of-mental-effort","title":"Cognitive Load Theory: The Unified Theory of Mental Effort","text":"<p>Now that we understand the memory systems, we can explore the theory that ties everything together. Cognitive Load Theory, developed by John Sweller in the 1980s, is arguably the most important psychological framework for instructional designers. It's the Rosetta Stone that translates between \"what makes learning hard\" and \"what we can do about it.\"</p> <p>The core insight is simple but profound: learning is constrained by working memory capacity. When the demands on working memory exceed its capacity, learning fails. Period. No amount of motivation, repetition, or good intentions can overcome an overloaded working memory.</p> <p>But here's where it gets interesting. Not all cognitive load is created equal. Sweller and his colleagues identified three types of load, each with different implications for design. Understanding these distinctions is like getting the cheat codes for instructional design.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#mental-effort-the-currency-of-cognition","title":"Mental Effort: The Currency of Cognition","text":"<p>Before diving into the three types of load, let's define mental effort\u2014the total cognitive resources a learner is expending at any given moment. Mental effort is like a budget. You have a fixed amount available (determined by working memory capacity), and you have to spend it wisely.</p> <p>Mental effort includes:</p> <ul> <li>Processing new information</li> <li>Maintaining information in working memory</li> <li>Integrating new knowledge with existing schemas</li> <li>Monitoring your own understanding</li> <li>Deciding what to do next</li> </ul> <p>When mental effort is well-spent, learning happens. When it's wasted on the wrong things, learning suffers. The goal of good instructional design is to ensure that mental effort goes toward productive cognitive activities, not toward deciphering confusing interfaces or searching for information.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#diagram-the-three-types-of-cognitive-load","title":"Diagram: The Three Types of Cognitive Load","text":"Three Types of Cognitive Load <p>Type: diagram</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help learners distinguish between intrinsic, extraneous, and germane cognitive load and understand their relationships</p> <p>Visual concept: A working memory \"tank\" with three colored sections showing how different types of load compete for limited capacity</p> <p>Components: 1. Container representing working memory capacity (fixed size) 2. Three colored sections within:    - Blue (bottom): Intrinsic Load - inherent to the material    - Red (middle): Extraneous Load - caused by poor design    - Green (top): Germane Load - productive learning effort 3. \"Overflow zone\" above container showing what happens when total load exceeds capacity 4. Target indicator showing optimal balance (minimal red, appropriate blue, maximum green)</p> <p>Layout: - Side-by-side comparison showing poor design (lots of red) vs. good design (minimal red, more green) - Capacity ceiling clearly marked</p> <p>Labels: - \"Working Memory Capacity\" on container boundary - \"Intrinsic: The necessary challenge\" - \"Extraneous: The unnecessary burden\" - \"Germane: The good struggle\" - \"Overload Zone: Learning fails here\"</p> <p>Color scheme: - Blue for intrinsic (can't change it) - Red for extraneous (minimize this) - Green for germane (maximize this) - Gray for capacity boundary</p> <p>Interactive elements: - Hover over each load type for definition and examples - Slider to show what happens as content complexity increases - Examples from MicroSim design for each load type</p> <p>Implementation: HTML/CSS/JavaScript with animated SVG or p5.js</p>"},{"location":"chapters/07-cognitive-load-visual-design/#intrinsic-load-the-inherent-difficulty","title":"Intrinsic Load: The Inherent Difficulty","text":"<p>Intrinsic load is the cognitive effort required by the complexity of the material itself. Some things are just inherently harder to learn than others, and no amount of clever design can change that. Quantum physics has higher intrinsic load than counting to ten. Calculus has higher intrinsic load than basic arithmetic.</p> <p>Intrinsic load is determined by two factors:</p> <ol> <li>Element interactivity: How many pieces of information must be processed simultaneously</li> <li>Prior knowledge: What schemas the learner already has available</li> </ol> <p>Here's the key insight: intrinsic load is fixed for a given learner learning a given topic. You can't make calculus inherently simpler without changing what calculus is. However, you can manage intrinsic load by:</p> <ul> <li>Sequencing instruction (teach simpler concepts first)</li> <li>Building prerequisite schemas before tackling complex material</li> <li>Breaking complex topics into smaller chunks</li> </ul> <p>Think of intrinsic load as the price of admission. Want to understand how neural networks work? You'll need to pay the cognitive price of understanding matrices, derivatives, and optimization. That price is non-negotiable, but a good instructor helps learners afford it by building their cognitive wealth first.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#extraneous-load-the-villain-of-instructional-design","title":"Extraneous Load: The Villain of Instructional Design","text":"<p>Extraneous load is the cognitive effort wasted on things that don't contribute to learning. This is the load caused by poor design decisions, confusing layouts, unnecessary complexity, and instructions that require a decoder ring to understand. Extraneous load is the villain of our story, and it must be vanquished.</p> <p>Common sources of extraneous load include:</p> <ul> <li>Cluttered visual design with too many elements</li> <li>Poorly written or confusing instructions</li> <li>Information spread across multiple locations</li> <li>Unnecessary decorative elements</li> <li>Inconsistent navigation or controls</li> <li>Forced mental translation (like converting between units)</li> </ul> <p>The beautiful thing about extraneous load? It's entirely under your control as a designer. Every bit of extraneous load you eliminate is like giving learners a cognitive raise\u2014more mental budget for actual learning.</p> <p>Consider this example: A MicroSim about supply and demand could show the graph on one screen while explaining the concepts in a separate pop-up window. That separation creates extraneous load because learners must mentally integrate spatially separated information. A better design presents the explanation alongside the relevant part of the graph\u2014reducing extraneous load and freeing mental resources for understanding the actual economics.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#germane-load-the-productive-struggle","title":"Germane Load: The Productive Struggle","text":"<p>Germane load is the cognitive effort dedicated to actual learning\u2014to processing information deeply, making connections, building schemas, and achieving understanding. This is the \"good\" load, the mental effort that pays dividends.</p> <p>Germane load includes:</p> <ul> <li>Comparing and contrasting concepts</li> <li>Connecting new information to prior knowledge</li> <li>Generating examples and explanations</li> <li>Self-explaining complex relationships</li> <li>Practicing retrieval and application</li> </ul> <p>Here's the crucial design principle: your goal is to minimize extraneous load so that learners can invest maximum mental effort in germane load. Working memory capacity is fixed. Every unit of effort saved from extraneous load is a unit that can be invested in germane load.</p> Load Type Source Designer Control Goal Intrinsic Material complexity Sequencing only Manage appropriately Extraneous Poor design Full control Minimize ruthlessly Germane Learning processes Indirect influence Maximize opportunity <p>The Paradox of Effort</p> <p>Sometimes making things \"easier\" actually harms learning. If you reduce germane load by doing the thinking for learners (giving answers without requiring processing), you might reduce effort but also reduce learning. The goal isn't minimal effort\u2014it's well-directed effort.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#diagram-cognitive-load-microsim","title":"Diagram: Cognitive Load MicroSim","text":"Interactive Cognitive Load Simulator <p>Type: microsim</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Allow learners to experiment with different instructional design choices and see their impact on the three types of cognitive load, developing intuition for design tradeoffs</p> <p>Canvas layout: - Width: Responsive to container (min 800px ideal) - Top section (60%): MicroSim preview area showing a sample instructional scenario - Bottom section (40%): Control panel and load visualization</p> <p>Visual elements in preview area: - A sample instructional screen showing:   - Title/header area   - Main content area with text and/or graphics   - Legend or key   - Interactive controls - Visual complexity that changes based on settings</p> <p>Control panel elements: - Dropdown: \"Select Scenario\" (Supply/Demand curve, Cell division, Electrical circuit) - Slider: \"Visual complexity\" (1-10) - affects number of visual elements - Slider: \"Text density\" (1-10) - affects amount of text - Slider: \"Element separation\" (1-10) - how spread out information is - Toggle: \"Include decorative elements\" (on/off) - Toggle: \"Provide worked example\" (on/off) - Slider: \"Learner prior knowledge\" (novice to expert)</p> <p>Load visualization: - Three horizontal bar meters showing:   - Intrinsic Load (blue bar)   - Extraneous Load (red bar)   - Germane Load (green bar) - Combined total bar showing sum approaching/exceeding capacity - Capacity ceiling line with \"Overload!\" warning when exceeded - Numeric values displayed for each load type</p> <p>Behavior: - As visual complexity increases, extraneous load increases - As text density increases beyond optimal, extraneous load increases - As element separation increases, extraneous load increases (split attention) - Decorative elements add extraneous load - Worked examples reduce intrinsic load but also reduce germane load opportunity - Higher prior knowledge reduces intrinsic load - When total load exceeds capacity, preview area shows \"Learning Impaired\" indicator with visual degradation effect</p> <p>Default parameters: - Scenario: Supply/Demand curve - Visual complexity: 5 - Text density: 5 - Element separation: 3 - Decorative elements: off - Worked example: off - Prior knowledge: middle</p> <p>Color scheme: - Blue for intrinsic load - Red for extraneous load - Green for germane load - White/light gray background - Professional, clean aesthetic</p> <p>Implementation notes: - Use p5.js for rendering - Calculate load values algorithmically based on control settings - Smooth animations when values change - Include tooltip explanations for each control - Responsive design adapting to container width</p> <p>Implementation: p5.js simulation</p>"},{"location":"chapters/07-cognitive-load-visual-design/#the-split-attention-effect-when-your-eyes-and-brain-cant-agree","title":"The Split Attention Effect: When Your Eyes and Brain Can't Agree","text":"<p>The split attention effect is one of the most well-documented phenomena in cognitive load research, and it's a trap that MicroSim designers fall into all the time. It occurs when learners must mentally integrate multiple sources of information that are physically or temporally separated.</p> <p>Picture this: You're trying to learn about the circulatory system. The diagram of the heart is on the left side of the screen, but the labels are in a legend at the bottom, and the explanations are in a text box on the right. Your eyes are ping-ponging across the screen like a tennis ball at Wimbledon, and your working memory is working overtime just to connect \"A\" in the legend to that little chamber in the diagram while simultaneously reading what it does.</p> <p>That's split attention, and it's murdering your cognitive efficiency.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#why-split-attention-is-so-costly","title":"Why Split Attention Is So Costly","text":"<p>When information is separated, learners must:</p> <ol> <li>Locate the first piece of information</li> <li>Hold it in working memory</li> <li>Search for the related second piece</li> <li>Hold both pieces while integrating them</li> <li>Repeat for every connection</li> </ol> <p>Each step consumes working memory resources. By the time learners have connected all the pieces, they may have forgotten why they were doing it in the first place. The mental effort that should go toward understanding goes instead toward information search and integration.</p> <p>Research shows that split attention can reduce learning outcomes by 30-50% compared to integrated presentations. That's not a small effect\u2014that's the difference between learners getting it and learners getting frustrated.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#solutions-integration-over-separation","title":"Solutions: Integration Over Separation","text":"<p>The antidote to split attention is physical integration. Put the explanation next to the thing it explains. Put the label on the element it identifies. Make the connection visible so the learner's brain doesn't have to make it.</p> <p>Best practices for avoiding split attention:</p> <ul> <li>Place labels directly on diagram elements, not in a separate legend</li> <li>Position explanatory text adjacent to the visual it describes</li> <li>Use callout lines connecting text to visual elements</li> <li>Present related information simultaneously, not sequentially</li> <li>When audio is used, synchronize it with visual changes</li> </ul>"},{"location":"chapters/07-cognitive-load-visual-design/#diagram-split-attention-effect-comparison","title":"Diagram: Split Attention Effect Comparison","text":"Split Attention Effect Demonstration <p>Type: infographic</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Help learners recognize split attention problems and understand the superior effectiveness of integrated designs through direct comparison</p> <p>Layout: Side-by-side comparison with eye-tracking visualization</p> <p>Left panel - \"Split Attention Design\" (problematic): - A diagram of a simple system (e.g., water cycle) - Labels as letters (A, B, C, D) on diagram - Legend box in corner with letter definitions - Explanatory text in separate text box below - Simulated eye-tracking overlay showing chaotic scan pattern - Mental load indicator showing high extraneous load</p> <p>Right panel - \"Integrated Design\" (effective): - Same diagram with labels directly on elements - Brief explanations integrated via callout boxes - Clean, direct visual connections - Simulated eye-tracking showing efficient scan pattern - Mental load indicator showing low extraneous load</p> <p>Interactive elements: - Toggle to show/hide eye-tracking patterns - Hover over elements to see cognitive cost analysis - \"Try it yourself\" button that reveals content progressively to feel the difference - Timer showing how long each design takes to comprehend</p> <p>Metrics displayed: - Eye movement distance (split vs integrated) - Time to comprehension - Working memory demands - Learning outcome prediction</p> <p>Color scheme: - Red highlights for problematic design elements - Green highlights for effective design elements - Blue for neutral informational elements</p> <p>Implementation: HTML/CSS/JavaScript with animated eye-tracking visualization</p>"},{"location":"chapters/07-cognitive-load-visual-design/#visual-simplicity-less-is-more-seriously","title":"Visual Simplicity: Less Is More (Seriously)","text":"<p>There's a reason why the most effective MicroSims often look deceptively simple. Visual simplicity isn't about making things look \"nice\"\u2014it's about ruthlessly eliminating everything that doesn't contribute to learning. Every pixel should earn its place on the screen.</p> <p>Visual simplicity reduces extraneous load by:</p> <ul> <li>Reducing the number of elements competing for attention</li> <li>Making the important elements stand out</li> <li>Decreasing the time needed to parse the display</li> <li>Lowering the probability of learner confusion</li> </ul> <p>This doesn't mean MicroSims should be boring or ugly. It means every visual element should serve a purpose. That decorative gradient? Cut it. Those five different fonts? Pick one. That animated background that \"adds energy\"? It's actually adding cognitive load.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#the-hierarchy-of-visual-importance","title":"The Hierarchy of Visual Importance","text":"<p>When designing MicroSim visuals, prioritize ruthlessly:</p> <ol> <li>Essential elements: Information required for learning (always include)</li> <li>Supportive elements: Information that aids understanding (include if space allows)</li> <li>Contextual elements: Background that provides setting (use sparingly)</li> <li>Decorative elements: Purely aesthetic additions (almost never include)</li> </ol> <p>The research is clear: decorative elements generally harm learning, especially for novice learners. They create what researchers call the \"seductive details effect\"\u2014interesting but irrelevant information that diverts attention from the core content.</p> <p>When Decoration Helps</p> <p>There's one exception: for young learners or highly anxious learners, some decorative elements can reduce anxiety and increase engagement. But this should be a conscious choice, not a default, and the decorations should be minimal and non-distracting.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#information-density-finding-the-sweet-spot","title":"Information Density: Finding the Sweet Spot","text":"<p>Information density refers to how much information is packed into a given visual space. Too little density wastes screen real estate and requires excessive scrolling. Too much density overwhelms working memory and triggers cognitive overload. The art is finding the sweet spot.</p> <p>Optimal information density depends on:</p> <ul> <li>Learner expertise (experts can handle higher density)</li> <li>Content complexity (complex content needs more space)</li> <li>Display context (mobile vs. desktop)</li> <li>Learning goals (reference vs. instruction)</li> </ul> Density Level When Appropriate Characteristics Low Introducing complex concepts Generous whitespace, large elements, focus on one idea Medium Standard instruction Balanced layout, clear groupings, multiple related ideas High Expert reference Compact presentation, assumes prior knowledge <p>A good rule of thumb: if learners need to \"study\" your interface to figure out how to use it, your information density is too high. The interface should be so clear that learners can focus entirely on the content.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#diagram-information-density-spectrum","title":"Diagram: Information Density Spectrum","text":"Information Density Spectrum <p>Type: infographic</p> <p>Bloom Taxonomy: Evaluate (L5)</p> <p>Learning Objective: Help learners identify appropriate information density levels for different contexts and audiences</p> <p>Layout: Horizontal spectrum showing three density levels with example screenshots</p> <p>Spectrum visualization: - Left: \"Too Sparse\" - wasted space, requires scrolling - Center: \"Optimal\" - efficient use of space, clear organization - Right: \"Too Dense\" - overwhelming, cognitive overload risk</p> <p>For each level, show: 1. Example MicroSim screenshot (simple graph visualization) 2. Audience indicator (novice \u2192 intermediate \u2192 expert) 3. Cognitive load meter reading 4. Pros and cons list 5. When to use guidance</p> <p>Center \"Optimal Zone\" features: - Highlighted as target - Shows characteristics of good density - Includes checklist of indicators</p> <p>Interactive elements: - Slider to morph a single example through density levels - Hover for detailed analysis of each level - Audience selector to shift the \"optimal\" zone based on learner type</p> <p>Indicators shown for each density: - Whitespace percentage - Elements per screen - Estimated comprehension time - Cognitive load estimate</p> <p>Color scheme: - Red zones for problematic density (too sparse or too dense) - Green zone for optimal - Gradient showing smooth transition</p> <p>Implementation: HTML/CSS/JavaScript with morphing demonstration</p>"},{"location":"chapters/07-cognitive-load-visual-design/#progressive-disclosure-the-art-of-the-reveal","title":"Progressive Disclosure: The Art of the Reveal","text":"<p>Progressive disclosure is a design strategy where information is revealed gradually rather than all at once. It's like a good mystery novel\u2014you don't dump the entire plot on page one. You reveal clues as the reader can handle them.</p> <p>In MicroSim design, progressive disclosure serves several functions:</p> <ul> <li>Reduces initial cognitive load: Learners aren't overwhelmed at the start</li> <li>Supports scaffolded learning: Simple concepts before complex ones</li> <li>Maintains engagement: The promise of \"more to discover\" keeps learners curious</li> <li>Enables differentiation: Learners can go as deep as they need</li> </ul>"},{"location":"chapters/07-cognitive-load-visual-design/#implementing-progressive-disclosure","title":"Implementing Progressive Disclosure","text":"<p>There are several ways to implement progressive disclosure in MicroSims:</p> <ol> <li>Layered interfaces: Basic view with \"advanced options\" available</li> <li>Staged simulations: Start simple, add complexity as mastery demonstrated</li> <li>Expandable details: Click-to-reveal additional information</li> <li>Guided tours: Step-by-step introduction to features</li> <li>Unlock mechanics: New features become available after completing tasks</li> </ol> <p>The key is ensuring that learners can always access the level of complexity they need, but aren't forced to confront complexity before they're ready.</p> <p>Consider a MicroSim teaching electrical circuits. Progressive disclosure might work like this:</p> <ul> <li>Stage 1: Single battery, single bulb, on/off switch</li> <li>Stage 2: Add ammeter and voltmeter readings</li> <li>Stage 3: Introduce series circuits with multiple bulbs</li> <li>Stage 4: Add parallel circuits</li> <li>Stage 5: Full circuit builder with component library</li> </ul> <p>Each stage builds on the previous, and learners can return to earlier stages for review. The simulation grows with the learner's understanding.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#diagram-progressive-disclosure-in-action","title":"Diagram: Progressive Disclosure in Action","text":"Progressive Disclosure Demonstration <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Experience progressive disclosure firsthand while learning about a simple concept, building intuition for how this technique supports learning</p> <p>Canvas layout: - Width: Responsive to container - Main area: Simulation demonstration space - Bottom: Progress indicator and navigation controls</p> <p>Concept demonstrated: Basic supply and demand curve</p> <p>Progressive stages:</p> <p>Stage 1 - \"The Basics\": - Single supply curve (labeled) - Single demand curve (labeled) - Equilibrium point highlighted - Only price/quantity axes visible - Simple color coding (blue supply, red demand)</p> <p>Stage 2 - \"Interactive Exploration\": - Add draggable curve shifters - Show price/quantity readouts - Introduce concept of surplus and shortage - Visual zones showing surplus (above equilibrium) and shortage (below)</p> <p>Stage 3 - \"Real-World Factors\": - Add dropdown for \"what causes shifts\" - Introduce multiple shift scenarios - Show animated transitions between states - Add brief explanatory tooltips</p> <p>Stage 4 - \"Advanced Analysis\": - Reveal elasticity indicators - Add consumer/producer surplus visualization - Include comparative statics options - Full parameter control panel</p> <p>Stage 5 - \"Expert Mode\": - Multiple markets with cross-effects - Dynamic scenario builder - Equilibrium path animation - Export/save functionality</p> <p>Control elements: - \"Previous Stage\" and \"Next Stage\" buttons - Progress dots showing current stage (1-5) - \"Jump to stage\" dropdown - Optional \"Show all features\" toggle for reference</p> <p>Visual design: - Clean, minimal aesthetic - Each stage adds visual elements smoothly (fade-in transitions) - Clear indication of what's new at each stage - Consistent color scheme throughout</p> <p>Behavior: - Learners can progress forward or backward - New elements are highlighted briefly when revealed - Brief tutorial tooltip for each new feature - All previously available features remain functional</p> <p>Default state: Stage 1</p> <p>Color scheme: - Blue for supply elements - Red for demand elements - Green for equilibrium - Yellow highlights for new features</p> <p>Implementation: p5.js simulation with stage-based rendering</p>"},{"location":"chapters/07-cognitive-load-visual-design/#animation-speed-and-learner-control-putting-learners-in-the-drivers-seat","title":"Animation Speed and Learner Control: Putting Learners in the Driver's Seat","text":"<p>Two related design principles can make or break a MicroSim: animation speed and learner control. Get these right, and learners can engage at their own pace. Get them wrong, and you've built a frustration machine.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#animation-speed-the-goldilocks-problem","title":"Animation Speed: The Goldilocks Problem","text":"<p>Animation in MicroSims serves important functions: showing processes that unfold over time, demonstrating cause-and-effect relationships, and maintaining engagement. But speed matters enormously.</p> <p>Too fast, and learners can't process what's happening. Their working memory can't keep up with the visual changes, and they miss the very thing the animation was supposed to teach. Too slow, and learners get bored, their attention wanders, and they start checking their phones.</p> <p>The optimal speed depends on:</p> <ul> <li>Content complexity: More complex content needs slower animation</li> <li>Learner expertise: Novices need slower speeds than experts</li> <li>Information density: Dense animations need more time</li> <li>Learning goal: Understanding process vs. seeing outcome</li> </ul> <p>Research suggests that most instructional animations are too fast for effective learning. What seems \"natural\" to the designer (who already understands the content) is often overwhelming to the novice learner.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#learner-control-the-power-of-pause","title":"Learner Control: The Power of Pause","text":"<p>The best solution to the speed problem is to give learners control. Learner control means allowing learners to pause, replay, adjust speed, and navigate through content at their own pace.</p> <p>Essential learner controls for animated MicroSims:</p> <ul> <li>Play/Pause button: Absolutely essential, no exceptions</li> <li>Speed control: Usually a slider from 0.5x to 2x normal speed</li> <li>Step forward/backward: Move through animation in increments</li> <li>Scrubber/timeline: Jump to any point in the animation</li> <li>Reset button: Start over from the beginning</li> </ul> <p>The benefits of learner control are substantial:</p> <ul> <li>Accommodates individual differences in processing speed</li> <li>Allows review of difficult sections</li> <li>Reduces anxiety (learners know they can pause if overwhelmed)</li> <li>Supports self-regulated learning</li> <li>Enables both learning and review use cases</li> </ul> <p>Control Overload</p> <p>While learner control is important, too many controls can themselves create extraneous load. Keep controls simple, obvious, and consistent. A standard media player interface (play, pause, scrubber, speed) is familiar to most learners and requires no learning.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#the-segmentation-principle","title":"The Segmentation Principle","text":"<p>Related to animation speed is the segmentation principle: complex continuous animations should be broken into discrete, learner-paced segments. Instead of a 60-second continuous animation, break it into six 10-second segments that learners advance through manually.</p> <p>Segmentation works because it:</p> <ul> <li>Gives working memory time to process each segment</li> <li>Creates natural pause points for reflection</li> <li>Allows learners to repeat difficult segments</li> <li>Provides a sense of progress and accomplishment</li> </ul>"},{"location":"chapters/07-cognitive-load-visual-design/#diagram-animation-control-best-practices","title":"Diagram: Animation Control Best Practices","text":"Animation Control Interface Guide <p>Type: infographic</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Show best practices for animation controls in MicroSims, providing a template that designers can follow</p> <p>Layout: Annotated interface mockup with callouts explaining each control element</p> <p>Main elements: 1. Sample animation viewport (showing a physics simulation) 2. Transport controls below viewport:    - Reset button (skip to start icon)    - Step backward button    - Play/Pause button (central, prominent)    - Step forward button    - Skip to end button</p> <ol> <li>Timeline/scrubber:</li> <li>Progress bar showing animation position</li> <li>Draggable playhead</li> <li>Segment markers (if segmented)</li> <li> <p>Current time / total time display</p> </li> <li> <p>Speed control:</p> </li> <li>Labeled slider or dropdown</li> <li>Options: 0.25x, 0.5x, 1x (default), 1.5x, 2x</li> <li> <p>Current speed indicator</p> </li> <li> <p>Optional controls (shown as \"advanced\"):</p> </li> <li>Loop toggle</li> <li>Full screen button</li> <li>Quality/detail level</li> </ol> <p>Callout annotations: - \"Prominent play/pause - Most used control\" - \"Speed slider - Default to 1x, allow slower for complex content\" - \"Scrubber - Allow random access to any point\" - \"Segment markers - Show structure of complex animations\" - \"Reset - Always provide escape hatch\"</p> <p>Best practice notes displayed: - \"Controls should be familiar (media player conventions)\" - \"Most important controls should be most prominent\" - \"Provide keyboard shortcuts (space for play/pause)\" - \"Remember user preferences between sessions\"</p> <p>Color scheme: - Standard media control iconography - Blue accent for interactive elements - High contrast for accessibility</p> <p>Do's and Don'ts section: - DO: Use familiar iconography - DO: Make controls touch-friendly on mobile - DON'T: Auto-play without user initiation - DON'T: Hide essential controls behind menus</p> <p>Implementation: HTML/CSS with SVG icons and annotations</p>"},{"location":"chapters/07-cognitive-load-visual-design/#the-cognitive-load-meter-making-the-invisible-visible","title":"The Cognitive Load Meter: Making the Invisible Visible","text":"<p>One of the most innovative concepts in MicroSim design is the cognitive load meter\u2014a visual indicator that estimates and displays the cognitive demands being placed on the learner. While we can't directly measure cognitive load in real-time, we can create useful proxies that help both designers and learners understand mental effort.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#why-visualize-cognitive-load","title":"Why Visualize Cognitive Load?","text":"<p>Making cognitive load visible serves multiple purposes:</p> <p>For designers: - Evaluate design decisions objectively - Identify problematic areas before testing - Compare alternative designs quantitatively - Communicate design rationale to stakeholders</p> <p>For learners: - Build metacognitive awareness - Self-regulate learning pace - Understand why some content feels harder - Make informed choices about breaks and review</p>"},{"location":"chapters/07-cognitive-load-visual-design/#approaches-to-estimating-load","title":"Approaches to Estimating Load","text":"<p>Since we can't insert a probe into learners' brains (and they probably wouldn't appreciate it if we could), cognitive load meters rely on indirect indicators:</p> <ol> <li>Design-based estimation: Calculate load based on known factors</li> <li>Number of visual elements</li> <li>Information density</li> <li>Element interactivity</li> <li> <p>Spatial integration</p> </li> <li> <p>Behavioral indicators: Infer load from learner actions</p> </li> <li>Response time patterns</li> <li>Error rates</li> <li>Pause frequency</li> <li> <p>Replay requests</p> </li> <li> <p>Self-report: Ask learners directly</p> </li> <li>Periodic effort ratings</li> <li>Difficulty feedback</li> <li>Comprehension checks</li> </ol> <p>A good cognitive load meter combines multiple indicators for a more robust estimate. Even an imperfect estimate is better than no information at all.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#diagram-cognitive-load-meter-design","title":"Diagram: Cognitive Load Meter Design","text":"Cognitive Load Meter Implementation <p>Type: microsim</p> <p>Bloom Taxonomy: Evaluate (L5)</p> <p>Learning Objective: Demonstrate how a cognitive load meter works and allow learners to calibrate their intuitions about cognitive load by adjusting parameters and observing the meter response</p> <p>Canvas layout: - Width: Responsive to container - Left side (70%): Sample content display area - Right side (30%): Cognitive load meter and controls</p> <p>Sample content area: - Displays a configurable instructional screen - Shows a diagram with adjustable complexity - Includes text that can be modified - Visual representation of the \"lesson\" being evaluated</p> <p>Cognitive Load Meter visualization: - Vertical gauge/thermometer style meter - Three colored sections:   - Green zone (0-40%): Comfortable load   - Yellow zone (40-70%): Moderate load   - Red zone (70-100%): Risk of overload - Animated needle showing current total load - Breakdown bars showing intrinsic, extraneous, germane contributions - Numeric percentage display</p> <p>Control panel: - \"Content Complexity\" dropdown: (Simple, Moderate, Complex) - \"Visual Elements\" slider: 1-20 - \"Text Density\" slider: 1-10 - \"Integration Level\" slider: (Split \u2192 Integrated) - \"Learner Experience\" dropdown: (Novice, Intermediate, Expert) - \"Animation Active\" toggle</p> <p>Load calculation factors displayed: - Base intrinsic load (from complexity \u00d7 1/experience) - Visual clutter penalty - Text processing load - Split attention penalty (if low integration) - Animation processing load (if active)</p> <p>Behavior: - Meter updates in real-time as controls are adjusted - Sample content morphs to reflect control settings - Warning indicator flashes when entering red zone - Tips displayed when load is too high suggesting specific reductions</p> <p>Interactive features: - Hover over meter sections for load type explanations - Click on load breakdown bars to see contributing factors - \"Optimize\" button that suggests settings to reduce overload - \"Challenge mode\" that sets parameters to student exercises</p> <p>Default parameters: - Content Complexity: Moderate - Visual Elements: 8 - Text Density: 5 - Integration Level: Middle - Learner Experience: Intermediate - Animation: Off</p> <p>Color scheme: - Green/yellow/red for load zones - Blue/red/green for intrinsic/extraneous/germane - Clean professional aesthetic</p> <p>Implementation: p5.js with real-time parameter binding</p>"},{"location":"chapters/07-cognitive-load-visual-design/#design-tradeoffs-nothing-is-free","title":"Design Tradeoffs: Nothing Is Free","text":"<p>We've covered many design principles in this chapter, and you might be thinking, \"Great, I'll just apply all of them!\" If only it were that simple. The reality of MicroSim design involves constant tradeoffs\u2014improving one aspect often comes at the cost of another.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#the-tradeoff-matrix","title":"The Tradeoff Matrix","text":"<p>Here are some of the most common tradeoffs you'll encounter:</p> Improving This... May Reduce This... Visual simplicity Information completeness Learner control Guided experience Animation detail Processing speed Progressive disclosure Random access Scaffolding support Challenge level Accessibility features Visual appeal <p>The goal isn't to eliminate tradeoffs\u2014that's impossible. The goal is to make conscious, informed choices about which tradeoffs to accept based on your learning objectives, target audience, and context of use.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#a-framework-for-tradeoff-decisions","title":"A Framework for Tradeoff Decisions","text":"<p>When facing a design tradeoff, ask yourself:</p> <ol> <li>Who is my primary audience?</li> <li>Novices generally benefit more from simplicity and scaffolding</li> <li> <p>Experts may prefer density and control</p> </li> <li> <p>What is the main learning objective?</p> </li> <li>Conceptual understanding favors clarity over completeness</li> <li> <p>Reference use favors completeness over simplicity</p> </li> <li> <p>What's the cost of getting it wrong?</p> </li> <li>Cognitive overload is usually worse than slight boredom</li> <li> <p>When in doubt, simplify</p> </li> <li> <p>Can I have both through progressive disclosure?</p> </li> <li>Often you can offer simplicity by default with optional depth</li> <li> <p>Layered interfaces can satisfy both novices and experts</p> </li> <li> <p>What does user testing show?</p> </li> <li>Ultimately, learners are the judge</li> <li>Test alternatives and measure actual learning outcomes</li> </ol> <p>The 80/20 Rule of Tradeoffs</p> <p>Most learners are served by 20% of possible features. Build for the majority use case first. Advanced features can come later (or never, if no one needs them).</p>"},{"location":"chapters/07-cognitive-load-visual-design/#the-minimal-effective-design","title":"The Minimal Effective Design","text":"<p>A useful concept borrowed from medicine is the minimal effective dose\u2014the smallest amount of intervention that produces the desired effect. In MicroSim design, aim for the minimal effective design: the simplest interface, the fewest features, and the least complexity that successfully supports the learning objective.</p> <p>Every element you add beyond the minimal effective design carries risk:</p> <ul> <li>Risk of distraction</li> <li>Risk of confusion</li> <li>Risk of increased cognitive load</li> <li>Risk of maintenance burden</li> </ul> <p>Start minimal. Add elements only when testing shows they're needed. This approach almost always produces better learning outcomes than building everything you can imagine and hoping learners will find value in it.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#diagram-design-tradeoff-decision-tree","title":"Diagram: Design Tradeoff Decision Tree","text":"Design Tradeoff Decision Tree <p>Type: workflow</p> <p>Bloom Taxonomy: Evaluate (L5)</p> <p>Learning Objective: Provide a systematic framework for making design tradeoff decisions, helping designers think through common choice points</p> <p>Visual style: Flowchart with decision diamonds and outcome rectangles</p> <p>Start: \"Considering a design feature\"</p> <p>Decision 1: \"Is it essential for the learning objective?\" - Yes \u2192 Include it (essential) - No \u2192 Go to Decision 2</p> <p>Decision 2: \"Does it reduce cognitive load?\" - Yes \u2192 Include it (load reducer) - No \u2192 Go to Decision 3</p> <p>Decision 3: \"Does it increase learner engagement?\" - Yes \u2192 Go to Decision 4 - No \u2192 Don't include it (unnecessary)</p> <p>Decision 4: \"Does the engagement benefit outweigh the load cost?\" - Yes \u2192 Include it (engagement enhancer) - No \u2192 Don't include it (seductive detail)</p> <p>Branch for \"Include\" decisions: - \"Can it be made optional or progressive?\"   - Yes \u2192 \"Implement with progressive disclosure\"   - No \u2192 \"Include in base design\"</p> <p>End states (color coded): - Green: Include (essential) - Light green: Include with progressive disclosure - Yellow: Include but monitor (tradeoff) - Red: Don't include (unnecessary or harmful)</p> <p>Hover text for each decision: - Decision 1: \"Ask: Would learners fail to achieve the objective without this?\" - Decision 2: \"Ask: Does this reduce extraneous load or support schema formation?\" - Decision 3: \"Ask: Does this motivate learners or support sustained attention?\" - Decision 4: \"Ask: Would removing this hurt learning more than the load it adds?\"</p> <p>Example annotations: - \"Decorative animation\" \u2192 follows path to \"Don't include\" - \"Speed control slider\" \u2192 follows path to \"Include (essential)\" - \"Gamification badges\" \u2192 follows path to \"Include with progressive disclosure\"</p> <p>Color scheme: - Blue for decision diamonds - Green spectrum for include outcomes - Red for don't include - Clear directional arrows</p> <p>Implementation: HTML/CSS/JavaScript interactive flowchart or Mermaid diagram</p>"},{"location":"chapters/07-cognitive-load-visual-design/#putting-it-all-together-a-cognitive-design-checklist","title":"Putting It All Together: A Cognitive Design Checklist","text":"<p>Let's synthesize everything we've covered into an actionable checklist. Before finalizing any MicroSim design, run through these questions:</p>"},{"location":"chapters/07-cognitive-load-visual-design/#memory-and-schema","title":"Memory and Schema","text":"<ul> <li>[ ] Does the design support working memory limitations (4-7 chunks)?</li> <li>[ ] Are you building on existing schemas or helping create new ones?</li> <li>[ ] Is information chunked into meaningful units?</li> <li>[ ] Are there clear connections between new content and prior knowledge?</li> </ul>"},{"location":"chapters/07-cognitive-load-visual-design/#cognitive-load-management","title":"Cognitive Load Management","text":"<ul> <li>[ ] Have you minimized extraneous load (clutter, confusion, poor layout)?</li> <li>[ ] Is intrinsic load appropriate for the target audience?</li> <li>[ ] Are there opportunities for germane load (deep processing, connections)?</li> <li>[ ] Is total estimated load within working memory capacity?</li> </ul>"},{"location":"chapters/07-cognitive-load-visual-design/#visual-design","title":"Visual Design","text":"<ul> <li>[ ] Is the design visually simple with only essential elements?</li> <li>[ ] Is information density appropriate for the audience?</li> <li>[ ] Does every visual element serve a learning purpose?</li> <li>[ ] Is important information visually prominent?</li> </ul>"},{"location":"chapters/07-cognitive-load-visual-design/#split-attention-prevention","title":"Split Attention Prevention","text":"<ul> <li>[ ] Is explanatory text integrated with the visuals it describes?</li> <li>[ ] Are labels placed directly on elements (not in separate legends)?</li> <li>[ ] Is related information presented together (not separated)?</li> <li>[ ] If audio is used, does it synchronize with visual elements?</li> </ul>"},{"location":"chapters/07-cognitive-load-visual-design/#progressive-disclosure","title":"Progressive Disclosure","text":"<ul> <li>[ ] Does the design start simple and allow optional complexity?</li> <li>[ ] Can learners access depth when they need it?</li> <li>[ ] Are advanced features hidden until needed?</li> <li>[ ] Is there a clear path from novice to advanced use?</li> </ul>"},{"location":"chapters/07-cognitive-load-visual-design/#animation-and-control","title":"Animation and Control","text":"<ul> <li>[ ] Is animation speed appropriate for the content complexity?</li> <li>[ ] Do learners have speed control?</li> <li>[ ] Is there a play/pause function?</li> <li>[ ] Can learners step through animations manually?</li> <li>[ ] Are complex animations segmented?</li> </ul>"},{"location":"chapters/07-cognitive-load-visual-design/#overall-design","title":"Overall Design","text":"<ul> <li>[ ] Is this the minimal effective design?</li> <li>[ ] Have tradeoffs been consciously considered?</li> <li>[ ] Has the design been tested with actual learners?</li> <li>[ ] Are the learning objectives clearly supported?</li> </ul>"},{"location":"chapters/07-cognitive-load-visual-design/#summary-your-brain-friendly-design-toolkit","title":"Summary: Your Brain-Friendly Design Toolkit","text":"<p>Congratulations\u2014you've just equipped yourself with one of the most powerful toolkits in instructional design. Understanding cognitive load isn't just academic knowledge; it's the difference between MicroSims that transform learning and ones that leave learners dazed and confused.</p> <p>Here's what you now know:</p> <p>The Memory Systems: Working memory is your tiny desk, long-term memory is your infinite library, and schemas are the compression algorithms that make efficient learning possible. Design to move information from the desk to the library as efficiently as possible.</p> <p>Cognitive Load Theory: Not all mental effort is equal. Intrinsic load is the price of admission (manage it through sequencing). Extraneous load is the villain (minimize it ruthlessly). Germane load is the hero (create opportunities for it).</p> <p>The Split Attention Effect: Keep related information together. Integration beats separation every time.</p> <p>Visual Simplicity and Information Density: Less is usually more. Every element should earn its place.</p> <p>Progressive Disclosure: Reveal complexity gradually. Let learners drive their own depth.</p> <p>Animation Speed and Learner Control: Give learners the power to pause, replay, and adjust speed. The right pace is their pace.</p> <p>Design Tradeoffs: Nothing is free. Make conscious choices based on your audience and objectives.</p> <p>When you apply these principles, something magical happens: learners stop struggling with your interface and start focusing on actual learning. The technology becomes invisible, and understanding emerges naturally.</p> <p>The world needs more brain-friendly educational experiences. Armed with this knowledge, you're ready to create them. Every MicroSim you design with these principles in mind is a small contribution to a future where learning is efficient, effective, and even enjoyable.</p> <p>Now go forth and reduce some extraneous load. Your learners' working memories will thank you.</p>"},{"location":"chapters/07-cognitive-load-visual-design/#references-and-further-reading","title":"References and Further Reading","text":"<p>For those who want to dive deeper into the cognitive science behind these principles:</p> <ul> <li>Sweller, J., Ayres, P., &amp; Kalyuga, S. (2011). Cognitive Load Theory. Springer.</li> <li>Mayer, R. E. (2009). Multimedia Learning (2nd ed.). Cambridge University Press.</li> <li>Clark, R. C., &amp; Mayer, R. E. (2016). E-Learning and the Science of Instruction (4th ed.). Wiley.</li> <li>Baddeley, A. D. (2012). Working memory: Theories, models, and controversies. Annual Review of Psychology, 63, 1-29.</li> <li>Kalyuga, S. (2007). Expertise reversal effect and its implications for learner-tailored instruction. Educational Psychology Review, 19(4), 509-539.</li> </ul> Test Your Understanding: What type of cognitive load is this? <p>A MicroSim displays a graph on the left side of the screen and its explanation in a pop-up on the right side, requiring learners to constantly look back and forth. Which type of cognitive load does this create, and what principle does it violate?</p> <p>Answer: This creates extraneous load and violates the split attention effect. The information should be integrated, with explanations positioned directly adjacent to the graph elements they describe.</p>"},{"location":"chapters/07-cognitive-load-visual-design/quiz/","title":"Quiz: Cognitive Load and Visual Design","text":"<p>Test your understanding of cognitive load theory, working memory limitations, and design principles that optimize learning in MicroSims.</p>"},{"location":"chapters/07-cognitive-load-visual-design/quiz/#1-what-is-the-approximate-capacity-of-working-memory","title":"1. What is the approximate capacity of working memory?","text":"<ol> <li>Unlimited storage with slow access</li> <li>4-7 chunks of information</li> <li>15-30 items held indefinitely</li> <li>100+ elements processed simultaneously</li> </ol> Show Answer <p>The correct answer is B. Working memory can only hold about 4-7 chunks of information at once, and even that's optimistic for complex material. This isn't a flaw\u2014it's a feature. Working memory is designed for active processing, not storage. Information also fades within 15-30 seconds without rehearsal.</p> <p>Concept Tested: Working Memory</p> <p>See: Chapter Content</p>"},{"location":"chapters/07-cognitive-load-visual-design/quiz/#2-what-are-schemas-in-the-context-of-cognitive-load-theory","title":"2. What are schemas in the context of cognitive load theory?","text":"<ol> <li>Visual layouts for organizing MicroSim interfaces</li> <li>Organized knowledge structures in long-term memory that help make sense of new information</li> <li>Database structures for storing user data</li> <li>Step-by-step procedures for completing tasks</li> </ol> Show Answer <p>The correct answer is B. Schemas are organized knowledge structures in long-term memory\u2014mental frameworks that help you make sense of new information. They act like the brain's compression algorithm. Instead of remembering every individual detail, you remember patterns and relationships. Schemas reduce cognitive load because experts process information more efficiently through chunking.</p> <p>Concept Tested: Schema Formation</p> <p>See: Chapter Content</p>"},{"location":"chapters/07-cognitive-load-visual-design/quiz/#3-which-type-of-cognitive-load-should-designers-minimize-ruthlessly","title":"3. Which type of cognitive load should designers minimize ruthlessly?","text":"<ol> <li>Intrinsic load</li> <li>Extraneous load</li> <li>Germane load</li> <li>Mental effort</li> </ol> Show Answer <p>The correct answer is B. Extraneous load is the cognitive effort wasted on things that don't contribute to learning\u2014clutter, confusing layouts, poor instructions, unnecessary complexity. It's entirely under the designer's control and should be minimized ruthlessly. Every bit of extraneous load eliminated gives learners more mental budget for actual learning.</p> <p>Concept Tested: Extraneous Load</p> <p>See: Chapter Content</p>"},{"location":"chapters/07-cognitive-load-visual-design/quiz/#4-what-is-the-split-attention-effect","title":"4. What is the split attention effect?","text":"<ol> <li>When learners try to multitask on two unrelated activities</li> <li>When learners must mentally integrate information that is physically or temporally separated</li> <li>When the screen is divided into too many panels</li> <li>When learners switch between different devices</li> </ol> Show Answer <p>The correct answer is B. The split attention effect occurs when learners must mentally integrate multiple sources of information that are physically or temporally separated. For example, when a diagram is on the left and its explanation is in a separate pop-up on the right, learners waste cognitive resources searching for and connecting information instead of understanding it.</p> <p>Concept Tested: Split Attention Effect</p> <p>See: Chapter Content</p>"},{"location":"chapters/07-cognitive-load-visual-design/quiz/#5-what-is-the-primary-solution-to-the-split-attention-effect","title":"5. What is the primary solution to the split attention effect?","text":"<ol> <li>Use more colors to differentiate elements</li> <li>Add audio narration to explain everything</li> <li>Physical integration\u2014place explanations next to the elements they describe</li> <li>Reduce the total amount of content presented</li> </ol> Show Answer <p>The correct answer is C. The antidote to split attention is physical integration. Put the explanation next to the thing it explains. Put labels directly on elements, not in separate legends. Make connections visible so the learner's brain doesn't have to make them. Research shows split attention can reduce learning outcomes by 30-50%.</p> <p>Concept Tested: Split Attention Effect</p> <p>See: Chapter Content</p>"},{"location":"chapters/07-cognitive-load-visual-design/quiz/#6-what-is-progressive-disclosure-in-microsim-design","title":"6. What is progressive disclosure in MicroSim design?","text":"<ol> <li>Showing all features at once so users can explore freely</li> <li>Revealing information gradually rather than all at once</li> <li>Disclosing the source code to advanced users</li> <li>Progressively increasing animation speed</li> </ol> Show Answer <p>The correct answer is B. Progressive disclosure is a design strategy where information is revealed gradually rather than all at once. It reduces initial cognitive load, supports scaffolded learning, maintains engagement through the promise of \"more to discover,\" and enables differentiation so learners can go as deep as they need.</p> <p>Concept Tested: Progressive Disclosure</p> <p>See: Chapter Content</p>"},{"location":"chapters/07-cognitive-load-visual-design/quiz/#7-why-is-learner-control-important-for-animated-microsims","title":"7. Why is learner control important for animated MicroSims?","text":"<ol> <li>It makes the MicroSim look more professional</li> <li>It allows learners to adjust speed, pause, and replay to match their processing pace</li> <li>It reduces development costs</li> <li>It prevents the animation from running automatically</li> </ol> Show Answer <p>The correct answer is B. Learner control\u2014including play/pause, speed control, and step functions\u2014accommodates individual differences in processing speed, allows review of difficult sections, reduces anxiety, and supports self-regulated learning. Research shows most instructional animations are too fast for effective learning; learner control solves this problem.</p> <p>Concept Tested: Learner Control, Animation Speed</p> <p>See: Chapter Content</p>"},{"location":"chapters/07-cognitive-load-visual-design/quiz/#8-what-does-the-minimal-effective-design-principle-suggest","title":"8. What does the \"minimal effective design\" principle suggest?","text":"<ol> <li>Use the minimum number of colors possible</li> <li>Create the simplest interface that successfully supports the learning objective</li> <li>Minimize the file size of the MicroSim</li> <li>Use minimal text and rely primarily on visuals</li> </ol> Show Answer <p>The correct answer is B. The minimal effective design aims for the simplest interface, fewest features, and least complexity that successfully supports the learning objective. Every element beyond this carries risk of distraction, confusion, and increased cognitive load. Start minimal and add elements only when testing shows they're needed.</p> <p>Concept Tested: Design Tradeoffs</p> <p>See: Chapter Content</p>"},{"location":"chapters/07-cognitive-load-visual-design/quiz/#9-what-is-germane-load-and-why-is-it-important","title":"9. What is germane load, and why is it important?","text":"<ol> <li>The load caused by irrelevant decorative elements; it should be eliminated</li> <li>The cognitive effort dedicated to actual learning, including building schemas; it should be maximized</li> <li>The inherent difficulty of the material; it cannot be changed</li> <li>The mental effort spent navigating the interface; it should be reduced</li> </ol> Show Answer <p>The correct answer is B. Germane load is the cognitive effort dedicated to actual learning\u2014processing information deeply, making connections, building schemas, and achieving understanding. This is the \"good\" load that produces learning. The design goal is to minimize extraneous load so learners can invest maximum mental effort in germane load.</p> <p>Concept Tested: Germane Load</p> <p>See: Chapter Content</p>"},{"location":"chapters/07-cognitive-load-visual-design/quiz/#10-according-to-cognitive-load-theory-what-determines-intrinsic-load","title":"10. According to cognitive load theory, what determines intrinsic load?","text":"<ol> <li>The quality of the visual design</li> <li>Element interactivity (how many pieces must be processed simultaneously) and prior knowledge</li> <li>The number of animation frames per second</li> <li>How long the learner spends on the MicroSim</li> </ol> Show Answer <p>The correct answer is B. Intrinsic load is determined by two factors: element interactivity (how many pieces of information must be processed simultaneously) and prior knowledge (what schemas the learner already has). Intrinsic load is fixed for a given learner learning a given topic\u2014you can't make calculus inherently simpler without changing what calculus is. However, you can manage it through sequencing and building prerequisite schemas.</p> <p>Concept Tested: Intrinsic Load</p> <p>See: Chapter Content</p>"},{"location":"chapters/08-anticipating-misconceptions/","title":"Anticipating Misconceptions","text":""},{"location":"chapters/08-anticipating-misconceptions/#summary","title":"Summary","text":"<p>This chapter focuses on understanding and addressing learner misconceptions through MicroSim design. You will learn about mental models and how they form, identify common misconceptions by subject area, and understand the difference between misconception correction and reinforcement. The chapter covers productive failure approaches, prediction prompts, conceptual boundaries, conceptual change processes, intuition testing, and model comparison techniques. These strategies help you design simulations that effectively challenge and correct faulty mental models.</p>"},{"location":"chapters/08-anticipating-misconceptions/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 11 concepts from the learning graph:</p> <ol> <li>Misconception</li> <li>Common Misconceptions</li> <li>Misconception Correction</li> <li>Misconception Reinforcement</li> <li>Productive Failure</li> <li>Prediction Prompt</li> <li>Conceptual Boundary</li> <li>Mental Model</li> <li>Conceptual Change</li> <li>Intuition Testing</li> <li>Model Comparison</li> </ol>"},{"location":"chapters/08-anticipating-misconceptions/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 7: Cognitive Load and Visual Design</li> </ul>"},{"location":"chapters/08-anticipating-misconceptions/#introduction-the-beautiful-mess-inside-every-learners-head","title":"Introduction: The Beautiful Mess Inside Every Learner's Head","text":"<p>Here's a humbling truth that every educator eventually discovers: your learners are not blank slates. They walk into your classroom, open your textbook, or launch your MicroSim carrying a lifetime of ideas, experiences, and\u2014let's be honest\u2014some truly creative misunderstandings about how the world works.</p> <p>A child believes that heavy objects fall faster than light ones (they watched a feather and a bowling ball, after all). A business student thinks that \"sunk costs\" should factor into future decisions (they paid for that gym membership, darn it). A physics undergraduate imagines that moving objects have an internal \"impetus\" that gradually runs out (thanks, medieval philosophy).</p> <p>These aren't signs of stupidity\u2014they're signs of humanity. Our brains are pattern-matching machines that constantly build explanations for what we observe. Sometimes those explanations are brilliant. Sometimes they're... less brilliant. And here's the kicker: the wrong explanations often feel just as right as the correct ones.</p> <p>This chapter is about the art and science of misconception hunting. You'll learn to anticipate what learners probably believe before you even meet them, design MicroSims that expose flawed thinking without causing shame, and guide learners through the sometimes uncomfortable process of changing their minds. It's challenging work, but when you see that lightbulb moment\u2014when a learner truly gets it after years of getting it wrong\u2014you'll understand why misconception correction is one of the most rewarding aspects of instructional design.</p> <p>Let's start by understanding what we're working with.</p>"},{"location":"chapters/08-anticipating-misconceptions/#mental-models-the-invisible-architects-of-understanding","title":"Mental Models: The Invisible Architects of Understanding","text":"<p>Every learner carries around mental models\u2014internal representations of how things work. Think of a mental model as a personal simulation running in someone's head. When you ask \"what happens if I drop this ball?\", your mental model of gravity runs a quick simulation and predicts the outcome.</p> <p>Mental models are incredibly useful. They let us:</p> <ul> <li>Predict outcomes without conducting experiments</li> <li>Understand explanations by mapping them to existing knowledge</li> <li>Solve new problems by analogy to familiar ones</li> <li>Communicate complex ideas using shared frameworks</li> </ul> <p>The challenge is that mental models can be accurate, partially accurate, or wildly wrong\u2014and the person holding them often can't tell the difference. Your mental model of your phone probably includes \"tap the icon, app opens.\" That's accurate enough for daily use. Your mental model of how tapping an icon opens an app is probably a mix of reasonable guesses and pure fantasy (unless you're a mobile developer, in which case it's still at least 30% fantasy).</p>"},{"location":"chapters/08-anticipating-misconceptions/#how-mental-models-form","title":"How Mental Models Form","text":"<p>Mental models develop through several mechanisms:</p> Source Example Risk Level for Misconceptions Direct experience \"Fire is hot\" (touched stove once) Low\u2014usually accurate Observation \"Heavy things sink\" (saw rocks sink) Medium\u2014limited sample Instruction \"Atoms are like tiny solar systems\" Medium\u2014depends on teacher Analogy \"Electricity flows like water\" High\u2014analogies have limits Intuition \"Objects need force to keep moving\" High\u2014intuition often wrong Cultural transmission \"We only use 10% of our brains\" Very high\u2014myths spread easily <p>The problem isn't that learners have mental models\u2014that's inevitable and necessary. The problem is that some mental models actively interfere with learning correct concepts. You can't simply add new information on top of a faulty foundation; you have to renovate the foundation first.</p> <p>Design Insight</p> <p>Before designing any MicroSim, ask yourself: \"What do learners probably already believe about this topic?\" The answer will shape everything from your visual design to your interaction patterns.</p>"},{"location":"chapters/08-anticipating-misconceptions/#diagram-mental-model-formation-and-influence","title":"Diagram: Mental Model Formation and Influence","text":"Mental Model Formation and Influence <p>Type: diagram</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Help learners visualize how mental models form from various sources and influence the interpretation of new information</p> <p>Components to show: - Central hub: \"Mental Model\" (brain/thought bubble icon) - Input sources radiating inward (6 spokes):   - Direct Experience (hand icon)   - Observation (eye icon)   - Instruction (teacher icon)   - Analogy (link/chain icon)   - Intuition (lightbulb icon)   - Cultural Transmission (people/group icon) - Output arrows showing influence on:   - Predictions   - Interpretations   - Problem-solving approaches   - Communication - Filter/lens between \"New Information\" input and mental model showing how existing models filter incoming data - Feedback loop showing how predictions/outcomes can modify mental models</p> <p>Visual style: Radial diagram with central hub and spokes, flow arrows showing information direction</p> <p>Color scheme: - Blue for input sources - Green for mental model (central) - Orange for outputs/behaviors - Red dotted line for filtering effect on new information</p> <p>Interactive elements: - Hover over each source to see examples and risk level - Click to see how that source contributed to a common misconception - Animation showing information flowing through the filter</p> <p>Implementation: HTML/CSS/JavaScript with SVG diagram</p>"},{"location":"chapters/08-anticipating-misconceptions/#misconceptions-when-mental-models-go-wrong","title":"Misconceptions: When Mental Models Go Wrong","text":"<p>A misconception is a mental model that conflicts with the accepted scientific or expert understanding of a concept. It's not just \"not knowing\"\u2014it's \"knowing something that isn't so.\" As Mark Twain (allegedly) said, \"It ain't what you don't know that gets you into trouble. It's what you know for sure that just ain't so.\"</p> <p>Misconceptions are particularly tricky because:</p> <ul> <li>They often produce correct predictions in everyday situations</li> <li>They feel intuitively right (that's why people hold them)</li> <li>They're resistant to simple correction (telling someone they're wrong rarely works)</li> <li>They can coexist with correct knowledge without apparent contradiction</li> <li>They regenerate even after being \"corrected\"</li> </ul> <p>Consider the classic misconception that \"seasons are caused by Earth's distance from the sun.\" This feels sensible\u2014closer to the fire means warmer, right? The mental model works for most heat sources we encounter. But it fails spectacularly when you realize that it's summer in Australia while it's winter in North America. If distance caused seasons, the whole planet would be warm or cold together.</p>"},{"location":"chapters/08-anticipating-misconceptions/#the-anatomy-of-a-misconception","title":"The Anatomy of a Misconception","text":"<p>Every misconception has a structure:</p> <ol> <li>The flawed core belief: The central incorrect idea</li> <li>Supporting evidence: Real observations that seem to confirm it</li> <li>Resistance mechanisms: Ways the misconception protects itself from correction</li> <li>Boundary conditions: Situations where it fails (often not encountered)</li> </ol> <p>Understanding this structure helps you design interventions. You can't just attack the flawed belief directly\u2014you need to undermine the supporting evidence, expose the boundary conditions, and provide a better alternative that explains everything the misconception explained plus more.</p>"},{"location":"chapters/08-anticipating-misconceptions/#common-misconceptions-a-field-guide-to-faulty-thinking","title":"Common Misconceptions: A Field Guide to Faulty Thinking","text":"<p>Common misconceptions are incorrect beliefs that appear repeatedly across learners, often due to shared experiences, intuitive reasoning patterns, or widespread misinformation. Knowing the common misconceptions in your subject area is like having a map of the minefield\u2014you can navigate more safely.</p>"},{"location":"chapters/08-anticipating-misconceptions/#physics-misconceptions","title":"Physics Misconceptions","text":"<p>Physics is a goldmine of misconceptions because everyday intuition developed in a friction-filled, air-resistant world doesn't prepare us for idealized physics:</p> Misconception Why It Feels Right The Reality Heavier objects fall faster Feathers fall slower than rocks In a vacuum, all objects fall at the same rate Objects need force to keep moving Things stop when you stop pushing Objects in motion stay in motion (Newton's 1st Law) Cold flows into warm objects Opening the fridge \"lets cold out\" Heat flows from hot to cold; cold isn't a substance Current gets \"used up\" in a circuit Bulbs near the battery seem brighter Current is conserved; voltage drops across components"},{"location":"chapters/08-anticipating-misconceptions/#biology-misconceptions","title":"Biology Misconceptions","text":"Misconception Why It Feels Right The Reality Evolution is goal-directed Species seem to \"want\" to adapt Evolution is blind; variation is random, selection isn't Humans evolved from chimps We share ancestry with apes We share a common ancestor; chimps evolved too Respiration only happens in lungs We \"breathe\" with our lungs Cellular respiration occurs in every cell"},{"location":"chapters/08-anticipating-misconceptions/#mathematics-misconceptions","title":"Mathematics Misconceptions","text":"Misconception Why It Feels Right The Reality Multiplication always makes bigger 3 \u00d7 4 = 12, which is bigger 0.5 \u00d7 4 = 2 (multiplication by fractions &lt; 1) More digits = larger number 1000 looks bigger than 999 0.125 &lt; 0.2 despite more digits Variables are specific unknowns \"Find x\" in early algebra Variables can represent any value in a set"},{"location":"chapters/08-anticipating-misconceptions/#businesseconomics-misconceptions","title":"Business/Economics Misconceptions","text":"Misconception Why It Feels Right The Reality Sunk costs matter for decisions \"I already invested so much\" Only future costs/benefits matter Correlation implies causation Ice cream sales and drowning correlate Both increase in summer (confounding variable) Good products sell themselves \"Build it and they will come\" Marketing, distribution, and timing matter enormously <p>Subject-Specific Research</p> <p>For any topic you're designing MicroSims for, search the research literature for \"common misconceptions in [topic].\" Decades of education research have cataloged misconceptions across nearly every domain.</p>"},{"location":"chapters/08-anticipating-misconceptions/#diagram-misconception-catalog-by-domain","title":"Diagram: Misconception Catalog by Domain","text":"Interactive Misconception Catalog <p>Type: infographic</p> <p>Bloom Taxonomy: Remember (L1)</p> <p>Learning Objective: Provide a browsable reference of common misconceptions organized by subject domain to help designers anticipate learner beliefs</p> <p>Layout: Tabbed interface with domain categories, searchable</p> <p>Domain tabs: - Physics - Biology - Chemistry - Mathematics - Economics/Business - Computer Science - Psychology - Earth Science</p> <p>For each domain, display cards showing: - Misconception statement (bold) - Why it feels intuitive (italic) - The correct concept - Percentage of students holding this misconception (where research available) - Age/level where most common - Tags for related concepts</p> <p>Interactive features: - Search bar to find misconceptions by keyword - Filter by grade level (K-5, 6-8, 9-12, College) - Sort by prevalence or difficulty to correct - \"Related misconceptions\" links between cards - Click card to expand with detailed explanation and suggested interventions - \"Add to my list\" feature for designers building MicroSims</p> <p>Visual design: - Clean card-based layout - Color-coded by domain - Icons indicating difficulty to correct (easy/medium/hard) - Research citation links where available</p> <p>Data source: Synthesized from education research literature</p> <p>Implementation: HTML/CSS/JavaScript with searchable/filterable card interface</p>"},{"location":"chapters/08-anticipating-misconceptions/#misconception-reinforcement-when-good-intentions-go-bad","title":"Misconception Reinforcement: When Good Intentions Go Bad","text":"<p>Here's a sobering thought: it's possible to design a MicroSim that actually strengthens misconceptions instead of correcting them. Misconception reinforcement occurs when instructional materials inadvertently confirm or entrench incorrect beliefs.</p> <p>This happens more often than you'd think:</p>"},{"location":"chapters/08-anticipating-misconceptions/#ways-microsims-can-reinforce-misconceptions","title":"Ways MicroSims Can Reinforce Misconceptions","text":"<ol> <li> <p>Using flawed analogies without limits: \"Electricity is like water flowing through pipes\" reinforces the idea that current gets \"used up\" as it flows.</p> </li> <li> <p>Simplified visualizations: Showing electrons as tiny balls orbiting a nucleus reinforces the \"planetary model\" that breaks down at the quantum level.</p> </li> <li> <p>Allowing misconception-consistent predictions: If a simulation lets learners predict that heavier objects fall faster, and the scenario doesn't clearly contradict this (maybe air resistance is involved), the misconception survives.</p> </li> <li> <p>Confirmation bias design: If learners only see examples that fit their mental model, they never encounter the contradictions that would force revision.</p> </li> <li> <p>Premature correct answers: Telling learners the right answer before they've grappled with why their intuition was wrong just creates a surface layer of \"right\" over an unchanged misconception.</p> </li> </ol> Reinforcement Pattern Example Why It Happens Comfortable confirmation Only showing falling objects with air resistance Avoids the counterintuitive vacuum case Oversimplified models Bohr atom model taught as \"reality\" Easy to visualize, hard to un-teach Missing contradictions Economics examples where correlation = causation Cherry-picked data supports wrong inference Superficial correction \"Heavy objects don't fall faster\" without demonstration Verbal correction doesn't change mental model <p>The Curse of Expertise</p> <p>As a subject matter expert, you may not even realize which simplifications are dangerous. What seems like a \"helpful analogy\" to you might be creating misconceptions in learners. Always test your MicroSims with actual novices.</p>"},{"location":"chapters/08-anticipating-misconceptions/#misconception-correction-the-art-of-changing-minds","title":"Misconception Correction: The Art of Changing Minds","text":"<p>Misconception correction is the process of helping learners replace incorrect mental models with accurate ones. This is harder than it sounds\u2014you're not filling an empty vessel, you're renovating a building while people are living in it.</p> <p>Effective misconception correction follows a general pattern:</p>"},{"location":"chapters/08-anticipating-misconceptions/#the-correction-cycle","title":"The Correction Cycle","text":"<ol> <li>Activate the misconception: Get learners to articulate their current belief</li> <li>Create cognitive conflict: Show a situation where the misconception fails</li> <li>Provide a better alternative: Offer a correct model that explains more</li> <li>Consolidate the change: Practice using the new model</li> </ol> <p>This isn't a one-shot process. Misconceptions are like weeds\u2014they grow back. Effective correction requires repeated exposure to cognitive conflict and consistent reinforcement of the correct model.</p>"},{"location":"chapters/08-anticipating-misconceptions/#why-simple-telling-doesnt-work","title":"Why Simple Telling Doesn't Work","text":"<p>Research consistently shows that simply telling learners the correct information is ineffective for misconception correction. The reasons include:</p> <ul> <li>Misconceptions are stored in long-term memory with strong associations</li> <li>New information gets filtered through existing mental models</li> <li>Without cognitive conflict, there's no motivation to change</li> <li>Verbal knowledge and intuitive mental models can coexist independently</li> </ul> <p>A learner might correctly answer \"Do heavier objects fall faster?\" with \"No\" on a test while still intuitively expecting a bowling ball to beat a marble to the ground. The verbal knowledge and the mental model haven't integrated.</p>"},{"location":"chapters/08-anticipating-misconceptions/#diagram-misconception-correction-cycle","title":"Diagram: Misconception Correction Cycle","text":"Misconception Correction Cycle <p>Type: workflow</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Provide a clear framework for the misconception correction process that designers can follow when creating MicroSims</p> <p>Visual style: Circular workflow diagram with four main stages</p> <p>Stages:</p> <p>Stage 1 - \"Activate\": - Icon: Lightbulb with question mark - Description: \"Surface the existing belief\" - Actions: Ask prediction questions, have learner commit to an answer - Hover text: \"Don't skip this! Learners must explicitly hold the misconception before it can be challenged.\"</p> <p>Stage 2 - \"Conflict\": - Icon: Lightning bolt / collision - Description: \"Create cognitive dissonance\" - Actions: Show evidence that contradicts prediction, demonstrate boundary failure - Hover text: \"The misconception must visibly FAIL. Learners must feel the contradiction.\"</p> <p>Stage 3 - \"Resolve\": - Icon: Puzzle pieces connecting - Description: \"Provide better alternative\" - Actions: Introduce correct model, show how it explains everything including the conflict - Hover text: \"The new model must explain MORE than the old one, not just be 'correct.'\"</p> <p>Stage 4 - \"Consolidate\": - Icon: Checkmark with repeat arrow - Description: \"Practice and reinforce\" - Actions: Apply new model to multiple examples, revisit potential misconception triggers - Hover text: \"Misconceptions regrow! Regular reinforcement prevents relapse.\"</p> <p>Center of cycle: - \"Mental Model\" text - Arrows showing cyclical nature - Note: \"May require multiple cycles\"</p> <p>Connections between stages: - Arrow from Activate to Conflict: \"Commitment made\" - Arrow from Conflict to Resolve: \"Dissonance created\" - Arrow from Resolve to Consolidate: \"Alternative accepted\" - Arrow from Consolidate back to Activate: \"Test with new contexts\"</p> <p>Color scheme: - Yellow for Activate (illumination) - Red for Conflict (tension) - Green for Resolve (solution) - Blue for Consolidate (stability) - Arrows in gray showing flow</p> <p>Implementation: HTML/CSS/JavaScript with animated circular workflow</p>"},{"location":"chapters/08-anticipating-misconceptions/#conceptual-change-the-deep-restructuring-of-understanding","title":"Conceptual Change: The Deep Restructuring of Understanding","text":"<p>Conceptual change is the cognitive process by which learners fundamentally restructure their understanding of a concept. It's not just adding new information\u2014it's reorganizing the mental furniture.</p> <p>Conceptual change theory, developed by educational researchers like Posner, Strike, Hewson, and Gertzog in the 1980s, identifies four conditions necessary for conceptual change to occur:</p> <ol> <li>Dissatisfaction: The learner must be dissatisfied with their current conception</li> <li>Intelligibility: The new conception must be understandable</li> <li>Plausibility: The new conception must seem potentially true</li> <li>Fruitfulness: The new conception must be useful for solving problems</li> </ol> <p>All four conditions must be met. Miss one, and conceptual change stalls:</p> Missing Condition Result No dissatisfaction \"My old way works fine, why change?\" Not intelligible \"I don't even understand what you're saying\" Not plausible \"That can't be right, it doesn't make sense\" Not fruitful \"Okay, but when would I ever use this?\""},{"location":"chapters/08-anticipating-misconceptions/#types-of-conceptual-change","title":"Types of Conceptual Change","text":"<p>Not all conceptual change is equally dramatic:</p> <ul> <li>Enrichment: Adding details to an existing conception (easiest)</li> <li>Revision: Modifying part of a conception while keeping the structure</li> <li>Restructuring: Fundamentally reorganizing how concepts relate (hardest)</li> </ul> <p>A learner who thinks \"plants get food from soil\" might go through:</p> <ol> <li>Enrichment: \"Plants get food from soil AND water\"</li> <li>Revision: \"Plants get some nutrients from soil but make their own food\"</li> <li>Restructuring: \"Plants are autotrophs that produce food through photosynthesis using light energy, carbon dioxide, and water\"</li> </ol> <p>MicroSims are particularly powerful for triggering restructuring-level change because they can make abstract processes visible and manipulable.</p>"},{"location":"chapters/08-anticipating-misconceptions/#conceptual-boundaries-knowing-the-limits-of-models","title":"Conceptual Boundaries: Knowing the Limits of Models","text":"<p>Every mental model has conceptual boundaries\u2014situations where the model applies and situations where it breaks down. Expert understanding includes knowing these boundaries. Novice misconceptions often result from over-applying a model beyond its valid range.</p> <p>Consider the \"electricity is like water\" analogy:</p> Concept Water Analogy Boundary/Limitation Current Flow rate Works well Voltage Pressure Works reasonably Resistance Pipe narrowness Works reasonably Capacitance Tank storage Starts to strain Inductance ? Breaks down completely AC behavior ? Breaks down completely <p>The analogy is useful up to a point, then becomes a liability. Expert instructors explicitly teach these boundaries: \"The water analogy helps for basic circuits, but don't expect it to explain everything.\"</p>"},{"location":"chapters/08-anticipating-misconceptions/#teaching-boundary-awareness","title":"Teaching Boundary Awareness","text":"<p>Effective MicroSims can build boundary awareness by:</p> <ol> <li>Showing where models work: Reinforce the valid application range</li> <li>Explicitly demonstrating failures: Show cases where the model breaks down</li> <li>Introducing better models: When learners hit boundaries, provide the next-level model</li> <li>Comparing models: Show multiple models and their respective boundaries</li> </ol> <p>Progressive Model Disclosure</p> <p>Consider designing MicroSims that start with simpler models and progressively reveal their limitations. When learners experience the boundary, they're motivated to learn the more sophisticated model.</p>"},{"location":"chapters/08-anticipating-misconceptions/#diagram-model-boundaries-visualization","title":"Diagram: Model Boundaries Visualization","text":"Model Boundaries Visualization <p>Type: microsim</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Help learners understand that all models have boundaries of applicability by allowing them to explore where different models succeed and fail</p> <p>Canvas layout: - Width: Responsive to container - Left side (65%): Interactive model testing area - Right side (35%): Model selection and boundary indicators</p> <p>Concept domain: Projectile motion (multiple models)</p> <p>Available models (selectable): 1. \"Objects fall straight down\" (naive model) 2. \"Parabolic path, no air resistance\" (introductory physics) 3. \"Parabolic with linear air resistance\" (intermediate) 4. \"Full aerodynamic model\" (advanced)</p> <p>Testing area features: - Adjustable projectile launcher (angle, initial velocity) - Selectable projectile types: baseball, feather, bullet, basketball - \"Launch\" button to see trajectory - Toggle to show multiple model predictions simultaneously - Overlay showing where models diverge</p> <p>Parameters: - Launch angle: 0-90 degrees slider - Initial velocity: 1-100 m/s slider - Projectile mass: dropdown selection - Air density: slider (vacuum to dense atmosphere)</p> <p>Visual elements: - Trajectory paths in different colors per model - \"Prediction zone\" showing where models agree (green) vs. disagree (red/yellow) - Actual \"correct\" trajectory (dashed line, only revealed after prediction) - Landing point markers for each model - Distance/time displays</p> <p>Boundary indicators panel: - Traffic light indicator for each model (green/yellow/red) - Text explanation of why model works or fails for current parameters - \"Boundary condition\" warnings when approaching model limits</p> <p>Interactive features: - Challenge mode: \"Find parameters where Model X fails\" - Comparison mode: Show all models simultaneously - Hover over divergence points to see explanation - Record and replay trajectories</p> <p>Default parameters: - Model: \"Parabolic path, no air resistance\" - Projectile: Baseball - Angle: 45 degrees - Velocity: 20 m/s - Air density: Normal atmosphere</p> <p>Color scheme: - Blue: Naive model trajectory - Green: No-air-resistance model - Orange: Linear air resistance model - Purple: Full aerodynamic model - Dashed black: \"Actual\" result</p> <p>Implementation: p5.js simulation with physics calculations</p>"},{"location":"chapters/08-anticipating-misconceptions/#productive-failure-embracing-the-struggle","title":"Productive Failure: Embracing the Struggle","text":"<p>Productive failure is a learning design approach where learners are deliberately allowed (even encouraged) to struggle with problems before receiving instruction. It sounds counterintuitive\u2014why let people fail?\u2014but research shows it produces deeper learning than direct instruction alone.</p> <p>The key word is \"productive.\" Not all failure is productive. Productive failure has specific characteristics:</p>"},{"location":"chapters/08-anticipating-misconceptions/#characteristics-of-productive-failure","title":"Characteristics of Productive Failure","text":"<ol> <li>The problem is challenging but not impossible: Learners can make meaningful attempts</li> <li>Prior knowledge is activated: The struggle engages what learners already know</li> <li>Multiple solution paths exist: Different approaches can be compared</li> <li>Failure reveals gaps: The specific ways attempts fail are informative</li> <li>Instruction follows failure: The struggle is eventually resolved with expert guidance</li> </ol>"},{"location":"chapters/08-anticipating-misconceptions/#why-productive-failure-works","title":"Why Productive Failure Works","text":"<p>When learners struggle first, several beneficial things happen:</p> <ul> <li>Prior knowledge activation: Learners discover what they do and don't know</li> <li>Attention focusing: The struggle highlights what's important to learn</li> <li>Gap awareness: Learners feel the need for the upcoming instruction</li> <li>Deeper encoding: When the solution arrives, it connects to activated knowledge</li> <li>Metacognitive development: Learners experience their own thinking processes</li> </ul> <p>Compare this to direct instruction first: learners receive the solution before they've felt the problem. They may memorize the solution without understanding why it's needed or how it addresses the challenges they would have encountered.</p>"},{"location":"chapters/08-anticipating-misconceptions/#productive-failure-in-microsim-design","title":"Productive Failure in MicroSim Design","text":"<p>MicroSims are ideal for productive failure because they provide a safe space to fail. A simulation doesn't judge. You can crash the virtual airplane a hundred times without anyone getting hurt\u2014and each crash teaches you something.</p> <p>Design patterns for productive failure in MicroSims:</p> Pattern Implementation Example Predict-observe-explain Ask for prediction before showing result \"Which ball will land first?\" Explore before explain Let learners manipulate before teaching Circuit sandbox before Ohm's Law Progressive hints Withhold full solution, offer graduated help Debugging challenge with hint system Multiple attempts Allow and track multiple tries Physics puzzle with attempt counter Solution comparison Show learner attempt vs. expert solution Side-by-side trajectory comparison <p>The Emotional Dimension</p> <p>Productive failure only works if learners feel safe failing. If failure feels shameful or high-stakes, learners disengage rather than struggle productively. MicroSims should celebrate attempts, not just successes.</p>"},{"location":"chapters/08-anticipating-misconceptions/#prediction-prompts-the-power-of-commitment","title":"Prediction Prompts: The Power of Commitment","text":"<p>A prediction prompt is a instructional technique that asks learners to predict an outcome before they observe it. It seems simple\u2014\"What do you think will happen?\"\u2014but this small intervention has outsized effects on learning.</p>"},{"location":"chapters/08-anticipating-misconceptions/#why-prediction-works","title":"Why Prediction Works","text":"<p>When learners make predictions:</p> <ol> <li>Existing beliefs are activated: You can't predict without using your mental model</li> <li>Commitment is created: Having stated a position, learners are invested in the outcome</li> <li>Attention is focused: Learners watch more carefully to see if they were right</li> <li>Surprise is amplified: When wrong, the contradiction is more salient</li> <li>Memory is enhanced: Prediction activates elaborative encoding</li> </ol> <p>Research consistently shows that prediction-then-observation produces better learning than observation alone, even when predictions are wrong. Especially when predictions are wrong, in fact\u2014those are the moments of productive cognitive conflict.</p>"},{"location":"chapters/08-anticipating-misconceptions/#implementing-prediction-prompts-in-microsims","title":"Implementing Prediction Prompts in MicroSims","text":"<p>Effective prediction prompts in MicroSims:</p> <ul> <li>Require commitment: Don't let learners skip the prediction</li> <li>Record predictions visibly: Show them their prediction alongside the result</li> <li>Allow for reasoning: Ask \"why do you think that?\" after the prediction</li> <li>Vary difficulty: Some predictions should be easy (builds confidence), some challenging</li> <li>Address common misconceptions: Design predictions that expose typical wrong beliefs</li> </ul>"},{"location":"chapters/08-anticipating-misconceptions/#diagram-prediction-prompt-interface-design","title":"Diagram: Prediction Prompt Interface Design","text":"Prediction Prompt Interface Design <p>Type: infographic</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Demonstrate best practices for designing prediction prompt interfaces in MicroSims</p> <p>Layout: Annotated mockup of an ideal prediction prompt sequence</p> <p>Sequence panels:</p> <p>Panel 1 - \"Setup\": - Shows scenario description - Clear visual of initial conditions - \"Predict what happens\" prompt prominently displayed - Example: Ball at top of ramp, another ball in projectile launcher</p> <p>Panel 2 - \"Prediction Input\": - Multiple choice options (common responses including misconception) - OR: Drawing/sketching tool for trajectory - OR: Slider/value selection - Confidence indicator: \"How sure are you?\" (1-5 scale) - \"Lock in prediction\" button (prevents changing after observation) - Example: \"Which ball lands first?\" with three options</p> <p>Panel 3 - \"Reasoning Capture\" (optional but valuable): - Text field: \"Why do you think this will happen?\" - Quick-select common reasons - Example: \"Because heavier objects fall faster\"</p> <p>Panel 4 - \"Observation\": - Simulation runs - Prediction shown alongside actual result - Clear visual comparison - Example: Both balls shown landing, prediction marker vs. actual landing</p> <p>Panel 5 - \"Reflection\": - \"Was your prediction correct?\" explicit acknowledgment - If wrong: \"What might explain the difference?\" - Connection to correct concept - Optional: Retry with new understanding</p> <p>Design annotations: - \"Don't allow skipping\u2014commitment is crucial\" - \"Show prediction during observation, not just after\" - \"Celebrate engagement, not just correctness\" - \"Track predictions for personalization\"</p> <p>Color scheme: - Blue for prompts/questions - Yellow for learner input areas - Green for observation/results - Orange for reflection</p> <p>Implementation: HTML/CSS mockup with annotations</p>"},{"location":"chapters/08-anticipating-misconceptions/#intuition-testing-probing-the-hidden-beliefs","title":"Intuition Testing: Probing the Hidden Beliefs","text":"<p>Intuition testing is a diagnostic technique that probes learners' implicit beliefs through carefully designed scenarios. Unlike direct questions (\"What do you believe about X?\"), intuition tests reveal what learners actually think by asking them to apply their knowledge to novel situations.</p> <p>The distinction matters because learners often hold both \"school knowledge\" and \"intuitive knowledge\" simultaneously. They can recite Newton's First Law while still expecting objects to slow down naturally. Intuition tests bypass the school knowledge to access the mental model actually being used.</p>"},{"location":"chapters/08-anticipating-misconceptions/#designing-effective-intuition-tests","title":"Designing Effective Intuition Tests","text":"<p>Good intuition tests share several features:</p> <ol> <li>Novel contexts: Use unfamiliar scenarios so learners can't rely on memorized answers</li> <li>Forced choice: Require selection among options that distinguish different mental models</li> <li>Distractor quality: Include options that would be correct if common misconceptions were true</li> <li>Explanation requirement: Ask learners to justify their choice</li> <li>Rapid administration: Intuitions are fast; slow, deliberate reasoning may override them</li> </ol>"},{"location":"chapters/08-anticipating-misconceptions/#examples-of-intuition-test-questions","title":"Examples of Intuition Test Questions","text":"<p>Physics (motion): A ball is released inside a circular tube and exits at point P. Which path will it follow?</p> <ul> <li>Option A: Continues in a curve (misconception: objects \"remember\" circular motion)</li> <li>Option B: Straight line tangent to circle (correct: Newton's First Law)</li> <li>Option C: Falls straight down (misconception: objects stop moving without force)</li> </ul> <p>Biology (evolution): A population of beetles lives on a brown forest floor. After 100 generations on a new green-colored floor, what happens?</p> <ul> <li>Option A: Beetles gradually turn green because they need to hide (misconception: Lamarckian/goal-directed)</li> <li>Option B: Green beetles survive better, so the population shifts green (correct: natural selection)</li> <li>Option C: Beetles choose to change color (misconception: intentional adaptation)</li> </ul> <p>Economics (sunk costs): You paid $100 for a concert ticket. On the night of the concert, you feel sick. A friend offers you a free ticket to a different event you'd enjoy more. What should you do?</p> <ul> <li>Option A: Go to the concert because you already paid (misconception: sunk cost fallacy)</li> <li>Option B: Go to the friend's event since you'd enjoy it more (correct: sunk costs are irrelevant)</li> <li>Option C: Stay home since you're sick (partially correct but avoids the core question)</li> </ul>"},{"location":"chapters/08-anticipating-misconceptions/#diagram-intuition-testing-microsim","title":"Diagram: Intuition Testing MicroSim","text":"Intuition Testing MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Allow learners to experience intuition testing and see how their intuitive responses compare to scientific understanding, revealing potential misconceptions</p> <p>Canvas layout: - Width: Responsive to container - Center: Scenario presentation area - Bottom: Response options and feedback area</p> <p>Test structure (multiple scenarios):</p> <p>Scenario 1 - Circular Motion: - Animation: Ball exits circular tube - Question: \"Draw/select the ball's path after exiting\" - Options: Curved path, straight tangent, straight down - Correct: Straight tangent - Common misconception: Curved path (circular impetus)</p> <p>Scenario 2 - Falling Objects: - Setup: Heavy and light ball at same height - Question: \"Which lands first?\" - Options: Heavy first, same time, light first - Correct: Same time (in vacuum; adjust for air resistance context) - Common misconception: Heavy first</p> <p>Scenario 3 - Current in Circuit: - Setup: Simple circuit with bulb and battery - Question: \"Is current the same at both sides of the bulb?\" - Options: Less after bulb, same, more after bulb - Correct: Same (current conservation) - Common misconception: Less after bulb (current \"used up\")</p> <p>Scenario 4 - Evolution: - Setup: Beetle population scenario - Question: \"Why did the population become mostly green?\" - Options: Beetles changed to survive, green beetles survived better, beetles chose to adapt - Correct: Green beetles survived better - Common misconception: Beetles changed intentionally</p> <p>Interface elements: - Progress indicator (scenario X of Y) - Timer (encourages intuitive rather than deliberate response) - Confidence slider before each answer - \"Lock in\" button - After answer: Animated demonstration of correct answer - Running score: Intuitive accuracy percentage</p> <p>Feedback features: - No judgment language (\"interesting\" not \"wrong\") - Explanation of why each misconception feels right - Link to deeper learning resources - Comparison to how others answered (percentages)</p> <p>Visual design: - Clean, distraction-free scenarios - Clear visual options - Smooth animations for demonstrations - Friendly, non-threatening aesthetic</p> <p>Tracking: - Store all responses for pattern analysis - Identify which misconceptions learner holds - Suggest targeted learning resources</p> <p>Implementation: p5.js with scenario-based state machine</p>"},{"location":"chapters/08-anticipating-misconceptions/#model-comparison-side-by-side-enlightenment","title":"Model Comparison: Side-by-Side Enlightenment","text":"<p>Model comparison is a technique that presents multiple mental models side by side, allowing learners to compare their explanatory power and see where each succeeds or fails. It's the cognitive equivalent of a product comparison chart\u2014except the \"products\" are ways of understanding the world.</p>"},{"location":"chapters/08-anticipating-misconceptions/#why-comparison-works","title":"Why Comparison Works","text":"<p>Comparing models is more effective than teaching only the \"correct\" model because:</p> <ol> <li>Misconceptions are addressed, not ignored: The incorrect model is explicitly considered</li> <li>Explanatory power becomes visible: Learners see what each model can and cannot explain</li> <li>Conditions of applicability become clear: When each model is appropriate emerges naturally</li> <li>Transfer is enhanced: Understanding multiple models builds flexible knowledge</li> <li>Metacognition develops: Learners think about thinking about the topic</li> </ol>"},{"location":"chapters/08-anticipating-misconceptions/#designing-model-comparison-activities","title":"Designing Model Comparison Activities","text":"<p>Effective model comparison requires:</p> Element Purpose Example Clear model articulation Each model must be explicitly stated \"Model A: Objects need force to keep moving\" Test cases Scenarios that differentiate models What happens to a hockey puck on ice? Success tracking Record where each model gets it right Model A predicts: stops. Model B predicts: continues. Failure exposure Show where each model breaks down \"Model A cannot explain satellite orbits\" Synthesis Help learners choose the best model \"Model B explains everything Model A does, plus more\""},{"location":"chapters/08-anticipating-misconceptions/#the-comparison-table-technique","title":"The Comparison Table Technique","text":"<p>A powerful visual tool is the model comparison table:</p> Phenomenon Naive Model Prediction Scientific Model Prediction Observation Dropped ball Falls Falls Falls Thrown ball Falls back Follows parabola Follows parabola Satellite Falls Orbits Orbits Moon ??? Orbits Earth Orbits <p>The naive model works for some cases, fails for others. The scientific model explains all cases consistently. Seeing this comparison makes the superiority of the scientific model undeniable.</p>"},{"location":"chapters/08-anticipating-misconceptions/#diagram-model-comparison-interactive-tool","title":"Diagram: Model Comparison Interactive Tool","text":"Model Comparison Interactive Tool <p>Type: microsim</p> <p>Bloom Taxonomy: Evaluate (L5)</p> <p>Learning Objective: Enable learners to actively compare multiple models against various test cases, building understanding of how scientific models are evaluated and selected</p> <p>Canvas layout: - Width: Responsive to container - Top: Scenario/test case selection - Middle: Side-by-side model prediction displays - Bottom: Comparison summary table</p> <p>Domain: Motion and forces (three models)</p> <p>Models available: 1. \"Impetus Model\" (medieval): Objects move because they contain impetus, which gradually depletes 2. \"Common Sense Model\" (naive physics): Objects need force to keep moving; heavy things fall faster 3. \"Newtonian Model\" (scientific): Objects in motion stay in motion; gravity accelerates all objects equally</p> <p>Test cases (selectable): - Ball dropped from tower - Ball thrown horizontally - Ball on frictionless ice - Projectile on Moon (no air) - Satellite in orbit - Pendulum swinging - Car after engine turned off</p> <p>For each test case, show: - Initial setup (visual) - Model 1 prediction (animated if applicable) - Model 2 prediction (animated) - Model 3 prediction (animated) - \"Actual\" result (real physics simulation) - Checkmarks/X marks for which models got it right</p> <p>Comparison table (builds as learner tests scenarios): - Rows: Test cases - Columns: Each model - Cells: Correct/Incorrect icons - Running score: \"Model X correct: Y of Z cases\"</p> <p>Interactive features: - \"Add your own test case\" option - Slider to slow down animations for comparison - \"Why did Model X fail here?\" expandable explanations - Side-by-side and overlay view options - Export comparison table as image/PDF</p> <p>Culminating question: - \"Based on your testing, which model best explains motion?\" - Learner selects and provides reasoning - Feedback on reasoning quality</p> <p>Visual design: - Distinct colors for each model's predictions - Clear visual distinction between prediction and observation - Transparent overlay when comparing trajectories - Score/summary panel always visible</p> <p>Implementation: p5.js simulation with multiple physics models</p>"},{"location":"chapters/08-anticipating-misconceptions/#putting-it-all-together-a-misconception-targeting-design-framework","title":"Putting It All Together: A Misconception-Targeting Design Framework","text":"<p>Let's synthesize everything we've learned into a practical framework for designing MicroSims that effectively address misconceptions.</p>"},{"location":"chapters/08-anticipating-misconceptions/#the-predict-framework","title":"The PREDICT Framework","text":"<p>When designing misconception-targeting MicroSims, follow the PREDICT framework:</p> <p>P - Probe existing beliefs Before designing, research what misconceptions learners typically hold about your topic. Use the literature, intuition tests, or preliminary interviews.</p> <p>R - Require prediction Build prediction prompts into your MicroSim. Don't let learners passively observe\u2014make them commit to an expected outcome first.</p> <p>E - Engineer cognitive conflict Design scenarios where misconceptions fail visibly. The failure should be obvious and undeniable, not subtle.</p> <p>D - Deliver the better model After conflict, provide the correct model. Show how it explains everything the misconception explained plus the cases where it failed.</p> <p>I - Integrate through comparison Use model comparison to solidify understanding. Help learners see why the new model is genuinely better, not just \"what the teacher wants.\"</p> <p>C - Consolidate with practice Provide multiple opportunities to apply the correct model. Revisit potential misconception triggers to ensure the new model sticks.</p> <p>T - Track and respond Monitor learner responses. Identify persistent misconceptions and adjust your approach. One size doesn't fit all.</p>"},{"location":"chapters/08-anticipating-misconceptions/#the-misconception-targeting-microsim-checklist","title":"The Misconception-Targeting MicroSim Checklist","text":"<p>Before deploying any MicroSim aimed at misconceptions:</p> <ul> <li>[ ] Have you identified the specific misconception(s) being targeted?</li> <li>[ ] Does the MicroSim include prediction prompts that activate the misconception?</li> <li>[ ] Is there a scenario where the misconception demonstrably fails?</li> <li>[ ] Is the correct model presented clearly after the conflict?</li> <li>[ ] Does the comparison make the correct model's superiority evident?</li> <li>[ ] Are there multiple scenarios to consolidate learning?</li> <li>[ ] Is failure treated as productive, not shameful?</li> <li>[ ] Does the MicroSim track responses for personalization?</li> </ul>"},{"location":"chapters/08-anticipating-misconceptions/#summary-your-misconception-hunting-toolkit","title":"Summary: Your Misconception-Hunting Toolkit","text":"<p>Congratulations\u2014you're now equipped to be a cognitive detective, hunting down the misconceptions that lurk in learners' minds and designing interventions that actually change how people think. That's powerful stuff.</p> <p>Here's what you've learned:</p> <p>Mental Models are the internal simulations learners use to understand the world. They form from experience, instruction, analogy, and intuition\u2014and they can be right, wrong, or somewhere in between.</p> <p>Misconceptions are mental models that conflict with expert understanding. They're not stupidity\u2014they're the result of pattern-matching brains making reasonable (but wrong) inferences.</p> <p>Common Misconceptions are predictable. Decades of research have cataloged what learners typically believe in every domain. Use this research to anticipate what you're working against.</p> <p>Misconception Reinforcement can happen accidentally. Well-meaning MicroSims can strengthen wrong beliefs if they're not carefully designed to challenge them.</p> <p>Misconception Correction requires more than telling. You need to activate the misconception, create cognitive conflict, provide a better alternative, and consolidate the change.</p> <p>Conceptual Change is deep restructuring. It requires dissatisfaction with the old model, intelligibility of the new model, plausibility of the new model, and fruitfulness of the new model.</p> <p>Conceptual Boundaries define where models apply. Expert understanding includes knowing the limits. Teach the boundaries explicitly.</p> <p>Productive Failure lets learners struggle before instruction. The struggle activates prior knowledge, creates attention, and deepens encoding when the solution arrives.</p> <p>Prediction Prompts force commitment and amplify surprise. Always ask learners what they expect before showing them what happens.</p> <p>Intuition Testing reveals hidden beliefs. Use novel scenarios and forced choices to bypass \"school knowledge\" and access actual mental models.</p> <p>Model Comparison makes explanatory power visible. Side-by-side comparison helps learners see why the scientific model is genuinely better.</p> <p>When you design MicroSims with these principles in mind, you're not just teaching content\u2014you're changing minds. You're helping learners replace flawed understanding with accurate models that will serve them for life.</p> <p>The world is full of misconceptions about climate change, economics, health, and countless other topics that matter. Every MicroSim you design that successfully corrects a misconception is a small step toward a more informed society.</p> <p>Now go forth and challenge some cherished but incorrect beliefs. Just remember to do it gently\u2014changing minds is delicate work.</p>"},{"location":"chapters/08-anticipating-misconceptions/#references-and-further-reading","title":"References and Further Reading","text":"<p>For those wanting to dive deeper into misconception research and conceptual change theory:</p> <ul> <li>Posner, G. J., Strike, K. A., Hewson, P. W., &amp; Gertzog, W. A. (1982). Accommodation of a scientific conception: Toward a theory of conceptual change. Science Education, 66(2), 211-227.</li> <li>Vosniadou, S. (Ed.). (2008). International Handbook of Research on Conceptual Change. Routledge.</li> <li>Kapur, M. (2008). Productive failure. Cognition and Instruction, 26(3), 379-424.</li> <li>Chi, M. T. H. (2008). Three types of conceptual change: Belief revision, mental model transformation, and categorical shift. In S. Vosniadou (Ed.), International Handbook of Research on Conceptual Change (pp. 61-82). Routledge.</li> <li>Driver, R., Squires, A., Rushworth, P., &amp; Wood-Robinson, V. (1994). Making Sense of Secondary Science: Research into Children's Ideas. Routledge.</li> </ul> Test Your Understanding: Diagnose this design <p>A MicroSim teaches that \"warm air rises.\" It shows a candle heating air, which then visibly moves upward. Learners can adjust the flame size and watch bigger flames push more air up. What potential problem does this design have?</p> <p>Answer: This design may reinforce the misconception that warm air rises because it's \"pushed up\" by the heat source. The correct concept is that warm air is less dense than surrounding cool air and therefore becomes buoyant\u2014it's not being pushed. A better design would show the comparison with cool air, demonstrate the density difference, and perhaps include a vacuum chamber comparison where convection doesn't occur.</p>"},{"location":"chapters/08-anticipating-misconceptions/quiz/","title":"Quiz: Anticipating Misconceptions","text":"<p>Test your understanding of mental models, common misconceptions, and design strategies for helping learners correct faulty beliefs.</p>"},{"location":"chapters/08-anticipating-misconceptions/quiz/#1-what-is-a-mental-model","title":"1. What is a mental model?","text":"<ol> <li>A 3D physical replica used for teaching</li> <li>An internal representation of how things work that allows predictions and understanding</li> <li>A mathematical equation describing system behavior</li> <li>A diagram showing the relationships between concepts</li> </ol> Show Answer <p>The correct answer is B. A mental model is an internal representation of how things work\u2014a personal simulation running in someone's head. When you ask \"what happens if I drop this ball?\", your mental model of gravity runs a quick simulation and predicts the outcome. Mental models can be accurate, partially accurate, or wildly wrong\u2014and the person holding them often can't tell the difference.</p> <p>Concept Tested: Mental Model</p> <p>See: Chapter Content</p>"},{"location":"chapters/08-anticipating-misconceptions/quiz/#2-what-distinguishes-a-misconception-from-simply-not-knowing-something","title":"2. What distinguishes a misconception from simply \"not knowing\" something?","text":"<ol> <li>Misconceptions only occur in scientific subjects</li> <li>A misconception is a mental model that conflicts with accepted understanding\u2014\"knowing something that isn't so\"</li> <li>Misconceptions are easier to correct than knowledge gaps</li> <li>Misconceptions only develop in childhood</li> </ol> Show Answer <p>The correct answer is B. A misconception isn't just \"not knowing\"\u2014it's \"knowing something that isn't so.\" Misconceptions are particularly tricky because they often produce correct predictions in everyday situations, feel intuitively right, are resistant to simple correction, and can regenerate even after being \"corrected.\"</p> <p>Concept Tested: Misconception</p> <p>See: Chapter Content</p>"},{"location":"chapters/08-anticipating-misconceptions/quiz/#3-why-does-simply-telling-learners-the-correct-information-usually-fail-to-correct-misconceptions","title":"3. Why does simply telling learners the correct information usually fail to correct misconceptions?","text":"<ol> <li>Learners don't pay attention to verbal explanations</li> <li>Misconceptions are stored in long-term memory with strong associations, and new information gets filtered through existing mental models</li> <li>Verbal explanations are too complex for learners to understand</li> <li>Learners prefer to discover information themselves</li> </ol> Show Answer <p>The correct answer is B. Research consistently shows that simply telling learners the correct information is ineffective because misconceptions are stored in long-term memory with strong associations, new information gets filtered through existing mental models, there's no cognitive conflict to motivate change, and verbal knowledge can coexist with incorrect intuitions without integration.</p> <p>Concept Tested: Misconception Correction</p> <p>See: Chapter Content</p>"},{"location":"chapters/08-anticipating-misconceptions/quiz/#4-what-are-the-four-conditions-necessary-for-conceptual-change-according-to-the-theory-developed-by-posner-et-al","title":"4. What are the four conditions necessary for conceptual change according to the theory developed by Posner et al.?","text":"<ol> <li>Motivation, practice, feedback, and repetition</li> <li>Dissatisfaction, intelligibility, plausibility, and fruitfulness</li> <li>Observation, hypothesis, experiment, and conclusion</li> <li>Recognition, understanding, application, and analysis</li> </ol> Show Answer <p>The correct answer is B. Conceptual change requires: Dissatisfaction (the learner must be dissatisfied with their current conception), Intelligibility (the new conception must be understandable), Plausibility (the new conception must seem potentially true), and Fruitfulness (the new conception must be useful for solving problems). All four conditions must be met for conceptual change to occur.</p> <p>Concept Tested: Conceptual Change</p> <p>See: Chapter Content</p>"},{"location":"chapters/08-anticipating-misconceptions/quiz/#5-what-is-productive-failure-and-why-does-it-work","title":"5. What is productive failure, and why does it work?","text":"<ol> <li>Failing repeatedly until giving up, which motivates seeking help</li> <li>Deliberately allowing learners to struggle before receiving instruction, which activates prior knowledge and creates readiness for learning</li> <li>Providing incorrect answers to test learner attention</li> <li>Designing MicroSims that occasionally crash to test resilience</li> </ol> Show Answer <p>The correct answer is B. Productive failure is a learning design approach where learners are deliberately allowed to struggle with problems before receiving instruction. It works because the struggle activates prior knowledge, focuses attention on what's important to learn, creates gap awareness that motivates learning, produces deeper encoding when the solution arrives, and develops metacognition.</p> <p>Concept Tested: Productive Failure</p> <p>See: Chapter Content</p>"},{"location":"chapters/08-anticipating-misconceptions/quiz/#6-what-is-the-purpose-of-a-prediction-prompt-in-microsim-design","title":"6. What is the purpose of a prediction prompt in MicroSim design?","text":"<ol> <li>To test whether the MicroSim code is working correctly</li> <li>To make learners commit to an expected outcome before observation, which activates beliefs and amplifies learning from surprises</li> <li>To help the AI system understand what learners want</li> <li>To predict how long learners will spend on the MicroSim</li> </ol> Show Answer <p>The correct answer is B. Prediction prompts ask learners to predict an outcome before observing it. This activates existing beliefs, creates commitment so learners are invested in the outcome, focuses attention, and amplifies surprise when wrong. Research shows prediction-then-observation produces better learning than observation alone\u2014especially when predictions are wrong, as those are moments of productive cognitive conflict.</p> <p>Concept Tested: Prediction Prompt</p> <p>See: Chapter Content</p>"},{"location":"chapters/08-anticipating-misconceptions/quiz/#7-what-is-misconception-reinforcement-and-how-might-a-microsim-accidentally-cause-it","title":"7. What is misconception reinforcement, and how might a MicroSim accidentally cause it?","text":"<ol> <li>Strengthening correct understanding through repetition</li> <li>When instructional materials inadvertently confirm or entrench incorrect beliefs, such as through flawed analogies used without limits</li> <li>Providing extra practice for struggling learners</li> <li>Using positive reinforcement to encourage exploration</li> </ol> Show Answer <p>The correct answer is B. Misconception reinforcement occurs when instructional materials inadvertently confirm or entrench incorrect beliefs. MicroSims can cause this through using flawed analogies without explaining their limits, showing simplified visualizations that reinforce incorrect models, allowing misconception-consistent predictions without contradiction, and providing premature correct answers before learners grapple with why their intuition was wrong.</p> <p>Concept Tested: Misconception Reinforcement</p> <p>See: Chapter Content</p>"},{"location":"chapters/08-anticipating-misconceptions/quiz/#8-what-are-conceptual-boundaries-in-the-context-of-mental-models","title":"8. What are conceptual boundaries in the context of mental models?","text":"<ol> <li>The physical borders of a diagram or visualization</li> <li>The situations where a model applies and where it breaks down</li> <li>The maximum number of concepts a learner can understand</li> <li>The dividing lines between different academic subjects</li> </ol> Show Answer <p>The correct answer is B. Conceptual boundaries define the situations where a model applies and where it breaks down. Expert understanding includes knowing these boundaries. Novice misconceptions often result from over-applying a model beyond its valid range. For example, the \"electricity is like water\" analogy works for basic concepts but breaks down completely for inductance and AC behavior.</p> <p>Concept Tested: Conceptual Boundary</p> <p>See: Chapter Content</p>"},{"location":"chapters/08-anticipating-misconceptions/quiz/#9-what-is-the-purpose-of-model-comparison-in-addressing-misconceptions","title":"9. What is the purpose of model comparison in addressing misconceptions?","text":"<ol> <li>To show learners that all models are equally valid</li> <li>To present multiple models side by side so learners can compare their explanatory power and see where each succeeds or fails</li> <li>To help learners choose their favorite model based on aesthetics</li> <li>To demonstrate that science is always uncertain</li> </ol> Show Answer <p>The correct answer is B. Model comparison presents multiple mental models side by side, allowing learners to compare their explanatory power. This is more effective than teaching only the \"correct\" model because misconceptions are addressed rather than ignored, explanatory power becomes visible, conditions of applicability emerge naturally, and learners understand why the scientific model is genuinely better.</p> <p>Concept Tested: Model Comparison</p> <p>See: Chapter Content</p>"},{"location":"chapters/08-anticipating-misconceptions/quiz/#10-in-the-predict-framework-for-designing-misconception-targeting-microsims-what-does-the-e-stand-for","title":"10. In the PREDICT framework for designing misconception-targeting MicroSims, what does the \"E\" stand for?","text":"<ol> <li>Evaluate learner performance</li> <li>Engineer cognitive conflict</li> <li>Establish baseline knowledge</li> <li>Explain the correct answer</li> </ol> Show Answer <p>The correct answer is B. In the PREDICT framework, E stands for \"Engineer cognitive conflict.\" The full framework is: Probe existing beliefs, Require prediction, Engineer cognitive conflict (design scenarios where misconceptions fail visibly), Deliver the better model, Integrate through comparison, Consolidate with practice, and Track and respond to persistent misconceptions.</p> <p>Concept Tested: Misconception Correction</p> <p>See: Chapter Content</p>"},{"location":"chapters/09-generating-microsims-ai-tools/","title":"Generating MicroSims with AI Tools","text":""},{"location":"chapters/09-generating-microsims-ai-tools/#summary","title":"Summary","text":"<p>This chapter teaches you how to effectively use AI tools to generate MicroSims from your specifications. You will learn prompt engineering techniques for effective communication with AI systems, how to craft refinement prompts for iterative improvement, and understand the complete generation workflow. The chapter covers output interpretation, issue identification, making regeneration decisions versus manual adjustments, version control practices, and iteration management strategies. These skills enable you to collaborate effectively with AI tools throughout the MicroSim development process.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 9 concepts from the learning graph:</p> <ol> <li>Prompt Engineering</li> <li>Refinement Prompt</li> <li>Generation Workflow</li> <li>Output Interpretation</li> <li>Issue Identification</li> <li>Regeneration Decision</li> <li>Manual Adjustment</li> <li>Version Control</li> <li>Iteration Management</li> </ol>"},{"location":"chapters/09-generating-microsims-ai-tools/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 4: Visualization Libraries and Tools</li> <li>Chapter 5: Writing Effective MicroSim Specifications</li> </ul>"},{"location":"chapters/09-generating-microsims-ai-tools/#introduction-your-new-ai-collaborator","title":"Introduction: Your New AI Collaborator","text":"<p>Welcome to the chapter that will fundamentally change how you create educational content. You're about to learn how to collaborate with one of the most powerful tools ever created for instructional design: AI-powered code generation. And yes, \"collaborate\" is the right word\u2014this isn't about giving orders to a machine, it's about a creative partnership that combines human pedagogical expertise with AI's ability to generate code at superhuman speed.</p> <p>Think of AI as that incredibly talented colleague who can code circles around most humans, works 24/7 without coffee breaks, and never complains about last-minute changes. The catch? This colleague speaks a slightly different language and needs clear, specific instructions to do their best work. Learning to communicate effectively with AI tools is the new superpower of instructional design.</p> <p>The good news: if you've made it through the previous chapters on specifications and visualization libraries, you already have most of what you need. Writing effective AI prompts is, at its core, just writing effective specifications\u2014with a few extra tricks for getting the best results. This chapter will teach you those tricks, from the fundamentals of prompt engineering to advanced techniques like Claude Code Skills and hierarchical rule systems that maintain consistency across entire textbook ecosystems.</p> <p>Let's fire up the generation engine.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-generation-workflow-from-specification-to-simulation","title":"The Generation Workflow: From Specification to Simulation","text":"<p>Before diving into specific techniques, let's understand the complete generation workflow\u2014the end-to-end process of transforming your MicroSim specification into a working simulation. Understanding this workflow helps you know where you are at any point and what to do next.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-five-phases-of-microsim-generation","title":"The Five Phases of MicroSim Generation","text":"<ol> <li>Specification: You create a detailed specification (covered in Chapter 5)</li> <li>Initial Generation: AI produces the first version of code</li> <li>Evaluation: You assess the output against your specification</li> <li>Refinement: You iterate through improvements</li> <li>Finalization: You deploy the polished result</li> </ol> <p>Each phase has its own skills and decision points. The workflow isn't always linear\u2014you might jump back to specification if you realize something was missing, or skip refinement if the initial generation nails it.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#diagram-microsim-generation-workflow","title":"Diagram: MicroSim Generation Workflow","text":"MicroSim Generation Workflow <p>Type: workflow</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Visualize the complete generation workflow and understand the decision points at each phase</p> <p>Visual style: Flowchart with five main phases, decision diamonds, and feedback loops</p> <p>Phases (left to right):</p> <p>Phase 1 - \"Specification\": - Box: \"Write MicroSim Specification\" - Outputs: Learning objectives, visual design, interactions, parameters - Decision: \"Specification complete?\" \u2192 If no, return to start</p> <p>Phase 2 - \"Initial Generation\": - Box: \"Submit to AI Tool\" - Sub-steps: Select skill/model, include context, submit prompt - Output: Generated code (HTML, CSS, JavaScript)</p> <p>Phase 3 - \"Evaluation\": - Diamond: \"Output Assessment\" - Three paths:   - \"Works perfectly\" \u2192 Phase 5   - \"Minor issues\" \u2192 Phase 4 (Manual Adjustment)   - \"Major issues\" \u2192 Decision diamond</p> <p>Phase 3b - \"Issue Assessment\": - Diamond: \"Can prompt fix it?\" - Two paths:   - \"Yes\" \u2192 Phase 4 (Refinement Prompt)   - \"No\" \u2192 Back to Phase 1 (revise specification)</p> <p>Phase 4 - \"Refinement\": - Two parallel tracks:   - Track A: \"Refinement Prompt\" \u2192 Back to Phase 3   - Track B: \"Manual Adjustment\" \u2192 Phase 5 - Decision: \"Improvement sufficient?\" \u2192 If no, loop</p> <p>Phase 5 - \"Finalization\": - Box: \"Deploy &amp; Document\" - Sub-steps: Version control, testing, deployment - Output: Production-ready MicroSim</p> <p>Feedback loops shown: - From Evaluation back to Specification (major redesign) - From Refinement back to Evaluation (iterative improvement) - From Finalization back to Refinement (post-deployment fixes)</p> <p>Color scheme: - Blue for main process boxes - Yellow for decision diamonds - Green for success paths - Orange for iteration/rework paths - Purple for final output</p> <p>Annotations: - \"Most iterations happen here\" pointing to Phase 3-4 loop - \"Typical: 2-5 iterations\" as note - \"Don't skip this!\" pointing to Evaluation</p> <p>Implementation: HTML/CSS/JavaScript or Mermaid flowchart</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#time-distribution-across-phases","title":"Time Distribution Across Phases","text":"<p>Understanding where time goes helps you plan effectively:</p> Phase Typical Time What Takes Time Specification 30-40% Getting requirements right Initial Generation 5-10% AI works fast Evaluation 15-20% Testing and assessment Refinement 20-30% Iteration cycles Finalization 10-15% Polish and deployment <p>Notice that specification takes the most time\u2014and that's exactly right. Time invested in clear specifications pays dividends in faster generation and fewer iterations. Rushing the spec to \"let AI figure it out\" is a false economy that leads to frustrating refinement cycles.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#prompt-engineering-speaking-ais-language","title":"Prompt Engineering: Speaking AI's Language","text":"<p>Prompt engineering is the art and science of crafting inputs that elicit optimal outputs from AI systems. It's not quite programming, not quite writing\u2014it's a new discipline that sits at the intersection of clear communication, technical understanding, and a bit of psychology (yes, even for machines).</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-anatomy-of-an-effective-prompt","title":"The Anatomy of an Effective Prompt","text":"<p>Every effective MicroSim generation prompt contains these elements:</p> <ol> <li>Context: What kind of output you need</li> <li>Specification: The detailed requirements</li> <li>Constraints: Technical limitations and requirements</li> <li>Examples: Sample outputs or patterns to follow</li> <li>Format: How the output should be structured</li> </ol> <p>Let's break down each component:</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#context-setting","title":"Context Setting","text":"<p>Context tells the AI what role to play and what kind of output you need. This primes the AI's \"mental model\" for the task ahead.</p> <p>Weak context: <pre><code>Make a simulation about supply and demand.\n</code></pre></p> <p>Strong context: <pre><code>You are creating an educational MicroSim for college-level economics\nstudents. The simulation should demonstrate the relationship between\nsupply, demand, and market equilibrium using interactive controls\nand real-time visualization.\n</code></pre></p> <p>The strong context establishes:</p> <ul> <li>The educational purpose (not entertainment or research)</li> <li>The audience level (college)</li> <li>The subject domain (economics)</li> <li>The interaction model (interactive controls, real-time)</li> </ul>"},{"location":"chapters/09-generating-microsims-ai-tools/#specification-inclusion","title":"Specification Inclusion","text":"<p>Your MicroSim specification (from Chapter 5) forms the core of the prompt. Include all relevant details:</p> <ul> <li>Learning objectives</li> <li>Visual elements and layout</li> <li>Interactive controls</li> <li>Default parameters and ranges</li> <li>Behavioral descriptions</li> <li>Edge cases and error handling</li> </ul> <p>Don't Summarize\u2014Include</p> <p>When working with AI tools that accept long inputs, include your full specification rather than summarizing it. AI can process detailed specs better than you might expect, and detail reduces ambiguity.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#technical-constraints","title":"Technical Constraints","text":"<p>Explicitly state technical requirements and limitations without specifying how to write the code:</p> <p>Prompt</p> <p>Technical requirements:</p> <ul> <li>Use p5.js for rendering (no other graphics libraries)</li> <li>Generated JavaScript code in script.js must work in the p5.js editor without changes</li> <li>All JavaScript code in single script.js file</li> <li>Use a main.html consistent with the p5.js editor</li> <li>Responsive design using container width to account for window resize events</li> <li>No external dependencies except p5.js CDN</li> <li>Support window resize events</li> </ul> <p>These technical deployment constraints prevent the AI from making choices that don't fit your design or testing environment. Without these deployment constraints you might get beautiful code that uses libraries you can't deploy.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#pattern-examples","title":"Pattern Examples","text":"<p>Showing examples of desired output structure dramatically improves results:</p> <pre><code>The structure of your main.html code like this example:\n\n```html\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/p5@1.11.11/lib/p5.js\"&gt;&lt;/script&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\"&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n  &lt;/head&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;main&gt;\n        &lt;h1&gt;Title&lt;/h1&gt;\n        &lt;!-- p5.js canvas will be placed in this element--&gt;\n        &lt;main&gt;&lt;/main&gt;\n        &lt;script src=\"sketch.js\"&gt;&lt;/script&gt;\n    &lt;/main&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>You can open the p5.js editor, look for the \"&gt;\" icon to open the Sketch Files to see the p5.js standard template.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#output-format-instructions","title":"Output Format Instructions","text":"<p>Tell AI exactly how to deliver the output:</p> <pre><code>Output format:\n- Use the p5.js main.html template\n- Include all CSS in the style.css file\n- Include JavaScript in the sketch.js file\n- Add comments explaining key sections in the index.md file\n- Do not truncate or abbreviate the generated code\n</code></pre>"},{"location":"chapters/09-generating-microsims-ai-tools/#using-a-claude-code-microsim-skill","title":"Using a Claude Code MicroSim Skill","text":"<p>The prompt for generating a MicroSim using a claude code skill does All of the \"rules\" for generating a MicroSim are located within the Skill. Template files for the main.html, style.css, script.js, index.md and metadata.json are also located within the skill. You do not need any of the deployment information in your prompt. You can just specify what functionality you want the MicroSim to do.</p> <p>Prompt</p> <p>Use the microsim-generator skill to create a new microsim called <code>bouncing-ball</code>. The simulation will show a ball bouncing around the drawing area of the canvas. Allow the user to start and pause the simulation and change the ball's speed.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#diagram-prompt-engineering-best-practices","title":"Diagram: Prompt Engineering Best Practices","text":"Prompt Engineering Best Practices <p>Type: infographic</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Provide a quick reference for crafting effective MicroSim generation prompts</p> <p>Layout: Visual checklist with examples for each element</p> <p>Main sections (cards or panels):</p> <p>Section 1 - \"Context Setting\": - Icon: Stage/theater curtain - Checklist items:   - [ ] Educational purpose stated   - [ ] Audience level specified   - [ ] Subject domain identified   - [ ] Output type described - Good example vs. Bad example comparison - Impact meter: \"High impact on relevance\"</p> <p>Section 2 - \"Specification Depth\": - Icon: Blueprint/document - Checklist items:   - [ ] Learning objectives included   - [ ] Visual elements described   - [ ] Interactions specified   - [ ] Parameters with ranges   - [ ] Edge cases covered - Spectrum showing: \"Vague \u2192 Detailed\" - Impact meter: \"Highest impact on accuracy\"</p> <p>Section 3 - \"Technical Constraints\": - Icon: Gear with boundaries - Checklist items:   - [ ] Libraries/frameworks specified   - [ ] File structure requirements   - [ ] Dependency limitations   - [ ] Browser/platform targets   - [ ] Performance requirements - Impact meter: \"High impact on usability\"</p> <p>Section 4 - \"Examples &amp; Patterns\": - Icon: Template/pattern - Checklist items:   - [ ] Code structure example   - [ ] Naming conventions   - [ ] Style patterns   - [ ] Similar MicroSim references - Impact meter: \"Medium-high impact on consistency\"</p> <p>Section 5 - \"Output Format\": - Icon: Package/delivery - Checklist items:   - [ ] File format specified   - [ ] Completeness requirement   - [ ] Comment expectations   - [ ] No truncation instruction - Impact meter: \"Medium impact on usability\"</p> <p>Bottom summary: - \"Time spent on prompt\" vs \"Time saved in iterations\" graph showing positive ROI</p> <p>Color scheme: - Blue for section headers - Green for good examples - Red for bad examples - Yellow for impact meters</p> <p>Interactive features: - Click sections to expand with more examples - Hover over checklist items for tips</p> <p>Implementation: HTML/CSS/JavaScript card-based layout</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#claude-code-skills-intelligent-context-management","title":"Claude Code Skills: Intelligent Context Management","text":"<p>Now we enter territory that's particularly exciting for power users: Claude Code Skills. Skills are a mechanism for packaging specialized knowledge and procedures that Claude can invoke when needed. Understanding how skills work\u2014and how to create your own\u2014is the key to consistent, high-quality MicroSim generation at scale.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#how-skills-work-the-context-window-challenge","title":"How Skills Work: The Context Window Challenge","text":"<p>Every AI conversation has a context window\u2014the total amount of text the AI can \"see\" at once. For Claude, this is substantial but not unlimited. Here's the challenge: you might want Claude to know about hundreds of things (coding standards, design patterns, domain knowledge, project conventions), but you can't include all of that in every conversation.</p> <p>Skills solve this problem through intelligent context management:</p> <ol> <li>Skill Registration: You define up to 30 skills that Claude can access</li> <li>Summary Indexing: Each skill has a short summary (~100 tokens) that Claude sees in its context</li> <li>On-Demand Loading: When a skill is relevant, Claude loads the full skill content</li> <li>Context Efficiency: Only relevant knowledge occupies context space</li> </ol> <p>Think of it like a library. Claude doesn't carry every book at all times\u2014that would be impossible. Instead, Claude has a card catalog (the skill summaries) always available and retrieves full books (skill content) only when needed.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-30-skill-limit-strategic-choices","title":"The 30-Skill Limit: Strategic Choices","text":"<p>Why 30 skills? It's a balance. Each skill summary occupies context space even when the skill isn't invoked. With 30 skills at ~100 tokens each, that's about 3,000 tokens just for the skill index\u2014a meaningful but manageable portion of the context window.</p> <p>This limit forces strategic thinking about what skills to include:</p> Skill Category Examples Priority Core generation <code>microsim-generator</code>, <code>diagram-creator</code> Essential Domain-specific <code>physics-sim</code>, <code>economics-charts</code> High for relevant projects Quality assurance <code>accessibility-checker</code>, <code>code-reviewer</code> Medium-high Formatting <code>readme-generator</code>, <code>glossary-creator</code> Medium Utilities <code>file-organizer</code>, <code>dependency-checker</code> Lower"},{"location":"chapters/09-generating-microsims-ai-tools/#anatomy-of-a-skill","title":"Anatomy of a Skill","text":"<p>A well-designed skill contains:</p> <pre><code># Skill Name\n\n## Description\nBrief description for the skill summary index (~100 tokens max)\n\n## When to Use\nConditions that trigger this skill\n\n## Workflow\nStep-by-step procedure Claude follows\n\n## References\nFiles, templates, or examples the skill can access\n\n## Best Practices\nGuidelines for optimal results\n</code></pre> <p>The description is critical\u2014it's what appears in Claude's \"card catalog\" and determines whether the skill gets invoked.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#creating-your-own-microsim-skills","title":"Creating Your Own MicroSim Skills","text":"<p>Here's an example skill structure for MicroSim generation:</p> <pre><code># microsim-p5js-generator\n\n## Description\nGenerates p5.js-based educational MicroSims from specifications.\nCreates responsive, interactive simulations with standard controls,\nconsistent styling, and accessibility features. Use when user\nrequests a MicroSim or provides simulation specifications.\n\n## When to Use\n- User requests a MicroSim or simulation\n- User provides a detailed specification\n- User asks to create an interactive visualization\n- User needs a p5.js-based educational component\n\n## Workflow\n1. Verify specification completeness\n2. Load project style guidelines\n3. Generate HTML/CSS/JavaScript code\n4. Validate against accessibility requirements\n5. Test responsive behavior\n6. Provide code with documentation\n\n## References\n- /templates/microsim-template.html\n- /styles/microsim-standard.css\n- /docs/p5js-patterns.md\n</code></pre> <p>Skill Economy</p> <p>With only 30 skill slots, consider creating composite skills that handle related tasks rather than highly specialized single-purpose skills. A <code>microsim-generator</code> skill that handles multiple visualization types is often more valuable than separate skills for each type.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#diagram-claude-code-skills-architecture","title":"Diagram: Claude Code Skills Architecture","text":"Claude Code Skills Architecture <p>Type: diagram</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Visualize how Claude Code Skills manage context efficiently through summary indexing and on-demand loading</p> <p>Components to show:</p> <ol> <li>Context Window (large rectangle representing total available space):</li> <li>Fixed boundary showing context limit</li> <li>Divided into sections:</li> <li>Conversation history (variable size)</li> <li>Skill Index (fixed ~3000 tokens)</li> <li>Active skill content (variable, loaded on demand)</li> <li> <p>Working space (available for generation)</p> </li> <li> <p>Skill Registry (sidebar):</p> </li> <li>30 slots shown as cards</li> <li>Each card shows:</li> <li>Skill name</li> <li>~100 token summary</li> <li>\"Load\" indicator</li> <li>Some cards highlighted as \"currently loaded\"</li> <li> <p>Some cards dimmed as \"available but not loaded\"</p> </li> <li> <p>Loading Mechanism (arrows):</p> </li> <li>Arrow from conversation to skill registry: \"Relevance detection\"</li> <li>Arrow from skill registry to context window: \"On-demand loading\"</li> <li> <p>Arrow showing skill content expanding into context</p> </li> <li> <p>Example scenario:</p> </li> <li>User message: \"Create a physics simulation\"</li> <li>Skill index scan: \"physics-sim\" and \"microsim-generator\" highlighted</li> <li>Both skills loaded into context</li> <li>Remaining space shown as available</li> </ol> <p>Flow annotations: - \"Step 1: User request enters context\" - \"Step 2: Claude scans skill summaries\" - \"Step 3: Relevant skills loaded\" - \"Step 4: Generation uses skill knowledge\"</p> <p>Visual metaphors: - Library card catalog for skill index - Books being pulled from shelves for skill loading - Desk workspace for active context</p> <p>Color scheme: - Blue for context window boundaries - Green for loaded/active skills - Gray for available but unloaded skills - Yellow for user input - Orange for skill summaries</p> <p>Implementation: HTML/CSS/JavaScript with animated loading demonstration</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#building-consistency-with-rule-hierarchies","title":"Building Consistency with Rule Hierarchies","text":"<p>Here's a challenge that emerges when you're creating not just one MicroSim, but dozens or hundreds across multiple textbooks: consistency. How do you ensure that all your MicroSims use the same styling, follow the same interaction patterns, and maintain the same quality standards?</p> <p>The answer is hierarchical rule systems\u2014layered sets of guidelines that cascade from broad organizational standards down to specific project requirements.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-rule-hierarchy-model","title":"The Rule Hierarchy Model","text":"<p>Think of rules like cascading style sheets (CSS) in web development. Just as CSS has specificity rules that determine which styles apply when conflicts occur, a rule hierarchy for MicroSim generation has layers that override each other in predictable ways.</p> <p>From broadest to most specific:</p> <ol> <li>Global/Enterprise Rules: Apply to all MicroSims across the organization - any changes have a large impact (blast radius)</li> <li>Business Unit Rules: Apply to a division or major product line</li> <li>Department Rules: Apply to a team or subject area</li> <li>Project Rules: Apply to a specific textbook or course</li> <li>Personal Rules: Apply to an individual creator's work</li> </ol>"},{"location":"chapters/09-generating-microsims-ai-tools/#rule-content-at-each-level","title":"Rule Content at Each Level","text":"Level Example Rules Typical Owner Enterprise Brand colors, accessibility standards, legal requirements Design system team Business Unit Technology stack, deployment platforms, API standards Technical leadership Department Subject-specific conventions, pedagogical approaches Department head Project Textbook styling, chapter conventions, navigation patterns Project lead Personal IDE preferences, code formatting, personal shortcuts Individual"},{"location":"chapters/09-generating-microsims-ai-tools/#example-rules-at-each-level","title":"Example: Rules at Each Level","text":"<p>Enterprise Rules (applies to all): <pre><code># Enterprise MicroSim Standards\n\n## Branding\n- Primary color: #1E88E5 (corporate blue)\n- Secondary color: #FFA726 (corporate orange)\n- Font family: \"Roboto\", sans-serif\n\n## Accessibility\n- WCAG 2.1 AA compliance required\n- Minimum contrast ratio: 4.5:1\n- Keyboard navigation mandatory\n- Screen reader compatibility required\n\n## Legal\n- Copyright notice in all files\n- No external tracking scripts\n- Data privacy compliance\n</code></pre></p> <p>Business Unit Rules (K-12 Education Division): <pre><code># K-12 Education MicroSim Standards\n\n## Extends: Enterprise Standards\n\n## Technology\n- p5.js for animations\n- Chart.js for data visualization\n- No dependencies requiring accounts/logins\n\n## Pedagogy\n- Maximum 3 interactive controls per MicroSim\n- Always include reset button\n- Provide scaffolded difficulty levels\n</code></pre></p> <p>Department Rules (Science Education): <pre><code># Science Education MicroSim Standards\n\n## Extends: K-12 Education Standards\n\n## Subject Conventions\n- SI units required (with Imperial conversions where appropriate)\n- Scientific notation for very large/small numbers\n- Significant figures displayed appropriately\n\n## Visualization\n- Real-world scale indicators where possible\n- Time controls for process simulations\n- Data collection/export capability\n</code></pre></p> <p>Project Rules (Physics Textbook, Grade 9): <pre><code># Grade 9 Physics Textbook MicroSim Standards\n\n## Extends: Science Education Standards\n\n## Visual Style\n- Chapter accent colors: Ch1=#E53935, Ch2=#8E24AA, Ch3=#1E88E5...\n- Mascot character appears in hint tooltips\n- Consistent control layout across all MicroSims\n\n## Pedagogical\n- Pre-test prediction prompts\n- Post-interaction reflection questions\n- Connection to specific textbook sections\n</code></pre></p> <p>Personal Rules (individual preferences): <pre><code># Dan's MicroSim Preferences\n\n## Extends: Grade 9 Physics Standards\n\n## Development\n- Always run updateCanvasSize() first in setup()\n- Console.log learning events for debugging\n- Include timing metadata in generated files\n</code></pre></p>"},{"location":"chapters/09-generating-microsims-ai-tools/#conflict-resolution-the-specificity-principle","title":"Conflict Resolution: The Specificity Principle","text":"<p>What happens when rules conflict? Like CSS, more specific rules override more general ones. The hierarchy provides automatic conflict resolution:</p> <p>General principle: The most specific rule wins.</p> <pre><code>Enterprise: \"Use corporate blue (#1E88E5) for primary actions\"\nProject: \"Use chapter accent color for primary actions\"\n\nResolution: Project rule wins (more specific)\n</code></pre>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-important-override","title":"The !important Override","text":"<p>Sometimes a higher-level rule must not be overridden\u2014it's a hard requirement, not a default. Like CSS's <code>!important</code> declaration, rule systems need an override mechanism:</p> <pre><code>## Accessibility [MANDATORY]\n- WCAG 2.1 AA compliance required\n- This rule cannot be overridden at lower levels\n</code></pre> <p>The <code>[MANDATORY]</code> tag (or similar marker) indicates that this rule is not subject to cascade override. Use sparingly\u2014too many mandatory rules eliminate the flexibility that makes hierarchies useful.</p> Override Level Meaning When to Use Normal Can be overridden by more specific rules Default preferences <code>[RECOMMENDED]</code> Should only be overridden with justification Best practices <code>[MANDATORY]</code> Cannot be overridden Legal, accessibility, safety"},{"location":"chapters/09-generating-microsims-ai-tools/#diagram-rule-hierarchy-cascade","title":"Diagram: Rule Hierarchy Cascade","text":"Rule Hierarchy Cascade <p>Type: diagram</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Visualize how rules cascade through hierarchy levels and how conflicts are resolved</p> <p>Visual layout: Vertical cascade showing rules flowing down through levels</p> <p>Structure (top to bottom):</p> <p>Level 1 - Enterprise (top, widest): - Box containing sample rules - Color: Dark blue - Width: Full width - Rules: \"Brand colors\", \"Accessibility [MANDATORY]\", \"Legal requirements\" - Arrow flowing down</p> <p>Level 2 - Business Unit: - Box slightly narrower - Color: Medium blue - Rules: \"Technology stack\", \"Platform standards\" - Shows some enterprise rules passing through unchanged - Shows one rule being overridden (strikethrough in enterprise box) - Arrow flowing down</p> <p>Level 3 - Department: - Box narrower still - Color: Light blue - Rules: \"Subject conventions\", \"Pedagogical approach\" - Conflict example highlighted:   - Enterprise says X   - Department says Y   - Y is applied (shown with checkmark) - Arrow flowing down</p> <p>Level 4 - Project: - Box even narrower - Color: Teal - Rules: \"Visual style\", \"Chapter conventions\" - MANDATORY rule from enterprise shown as unchanged/locked - Arrow flowing down</p> <p>Level 5 - Personal (bottom, narrowest): - Box narrowest - Color: Green - Rules: \"Development preferences\", \"Code style\" - Final applied rules shown</p> <p>Conflict Resolution Panel (side): - Example conflict scenario - Visual showing \"most specific wins\" - Exception: MANDATORY rules shown with lock icon - Resulting rule highlighted</p> <p>Visual indicators: - Lock icon for MANDATORY rules - Override arrows showing rule replacement - Passthrough arrows for unchanged rules - Color gradient from dark (general) to light (specific)</p> <p>Interactive elements: - Click on any rule to see its cascade path - Hover to see which level defined the rule - Toggle to show conflicts only</p> <p>Implementation: HTML/CSS/JavaScript with animated cascade demonstration</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#implementing-rule-repositories","title":"Implementing Rule Repositories","text":"<p>Where do these rules actually live? Several approaches:</p> <p>Option 1: Hierarchical Files <pre><code>/rules\n  /enterprise\n    - brand-standards.md\n    - accessibility.md\n    - legal.md\n  /business-unit\n    /k12-education\n      - technology.md\n      - pedagogy.md\n  /department\n    /science\n      - conventions.md\n  /project\n    /physics-grade9\n      - style-guide.md\n</code></pre></p> <p>Option 2: Flat with Tags <pre><code>/rules\n  - enterprise-brand-standards.md\n  - enterprise-accessibility.md\n  - bu-k12-technology.md\n  - dept-science-conventions.md\n  - proj-physics9-style.md\n</code></pre></p> <p>Option 3: Single File with Sections <pre><code>/rules\n  - complete-ruleset.md (with hierarchical sections)\n</code></pre></p> <p>Option 4: Database/CMS <pre><code>Rules stored in structured database with:\n  - Level (enterprise, bu, dept, project, personal)\n  - Category (visual, technical, pedagogical)\n  - Override status (normal, recommended, mandatory)\n  - Effective dates\n  - Owner/approver\n</code></pre></p>"},{"location":"chapters/09-generating-microsims-ai-tools/#pros-and-cons-of-hierarchy-depth","title":"Pros and Cons of Hierarchy Depth","text":"<p>Should you use all five levels, or keep it simpler?</p> Approach Pros Cons Deep hierarchy (5+ levels) Maximum flexibility; clear ownership; can scale to large orgs Complexity; harder to track which rule applies; maintenance burden Medium hierarchy (3 levels) Good balance; covers most use cases; manageable complexity May not fit very large or very small organizations Shallow hierarchy (1-2 levels) Simple; easy to understand; fast to implement Limited flexibility; may require frequent overrides; doesn't scale <p>Recommendation: Start with 3 levels (organization, project, personal) and add intermediate levels only when you have clear use cases. Complexity you add early tends to stick around.</p> <p>Hierarchy Maintenance</p> <ol> <li>Training for Top Level Editor - Be very carful about changing ANY high-level rules. Because these rules are widely used, they can have a large negative impact if they are not tested carefully.  Only your most experienced developers should be given permission to change enterprise-level rules.  This is where regression testing is most valuable.</li> <li>Every level you add requires someone to maintain it. An abandoned middle level is worse than no level at all\u2014it creates confusion about whether rules are current or deprecated. Only add levels if you have committed owners.</li> </ol>"},{"location":"chapters/09-generating-microsims-ai-tools/#refinement-prompts-the-art-of-iteration","title":"Refinement Prompts: The Art of Iteration","text":"<p>Unless you are using mature skills, the first version of a MicroSim rarely produces a perfect result. Refinement prompts are follow-up messages that guide AI toward improvements. This is where the real craft of AI collaboration emerges.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-refinement-mindset","title":"The Refinement Mindset","text":"<p>Refinement is not about criticizing AI or starting over\u2014it's about collaborative improvement. The AI has made an attempt based on its understanding; your refinement prompt helps clarify where that understanding was incomplete or where the result needs adjustment.</p> <p>Good refinement prompts:</p> <ul> <li>Acknowledge what's working (builds on successes)</li> <li>Identify specific issues (not vague dissatisfaction)</li> <li>Explain why something needs change (context helps AI understand)</li> <li>Suggest approaches (when you have ideas)</li> <li>Maintain context (reference the specification)</li> </ul>"},{"location":"chapters/09-generating-microsims-ai-tools/#refinement-prompt-patterns","title":"Refinement Prompt Patterns","text":"<p>Pattern 1: The Specific Fix <pre><code>The simulation works well, but the slider for \"price\" should range\nfrom $0 to $100, not 0 to 1. Please update the slider range and\nensure the display shows dollar signs.\n</code></pre></p> <p>Pattern 2: The Behavior Clarification <pre><code>When the user adjusts supply, the curve shifts correctly. However,\nthe equilibrium point should animate smoothly to its new position\nrather than jumping instantly. Add a transition animation of\napproximately 500ms.\n</code></pre></p> <p>Pattern 3: The Visual Adjustment <pre><code>The colors work but are hard to distinguish for colorblind users.\nPlease change the supply curve to #1E88E5 (blue) and the demand\ncurve to #E53935 (red), which have better contrast and are\naccessible.\n</code></pre></p> <p>Pattern 4: The Added Feature <pre><code>Please add a \"Reset\" button below the controls that returns all\nparameters to their default values and resets the visualization\nto its initial state.\n</code></pre></p> <p>Pattern 5: The Architecture Change <pre><code>The current approach uses separate functions for each control.\nPlease refactor to use a single updateVisualization() function\nthat reads all current control values and updates the display.\nThis will make future modifications easier.\n</code></pre></p>"},{"location":"chapters/09-generating-microsims-ai-tools/#what-makes-refinement-prompts-effective","title":"What Makes Refinement Prompts Effective","text":"Characteristic Example Why It Helps Specific \"Change slider range to 0-100\" AI knows exactly what to modify Contextual \"As specified in the requirements...\" References shared understanding Reasoned \"...for accessibility compliance\" Explains the why, aids future decisions Actionable \"Add a transition animation\" Clear next step Constructive \"The colors work but...\" Preserves what's good"},{"location":"chapters/09-generating-microsims-ai-tools/#the-refinement-conversation-flow","title":"The Refinement Conversation Flow","text":"<p>A typical refinement conversation looks like this:</p> <ol> <li>Initial generation: AI produces first version</li> <li>Testing: You test in browser, identify 3 issues</li> <li>Refinement 1: \"Please fix issues A and B\" (most important first)</li> <li>Testing: A fixed, B partially fixed, new issue C appeared</li> <li>Refinement 2: \"B still needs X, and the change caused C, please fix both\"</li> <li>Testing: All issues resolved</li> <li>Final refinement: \"Add code comments and format consistently\"</li> </ol> <p>Notice the batching strategy\u2014grouping related fixes reduces round trips while keeping prompts focused.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#output-interpretation-reading-what-ai-gives-you","title":"Output Interpretation: Reading What AI Gives You","text":"<p>Output interpretation is the skill of understanding what AI has generated and assessing its quality. This isn't just \"does it run?\"\u2014it's evaluating whether the generated code meets your specification, follows best practices, and will be maintainable.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#levels-of-output-assessment","title":"Levels of Output Assessment","text":"<p>Level 1: Syntax Validity - Does the code have any syntax errors? - Are all tags/brackets properly closed? - Are dependencies correctly referenced?</p> <p>Level 2: Functional Correctness - Does the MicroSim run without JavaScript errors? - Do the controls produce expected behavior? - Is the visualization mathematically/scientifically correct?</p> <p>Level 3: Specification Compliance - Are all specified features implemented? - Do parameters use correct ranges? - Does the visual design match requirements? - Are all learning objectives supported?</p> <p>Level 4: Quality Standards - Is the code clean and readable? - Are there appropriate comments? - Is the structure maintainable? - Does it follow project conventions?</p> <p>Level 5: Pedagogical Effectiveness - Does the interaction support learning? - Is the cognitive load appropriate? - Are misconceptions avoided? - Is feedback clear and helpful?</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#common-output-patterns-to-watch-for","title":"Common Output Patterns to Watch For","text":"Pattern What to Look For Risk Level Truncated code Code ends abruptly mid-function High\u2014won't run Placeholder comments <code>// TODO: implement this</code> Medium\u2014incomplete Hardcoded values Magic numbers instead of parameters Medium\u2014inflexible Missing error handling No validation of inputs Medium\u2014fragile Inconsistent naming Mixed camelCase and snake_case Low\u2014maintenance issue Excessive complexity Over-engineered solutions Low\u2014but watch for bugs"},{"location":"chapters/09-generating-microsims-ai-tools/#issue-identification-finding-whats-wrong","title":"Issue Identification: Finding What's Wrong","text":"<p>Issue identification is the systematic process of finding problems in generated output. This is detective work\u2014some issues are obvious, others hide in edge cases.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#issue-categories","title":"Issue Categories","text":"<p>Category 1: Crashes and Errors - JavaScript errors in console - Failed resource loading - Infinite loops - Memory leaks</p> <p>Category 2: Functional Bugs - Incorrect calculations - Broken interactions - State management problems - Edge case failures</p> <p>Category 3: Visual Issues - Layout problems - Color/contrast issues - Responsive breakage - Animation glitches</p> <p>Category 4: Specification Deviations - Missing features - Wrong parameter ranges - Incorrect default values - Unexpected behaviors</p> <p>Category 5: Quality Concerns - Poor code organization - Missing comments - Accessibility failures - Performance problems</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-issue-triage-process","title":"The Issue Triage Process","text":"<p>When you find multiple issues, triage them:</p> <ol> <li>Blockers: Must fix before any testing continues (crashes, data corruption)</li> <li>Critical: Prevents core functionality (feature doesn't work)</li> <li>Major: Significantly impacts usability (confusing UI, wrong behavior)</li> <li>Minor: Noticeable but functional (visual polish, code style)</li> <li>Enhancement: Would be nice but not required (extra features)</li> </ol> <p>Address blockers and critical issues first. Don't waste time on polish if fundamentals are broken.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#diagram-issue-identification-workflow","title":"Diagram: Issue Identification Workflow","text":"Issue Identification Workflow <p>Type: workflow</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Provide a systematic approach to finding and categorizing issues in generated MicroSim code</p> <p>Visual style: Decision tree with testing checkpoints</p> <p>Main flow:</p> <p>Start: \"Generated Code Received\"</p> <p>Checkpoint 1 - \"Syntax Check\": - Action: Open in code editor, check for red underlines - Decision: Syntax errors?   - Yes \u2192 Log as Blocker, request fix immediately   - No \u2192 Continue</p> <p>Checkpoint 2 - \"Load Test\": - Action: Open in browser - Decision: Does it load without console errors?   - No \u2192 Examine console, log as Blocker/Critical   - Yes \u2192 Continue</p> <p>Checkpoint 3 - \"Visual Inspection\": - Action: Compare to specification mockup - Checklist:   - [ ] Layout matches spec   - [ ] Colors correct   - [ ] Controls present   - [ ] Labels readable - Decision: Visual issues found?   - Yes \u2192 Log as Major/Minor depending on severity   - Continue regardless</p> <p>Checkpoint 4 - \"Functional Testing\": - Action: Test each control/interaction - For each control:   - Default value correct?   - Range works as specified?   - Updates visualization?   - Edge cases handled? - Log issues as Critical/Major</p> <p>Checkpoint 5 - \"Edge Case Testing\": - Action: Try unusual inputs - Tests:   - Minimum values   - Maximum values   - Rapid changes   - Browser resize - Log issues found</p> <p>Checkpoint 6 - \"Specification Review\": - Action: Compare to spec line by line - Check every specified feature - Log deviations as appropriate severity</p> <p>Checkpoint 7 - \"Quality Review\": - Action: Code review - Check:   - Comments present?   - Code organized?   - Accessibility features? - Log as Minor/Enhancement</p> <p>End: \"Issue List Complete\" \u2192 Priority sorting \u2192 Refinement prompt</p> <p>Color coding: - Red: Blocker checkpoints - Orange: Critical checkpoints - Yellow: Major checkpoints - Green: Minor/Enhancement checkpoints</p> <p>Implementation: HTML/CSS/JavaScript interactive checklist</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-regeneration-decision-try-again-or-fix-by-hand","title":"The Regeneration Decision: Try Again or Fix by Hand?","text":"<p>One of the most important judgment calls in AI-assisted development: when you find issues, should you ask AI to regenerate the code, or should you manually adjust what you have? This is the regeneration decision.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#factors-favoring-regeneration","title":"Factors Favoring Regeneration","text":"<p>Regenerate when:</p> <ul> <li>The fundamental approach is wrong (need different architecture)</li> <li>Multiple interrelated issues would require extensive manual changes</li> <li>The specification has changed significantly</li> <li>You want to try a completely different approach</li> <li>The code quality is too poor to salvage</li> </ul>"},{"location":"chapters/09-generating-microsims-ai-tools/#factors-favoring-manual-adjustment","title":"Factors Favoring Manual Adjustment","text":"<p>Manually adjust when:</p> <ul> <li>Issues are isolated and well-understood</li> <li>The overall structure is sound</li> <li>Changes are small and localized</li> <li>You need precise control over the fix</li> <li>You've already invested in understanding the generated code</li> <li>Manual fix is faster than explaining the change</li> </ul>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-decision-matrix","title":"The Decision Matrix","text":"Situation Regenerate Manual Fix Wrong algorithm \u2713 Missing feature \u2713 Incorrect parameter \u2713 Style/formatting \u2713 Multiple bugs from same cause \u2713 Single isolated bug \u2713 Major refactoring needed \u2713 Minor refactoring needed \u2713 Don't understand the code \u2713 Understand code well \u2713"},{"location":"chapters/09-generating-microsims-ai-tools/#the-hybrid-approach","title":"The Hybrid Approach","text":"<p>Often the best approach is hybrid:</p> <ol> <li>Make small manual fixes for quick wins</li> <li>Test to verify fixes don't break other things</li> <li>Use refinement prompts for larger changes</li> <li>Regenerate only if you're starting over conceptually</li> </ol>"},{"location":"chapters/09-generating-microsims-ai-tools/#manual-adjustment-when-you-take-the-wheel","title":"Manual Adjustment: When You Take the Wheel","text":"<p>Manual adjustment is directly editing AI-generated code yourself. This requires understanding the generated code well enough to modify it safely.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#when-manual-beats-ai","title":"When Manual Beats AI","text":"<p>Manual adjustment is often faster for:</p> <ul> <li>Changing a single value</li> <li>Fixing typos or naming</li> <li>Adding a simple feature to existing structure</li> <li>Adjusting CSS for visual tweaks</li> <li>Adding console.log for debugging</li> <li>Fixing obvious bugs</li> </ul>"},{"location":"chapters/09-generating-microsims-ai-tools/#safe-manual-modification-practices","title":"Safe Manual Modification Practices","text":"<ol> <li>Understand before modifying: Read the relevant section thoroughly</li> <li>Make one change at a time: Easier to identify what broke if something does</li> <li>Test after each change: Don't accumulate untested modifications</li> <li>Keep the original: Copy before modifying so you can compare</li> <li>Document your changes: Comments or commit messages explaining why</li> <li>Match existing style: Your additions should look like AI's code</li> </ol>"},{"location":"chapters/09-generating-microsims-ai-tools/#common-manual-adjustments","title":"Common Manual Adjustments","text":"<pre><code>// Original AI-generated code\nlet sliderValue = slider.value;  // Range 0-1\n\n// Manual adjustment: Change range to 0-100\nlet sliderValue = slider.value * 100;  // Range 0-100 (adjusted)\n</code></pre> <pre><code>// Original AI-generated code\nfunction draw() {\n  background(255);\n  // ... drawing code\n}\n\n// Manual adjustment: Add responsive sizing (from project rules)\nfunction draw() {\n  updateCanvasSize();  // Added: ensure responsive sizing\n  background(255);\n  // ... drawing code\n}\n</code></pre> <p>The 5-Minute Rule</p> <p>If you can make a manual fix in under 5 minutes with confidence, do it manually. If it would take longer or you're uncertain, write a refinement prompt instead. AI can often make complex changes faster and with fewer bugs than humans\u2014that's the whole point.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#personal-ownership-of-code-learning-to-let-go","title":"Personal Ownership of Code: Learning to Let Go","text":"<p>Here's a psychological challenge that surprises many developers new to AI-assisted development: letting go of code you've invested time in. Traditional development creates a strong sense of ownership. You wrote those functions. You debugged that tricky edge case at 2 AM. That code is yours.</p> <p>Then someone suggests: \"Why don't we just regenerate the whole thing with an updated specification?\"</p> <p>And something inside you screams: \"But I spent three hours on that animation timing!\"</p> <p>This reaction is natural, but it can sabotage your productivity. The code isn't the product\u2014the working MicroSim is the product. If regenerating from a better specification gets you to a working MicroSim faster than patching your existing code, then regeneration wins, regardless of your emotional attachment.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-sunk-cost-trap-in-code","title":"The Sunk Cost Trap in Code","text":"<p>Developers new to AI often fall into a familiar trap:</p> <ol> <li>Generate initial code</li> <li>Spend hours tweaking and adjusting manually</li> <li>Requirements change</li> <li>Refuse to regenerate because \"I already fixed so many things\"</li> <li>Spend more hours patching the patched code</li> <li>End up with a fragile mess that nobody understands</li> </ol> <p>This is the sunk cost fallacy applied to code. The time you already spent is gone\u2014it shouldn't influence whether regeneration is the right choice now.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-specification-first-mindset","title":"The Specification-First Mindset","text":"<p>The solution is a fundamental shift in thinking: invest in specifications, not code.</p> Traditional Approach Specification-First Approach Code is the asset Specification is the asset Protect and patch code Regenerate code freely Manual tweaks accumulate Improvements go into spec Hard to reproduce Always reproducible Knowledge in code comments Knowledge in specification \"Don't touch what works\" \"Regenerate when spec improves\" <p>When you discover something important during development\u2014a better interaction pattern, a cleaner visual design, an edge case that needs handling\u2014don't just fix the code. Update the specification. Then the fix becomes permanent, reproducible, and applicable to future MicroSims.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#training-teams-for-specification-driven-development","title":"Training Teams for Specification-Driven Development","text":"<p>If you're leading a team transitioning to AI-assisted development, expect resistance. Developers have spent years building intuitions about protecting and incrementally improving code. Those intuitions served them well\u2014but they can become obstacles in an AI-assisted workflow.</p> <p>Strategies for helping teams adapt:</p> <ol> <li> <p>Model the behavior: When requirements change, visibly choose regeneration over patching. Show that it works.</p> </li> <li> <p>Celebrate specification improvements: When someone adds a useful clause to the specification, recognize it. Make specification quality a team value.</p> </li> <li> <p>Track regeneration outcomes: Keep data on how long patching takes versus regenerating. The numbers usually speak for themselves.</p> </li> <li> <p>Create psychological safety: Make it clear that regenerating isn't \"throwing away work\"\u2014it's using the work (captured in the specification) more effectively.</p> </li> <li> <p>Start with low-stakes MicroSims: Let developers practice the regeneration workflow on simple projects before applying it to complex ones.</p> </li> </ol>"},{"location":"chapters/09-generating-microsims-ai-tools/#signs-you-should-regenerate-instead-of-patch","title":"Signs You Should Regenerate Instead of Patch","text":"<p>Watch for these indicators that regeneration would serve you better than continued patching:</p> <ul> <li>You've made more than 10 manual changes to generated code</li> <li>You're not sure what all your changes were anymore</li> <li>The code has become hard to understand</li> <li>A new requirement would touch multiple parts of the code</li> <li>You find yourself saying \"I'm not sure why this works, but don't change it\"</li> <li>The specification has evolved significantly since generation</li> </ul>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-courage-to-start-fresh","title":"The Courage to Start Fresh","text":"<p>There's a certain courage required to delete working code and regenerate. It feels wasteful, even reckless. But consider: if your specification is good, regeneration takes minutes. If it produces something worse, you can always revert (that's why we have version control).</p> <p>The developers who thrive with AI tools are those who learn to hold code loosely. They see code as a renewable resource\u2014cheap to generate, easy to replace. Their investment goes into specifications, rules, and skills\u2014the durable assets that make regeneration powerful.</p> <p>Patience with the Transition</p> <p>Adjusting to generative AI tools takes time. Developers who've spent years honing their craft may feel that AI threatens their expertise. Help them see that their expertise is more valuable than ever\u2014it just applies to specifications and evaluation rather than line-by-line coding. Be patient. The mindset shift is real, but so are the productivity gains on the other side.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#version-control-tracking-your-journey","title":"Version Control: Tracking Your Journey","text":"<p>Version control is the practice of systematically tracking changes to your MicroSim code over time. In AI-assisted development, this becomes even more important because you're potentially generating large amounts of code quickly.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#why-version-control-matters-for-ai-work","title":"Why Version Control Matters for AI Work","text":"<p>Traditional coding produces gradual changes. AI-assisted coding can produce massive changes in single generations. Without version control:</p> <ul> <li>You can't undo a bad regeneration</li> <li>You lose track of what worked before</li> <li>You can't compare approaches</li> <li>Collaboration becomes chaotic</li> </ul>"},{"location":"chapters/09-generating-microsims-ai-tools/#git-workflow-for-microsim-development","title":"Git Workflow for MicroSim Development","text":"<pre><code># Initial generation\ngit add physics-sim.html\ngit commit -m \"Initial AI generation of physics MicroSim\"\n\n# After refinement round 1\ngit add physics-sim.html\ngit commit -m \"Refinement: fixed slider range, added reset button\"\n\n# After manual adjustments\ngit add physics-sim.html\ngit commit -m \"Manual fix: adjusted animation timing for smoothness\"\n\n# After major regeneration\ngit add physics-sim.html\ngit commit -m \"Regeneration: new approach using requestAnimationFrame\"\n</code></pre>"},{"location":"chapters/09-generating-microsims-ai-tools/#meaningful-commit-messages-for-ai-work","title":"Meaningful Commit Messages for AI Work","text":"<p>Structure your commits to track the development process:</p> Prefix Meaning Example <code>gen:</code> Initial AI generation <code>gen: supply-demand simulation from spec v2</code> <code>ref:</code> Refinement via prompt <code>ref: added equilibrium animation</code> <code>fix:</code> Manual bug fix <code>fix: corrected price calculation formula</code> <code>regen:</code> Full regeneration <code>regen: new visualization approach</code> <code>style:</code> Formatting/style changes <code>style: applied project conventions</code>"},{"location":"chapters/09-generating-microsims-ai-tools/#branching-for-experimentation","title":"Branching for Experimentation","text":"<p>Use branches to try different approaches without losing your current progress:</p> <pre><code># Current working version on main\ngit checkout -b experiment/different-layout\n\n# Try regeneration with different approach\n# ... generate and test ...\n\n# If it works better, merge\ngit checkout main\ngit merge experiment/different-layout\n\n# If it doesn't, just delete the branch\ngit branch -d experiment/different-layout\n</code></pre>"},{"location":"chapters/09-generating-microsims-ai-tools/#iteration-management-orchestrating-the-process","title":"Iteration Management: Orchestrating the Process","text":"<p>Iteration management is the high-level skill of orchestrating the entire generation-refinement cycle. It's project management for AI collaboration.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#the-iteration-log","title":"The Iteration Log","text":"<p>Keep a log of your iterations:</p> <pre><code># Supply-Demand MicroSim Development Log\n\n## Iteration 1 - Initial Generation\n- Date: 2024-01-15\n- Prompt: Full spec v1\n- Result: Basic functionality, several issues\n- Issues: Slider ranges wrong, no reset button, colors too similar\n- Decision: Refinement prompt for all three\n\n## Iteration 2 - Refinement\n- Date: 2024-01-15\n- Prompt: Fix slider ranges, add reset, change colors\n- Result: Sliders fixed, reset added, colors still problematic\n- Issues: Colors changed but still low contrast\n- Decision: Manual adjustment (faster than explaining colorblindness again)\n\n## Iteration 3 - Manual + Refinement\n- Date: 2024-01-15\n- Manual: Fixed colors to accessible palette\n- Prompt: Add prediction prompt feature\n- Result: Both changes successful\n- Issues: None functional, code could use comments\n- Decision: Final refinement for polish\n\n## Iteration 4 - Finalization\n- Date: 2024-01-15\n- Prompt: Add code comments, format consistently\n- Result: Production ready\n- Decision: Deploy\n</code></pre>"},{"location":"chapters/09-generating-microsims-ai-tools/#knowing-when-to-stop","title":"Knowing When to Stop","text":"<p>Perfectionism is the enemy of shipped MicroSims. Establish stopping criteria:</p> <ol> <li>Core functionality works: All specified features operational</li> <li>Major issues resolved: No blockers or critical bugs</li> <li>Accessibility met: Keyboard navigation, contrast requirements</li> <li>Code is maintainable: Reasonably commented, structured</li> <li>Tested with learners: At least informal validation</li> </ol> <p>If you meet these criteria, ship it. You can always iterate post-deployment based on real feedback.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#iteration-anti-patterns","title":"Iteration Anti-Patterns","text":"Anti-Pattern Symptom Solution Endless refinement 10+ iterations without shipping Set iteration limit, ship MVP Specification drift Requirements changing mid-iteration Freeze spec, iterate within scope Manual takeover Writing more code than AI Step back, rethink prompts Regeneration spiral Each regeneration introduces new issues Return to last good version Polish paralysis Spending hours on minor visual tweaks Set time boxes for polish"},{"location":"chapters/09-generating-microsims-ai-tools/#diagram-iteration-management-dashboard","title":"Diagram: Iteration Management Dashboard","text":"Iteration Management Dashboard <p>Type: infographic</p> <p>Bloom Taxonomy: Evaluate (L5)</p> <p>Learning Objective: Provide a visual tool for tracking iteration progress and making go/no-go decisions</p> <p>Layout: Dashboard with multiple panels</p> <p>Panel 1 - \"Iteration Counter\": - Current iteration number (large display) - Iterations remaining (if limit set) - Color coding: Green (on track), Yellow (watch), Red (excessive) - Suggested max: 5-7 iterations</p> <p>Panel 2 - \"Issue Tracker\": - Stacked bar showing issues by severity - Resolved vs remaining - Trend line showing if issues are decreasing - Alert if issues increasing after iteration</p> <p>Panel 3 - \"Specification Coverage\": - Checklist of specified features - Percentage complete - Unchecked items highlighted - Must be 100% before finalization</p> <p>Panel 4 - \"Quality Gates\": - Checklist of quality criteria:   - [ ] Core functionality works   - [ ] No blocking issues   - [ ] Accessibility compliance   - [ ] Code maintainability   - [ ] Performance acceptable - Status: \"Ready to ship\" when all checked</p> <p>Panel 5 - \"Decision Point\": - Current status summary - Recommended action:   - \"Continue refinement\" (issues remaining)   - \"Ready for final polish\" (functionality complete)   - \"Ready to ship\" (all gates passed)   - \"Consider regeneration\" (too many issues)   - \"Stop and reassess\" (iteration limit hit)</p> <p>Panel 6 - \"Time Investment\": - Time spent per phase - Comparison to similar projects - ROI indicator (value created vs time spent)</p> <p>Interactive features: - Click to expand each panel - Input fields for logging iterations - Export iteration log as markdown - Reset for new MicroSim</p> <p>Color scheme: - Green: On track / complete - Yellow: Attention needed - Red: Problem / blocked - Blue: Information / neutral</p> <p>Implementation: HTML/CSS/JavaScript dashboard with local storage for persistence</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#putting-it-all-together-the-complete-ai-collaboration-workflow","title":"Putting It All Together: The Complete AI Collaboration Workflow","text":"<p>Let's synthesize everything into a practical workflow you can follow:</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#pre-generation-checklist","title":"Pre-Generation Checklist","text":"<p>Before starting generation:</p> <ul> <li>[ ] Specification complete and reviewed</li> <li>[ ] Project rules identified and accessible</li> <li>[ ] Generation skill selected</li> <li>[ ] Success criteria defined</li> <li>[ ] Iteration limit set (suggest: 5-7)</li> </ul>"},{"location":"chapters/09-generating-microsims-ai-tools/#generation-phase","title":"Generation Phase","text":"<ol> <li>Craft your prompt with full specification, context, constraints, examples</li> <li>Include relevant rules from your hierarchy</li> <li>Submit and wait for generation</li> <li>Save output immediately (version control)</li> </ol>"},{"location":"chapters/09-generating-microsims-ai-tools/#evaluation-phase","title":"Evaluation Phase","text":"<ol> <li>Syntax check in editor</li> <li>Load test in browser</li> <li>Visual inspection against spec</li> <li>Functional testing of all controls</li> <li>Edge case testing</li> <li>Specification compliance review</li> </ol>"},{"location":"chapters/09-generating-microsims-ai-tools/#refinement-phase","title":"Refinement Phase","text":"<ol> <li>Triage identified issues</li> <li>Decide: regenerate or refine?</li> <li>If refining: craft specific, contextual refinement prompt</li> <li>If manual: make isolated, tested changes</li> <li>Repeat evaluation</li> </ol>"},{"location":"chapters/09-generating-microsims-ai-tools/#finalization-phase","title":"Finalization Phase","text":"<ol> <li>Verify all quality gates passed</li> <li>Final polish iteration (comments, formatting)</li> <li>Commit with meaningful message</li> <li>Deploy to target environment</li> <li>Document in iteration log</li> </ol>"},{"location":"chapters/09-generating-microsims-ai-tools/#summary-your-ai-collaboration-toolkit","title":"Summary: Your AI Collaboration Toolkit","text":"<p>You've now mastered the skills for effective AI collaboration in MicroSim development. Let's recap what you've learned:</p> <p>Generation Workflow provides the five-phase framework from specification through finalization. Understanding this workflow helps you know where you are and what comes next.</p> <p>Prompt Engineering is the foundation skill\u2014crafting prompts with context, specifications, constraints, examples, and format instructions that elicit optimal results.</p> <p>Claude Code Skills offer intelligent context management through summary indexing and on-demand loading, letting you maintain up to 30 specialized skills that Claude can invoke when relevant.</p> <p>Rule Hierarchies enable consistency at scale\u2014from enterprise standards through business units, departments, projects, and personal preferences, with clear conflict resolution via specificity and mandatory overrides.</p> <p>Refinement Prompts are the art of iterative improvement\u2014specific, contextual, reasoned, and constructive messages that guide AI toward better results.</p> <p>Output Interpretation is reading what AI generates with critical assessment at multiple levels from syntax validity through pedagogical effectiveness.</p> <p>Issue Identification is systematic detective work, triaging problems by severity and addressing blockers before polish.</p> <p>Regeneration Decision is the judgment call between asking AI to try again versus fixing things yourself\u2014guided by factors like issue scope, code understanding, and time investment.</p> <p>Manual Adjustment is knowing when to take the wheel for quick fixes, always following safe modification practices.</p> <p>Version Control tracks your journey through iterations, enabling rollback, comparison, and collaboration.</p> <p>Iteration Management orchestrates the whole process, keeping logs, knowing when to stop, and avoiding anti-patterns like endless refinement.</p> <p>When you combine these skills with your pedagogical expertise and domain knowledge, you become something new: not just an instructional designer, not just a developer, but a conductor orchestrating AI capabilities toward educational goals. That's a powerful role, and the world needs more people who can fill it.</p> <p>The MicroSims you create with these skills will help learners around the globe understand complex concepts through interactive exploration. Every simulation you generate is a small step toward a world where education is more engaging, more effective, and more accessible to everyone.</p> <p>Now go forth and generate. Your AI collaborator is ready when you are.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/#references-and-further-reading","title":"References and Further Reading","text":"<p>For those wanting to dive deeper into AI-assisted development:</p> <ul> <li>Anthropic. (2024). Claude Documentation: Prompt Engineering Guide. https://docs.anthropic.com</li> <li>White, J., et al. (2023). A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. arXiv preprint.</li> <li>GitHub. (2024). GitHub Copilot Documentation. https://docs.github.com/copilot</li> <li>Nielsen, J. (2023). AI Tools in Professional Workflows. Nielsen Norman Group.</li> </ul> Test Your Understanding: Refinement vs. Regeneration <p>You've generated a MicroSim and found three issues: (1) a single wrong default value, (2) the entire animation approach is too slow and needs a different implementation, and (3) a typo in a label. How would you handle each issue?</p> <p>Answer:</p> <ol> <li>Manual fix - changing one value is faster than explaining it. </li> <li>Regeneration or major refinement prompt - a fundamental approach change needs AI help. </li> <li>Manual fix - typos are trivial to fix directly. The optimal approach is hybrid: manually fix the value and typo, then submit a refinement prompt focused solely on the animation approach.</li> </ol>"},{"location":"chapters/09-generating-microsims-ai-tools/quiz/","title":"Quiz: Generating MicroSims with AI Tools","text":"<p>Test your understanding of AI collaboration techniques, prompt engineering, and the complete generation workflow for creating educational MicroSims.</p>"},{"location":"chapters/09-generating-microsims-ai-tools/quiz/#1-what-is-prompt-engineering-in-the-context-of-microsim-generation","title":"1. What is prompt engineering in the context of MicroSim generation?","text":"<ol> <li>Writing code that generates random prompts for users</li> <li>The art and science of crafting inputs that elicit optimal outputs from AI systems</li> <li>Engineering hardware that can display prompts faster</li> <li>Creating database schemas for storing user prompts</li> </ol> Show Answer <p>The correct answer is B. Prompt engineering is the art and science of crafting inputs that elicit optimal outputs from AI systems. It's not quite programming, not quite writing\u2014it's a new discipline that sits at the intersection of clear communication, technical understanding, and understanding how AI systems interpret and respond to requests.</p> <p>Concept Tested: Prompt Engineering</p> <p>See: Chapter Content</p>"},{"location":"chapters/09-generating-microsims-ai-tools/quiz/#2-what-are-the-five-phases-of-the-microsim-generation-workflow","title":"2. What are the five phases of the MicroSim generation workflow?","text":"<ol> <li>Planning, Coding, Testing, Debugging, Deployment</li> <li>Specification, Initial Generation, Evaluation, Refinement, Finalization</li> <li>Design, Development, Review, Release, Maintenance</li> <li>Brainstorming, Prototyping, Implementation, Validation, Launch</li> </ol> Show Answer <p>The correct answer is B. The five phases of MicroSim generation are: Specification (creating detailed requirements), Initial Generation (AI produces first version), Evaluation (assessing output against specification), Refinement (iterating through improvements), and Finalization (deploying the polished result). The workflow isn't always linear\u2014you might jump back to specification if something is missing.</p> <p>Concept Tested: Generation Workflow</p> <p>See: Chapter Content</p>"},{"location":"chapters/09-generating-microsims-ai-tools/quiz/#3-according-to-the-chapter-which-phase-typically-takes-the-most-time-in-microsim-generation","title":"3. According to the chapter, which phase typically takes the most time in MicroSim generation?","text":"<ol> <li>Initial Generation (AI works fast)</li> <li>Finalization (polish and deployment)</li> <li>Specification (getting requirements right)</li> <li>Evaluation (testing and assessment)</li> </ol> Show Answer <p>The correct answer is C. Specification typically takes 30-40% of the total time because getting requirements right is crucial. Time invested in clear specifications pays dividends in faster generation and fewer iterations. Rushing the spec to \"let AI figure it out\" is a false economy that leads to frustrating refinement cycles.</p> <p>Concept Tested: Generation Workflow</p> <p>See: Chapter Content</p>"},{"location":"chapters/09-generating-microsims-ai-tools/quiz/#4-what-is-a-refinement-prompt","title":"4. What is a refinement prompt?","text":"<ol> <li>The first prompt you send to an AI system</li> <li>A follow-up message that guides AI toward improvements after the initial generation</li> <li>A prompt that automatically refines itself</li> <li>A template for writing specifications</li> </ol> Show Answer <p>The correct answer is B. Refinement prompts are follow-up messages that guide AI toward improvements after the initial generation. Good refinement prompts acknowledge what's working, identify specific issues, explain why something needs change, suggest approaches when you have ideas, and maintain context by referencing the specification.</p> <p>Concept Tested: Refinement Prompt</p> <p>See: Chapter Content</p>"},{"location":"chapters/09-generating-microsims-ai-tools/quiz/#5-what-is-the-regeneration-decision-and-when-should-you-regenerate-rather-than-manually-fix","title":"5. What is the regeneration decision, and when should you regenerate rather than manually fix?","text":"<ol> <li>The choice between using different AI models; regenerate when cost is high</li> <li>The judgment call between asking AI to try again versus fixing things yourself; regenerate when the fundamental approach is wrong</li> <li>The decision about which version control system to use; regenerate when files are corrupted</li> <li>The choice between manual and automated testing; regenerate when tests fail</li> </ol> Show Answer <p>The correct answer is B. The regeneration decision is the judgment call between asking AI to regenerate code versus manually adjusting what you have. You should regenerate when the fundamental approach is wrong, multiple interrelated issues require extensive manual changes, the specification has changed significantly, or the code quality is too poor to salvage. You should manually adjust when issues are isolated, the structure is sound, and changes are small and localized.</p> <p>Concept Tested: Regeneration Decision</p> <p>See: Chapter Content</p>"},{"location":"chapters/09-generating-microsims-ai-tools/quiz/#6-what-problem-do-claude-code-skills-solve","title":"6. What problem do Claude Code Skills solve?","text":"<ol> <li>They replace the need for human oversight in AI development</li> <li>They solve the context window challenge by providing intelligent context management through summary indexing and on-demand loading</li> <li>They automatically write all code without human input</li> <li>They eliminate the need for version control systems</li> </ol> Show Answer <p>The correct answer is B. Claude Code Skills solve the context window challenge. Every AI conversation has limited context space, but you might want Claude to know about hundreds of things. Skills solve this through intelligent context management: each skill has a short summary (~100 tokens) in the context, and full skill content is loaded on-demand only when relevant. This is like a library card catalog\u2014Claude doesn't carry every book, just indexes them.</p> <p>Concept Tested: Claude Code Skills</p> <p>See: Chapter Content</p>"},{"location":"chapters/09-generating-microsims-ai-tools/quiz/#7-what-is-the-purpose-of-rule-hierarchies-in-microsim-generation","title":"7. What is the purpose of rule hierarchies in MicroSim generation?","text":"<ol> <li>To rank MicroSims by popularity</li> <li>To ensure consistency across multiple MicroSims by providing layered guidelines that cascade from organizational standards to project-specific rules</li> <li>To determine which AI model to use</li> <li>To prioritize bug fixes in order of severity</li> </ol> Show Answer <p>The correct answer is B. Rule hierarchies enable consistency at scale by providing layered sets of guidelines that cascade from broad organizational standards down to specific project requirements. Like CSS specificity rules, more specific rules override more general ones. This ensures that all MicroSims use the same styling, follow the same interaction patterns, and maintain the same quality standards.</p> <p>Concept Tested: Rule Hierarchy</p> <p>See: Chapter Content</p>"},{"location":"chapters/09-generating-microsims-ai-tools/quiz/#8-what-does-the-chapter-recommend-regarding-personal-ownership-of-ai-generated-code","title":"8. What does the chapter recommend regarding personal ownership of AI-generated code?","text":"<ol> <li>Always protect and patch code you've worked on, never delete it</li> <li>Learn to hold code loosely\u2014invest in specifications rather than code, as code is a renewable resource</li> <li>Never modify AI-generated code manually</li> <li>Delete all AI-generated code and rewrite from scratch</li> </ol> Show Answer <p>The correct answer is B. The chapter recommends a fundamental shift in thinking: invest in specifications, not code. Developers who thrive with AI tools learn to hold code loosely, seeing it as a renewable resource\u2014cheap to generate, easy to replace. Their investment goes into specifications, rules, and skills\u2014the durable assets that make regeneration powerful. When you discover something important, update the specification so the fix becomes permanent and reproducible.</p> <p>Concept Tested: Manual Adjustment, Iteration Management</p> <p>See: Chapter Content</p>"},{"location":"chapters/09-generating-microsims-ai-tools/quiz/#9-what-is-the-recommended-structure-for-git-commit-messages-when-working-with-ai-generated-microsims","title":"9. What is the recommended structure for git commit messages when working with AI-generated MicroSims?","text":"<ol> <li>Use random unique identifiers only</li> <li>Use prefixes like \"gen:\", \"ref:\", \"fix:\", and \"regen:\" to track the development process</li> <li>Only commit final versions, never intermediate work</li> <li>Use the AI-generated timestamp as the commit message</li> </ol> Show Answer <p>The correct answer is B. The chapter recommends using meaningful commit prefixes to track the development process: <code>gen:</code> for initial AI generation, <code>ref:</code> for refinement via prompt, <code>fix:</code> for manual bug fixes, <code>regen:</code> for full regeneration, and <code>style:</code> for formatting changes. This helps track whether changes came from AI generation, AI refinement, or manual editing.</p> <p>Concept Tested: Version Control</p> <p>See: Chapter Content</p>"},{"location":"chapters/09-generating-microsims-ai-tools/quiz/#10-what-are-iteration-anti-patterns-that-should-be-avoided","title":"10. What are iteration anti-patterns that should be avoided?","text":"<ol> <li>Testing after every change, documenting decisions, using version control</li> <li>Endless refinement without shipping, specification drift, manual takeover where you write more code than AI</li> <li>Setting iteration limits, shipping MVPs, freezing specifications</li> <li>Using Claude Code Skills, following rule hierarchies, writing refinement prompts</li> </ol> Show Answer <p>The correct answer is B. Iteration anti-patterns include: endless refinement (10+ iterations without shipping), specification drift (requirements changing mid-iteration), manual takeover (writing more code than AI generates), regeneration spiral (each regeneration introduces new issues), and polish paralysis (spending hours on minor visual tweaks). Solutions include setting iteration limits, freezing specs, stepping back to rethink prompts, and using time boxes for polish.</p> <p>Concept Tested: Iteration Management</p> <p>See: Chapter Content</p>"},{"location":"chapters/10-quality-evaluation-frameworks/","title":"Quality Evaluation Frameworks","text":""},{"location":"chapters/10-quality-evaluation-frameworks/#summary","title":"Summary","text":"<p>This chapter presents comprehensive frameworks for evaluating MicroSim quality across three dimensions: technical, pedagogical, and user experience. You will learn output validation techniques, quality scores, functionality and responsiveness testing methods, and bug identification strategies. The chapter covers pedagogical evaluation including objective alignment, cognitive level matching, and effectiveness measurement. UX evaluation addresses intuitiveness and engagement balance. You will also learn rubric development, automated versus human evaluation methods, documentation standards, metadata standards, and designing for reusability.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following concepts from the learning graph:</p> <ol> <li>Output Validation</li> <li>Technical Evaluation</li> <li>Pedagogical Evaluation</li> <li>UX Evaluation</li> <li>Functionality Testing</li> <li>Responsiveness Testing</li> <li>Bug Identification</li> <li>Objective Alignment</li> <li>Cognitive Level Match</li> <li>Effectiveness Measure</li> <li>Intuitiveness</li> <li>Engagement Balance</li> <li>Evaluation Rubric</li> <li>Rubric Development</li> <li>Automated Evaluation</li> <li>Human Evaluation</li> <li>Documentation Standard</li> <li>Reusability</li> <li>Quality Score</li> <li>Standardization Score</li> <li>MicroSim Metadata</li> <li>Search and Reuse</li> </ol>"},{"location":"chapters/10-quality-evaluation-frameworks/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Foundations of Learning Objective Analysis</li> <li>Chapter 9: Generating MicroSims with AI Tools</li> </ul>"},{"location":"chapters/10-quality-evaluation-frameworks/#introduction-why-quality-matters-and-why-you-should-care","title":"Introduction: Why Quality Matters (And Why You Should Care)","text":"<p>Here's a joke for you: How many instructional designers does it take to evaluate a MicroSim? Just one\u2014but they'll need three different rubrics, a metadata schema, and a strong cup of coffee.</p> <p>All joking aside, creating a MicroSim is only half the battle. Without rigorous quality evaluation, you might deploy a simulation that looks pretty but teaches poorly, works on your laptop but crashes on a student's tablet, or disappears into the digital void because nobody can find it when they need it.</p> <p>This chapter arms you with the frameworks, rubrics, and standards needed to ensure your MicroSims are not just functional, but genuinely effective educational tools. Think of quality evaluation as the immune system of instructional design\u2014it protects learners from buggy interfaces, misaligned objectives, and the heartbreak of a beautiful simulation that nobody can find or reuse.</p> <p>The three pillars of MicroSim quality form what we call the Three-Lens Evaluation Model:</p> <ul> <li>Technical Quality: Does it work reliably across devices and browsers?</li> <li>Pedagogical Quality: Does it actually teach what it claims to teach?</li> <li>User Experience Quality: Is it intuitive and engaging without being distracting?</li> </ul> <p>Let's explore each lens in detail, starting with the nuts and bolts of technical evaluation.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#the-three-lens-evaluation-model","title":"The Three-Lens Evaluation Model","text":"<p>Before diving into specifics, let's visualize how these three evaluation lenses work together to create a holistic quality assessment.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#diagram-three-lens-evaluation-model","title":"Diagram: Three-Lens Evaluation Model","text":"Three-Lens Evaluation Model <p>Type: diagram</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Students will understand how technical, pedagogical, and UX evaluation dimensions interact to form a complete quality assessment framework.</p> <p>Components to show: - Three overlapping circles representing Technical, Pedagogical, and UX evaluation - Central intersection labeled \"High-Quality MicroSim\" - Each circle contains key evaluation criteria:   - Technical: Functionality, Responsiveness, Bug-Free, Standardization   - Pedagogical: Objective Alignment, Cognitive Level Match, Effectiveness   - UX: Intuitiveness, Engagement Balance, Accessibility - Arrows showing how deficiencies in one area impact overall quality</p> <p>Visual style: Venn diagram with three circles, center highlighted in gold</p> <p>Color scheme: - Technical: Blue (#3B82F6) - Pedagogical: Green (#10B981) - UX: Purple (#8B5CF6) - Center intersection: Gold (#F59E0B)</p> <p>Implementation: HTML/CSS/JavaScript with SVG or p5.js</p> <p>The magic happens at the intersection of all three circles. A MicroSim might achieve technical perfection\u2014zero bugs, responsive across all devices\u2014but if it confuses learners or targets the wrong cognitive level, it fails its educational mission. Similarly, a pedagogically brilliant simulation that crashes on mobile devices serves no one.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#technical-evaluation-the-foundation-of-trust","title":"Technical Evaluation: The Foundation of Trust","text":"<p>Technical evaluation answers a fundamental question: Does this thing actually work?</p> <p>It sounds simple, but \"working\" encompasses far more than just loading without errors. Technical quality includes functionality testing, responsiveness across devices, bug identification, and adherence to standardization requirements.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#output-validation-the-first-line-of-defense","title":"Output Validation: The First Line of Defense","text":"<p>Output validation is the process of verifying that a MicroSim produces expected results given specific inputs. Think of it as the quality control checkpoint on the assembly line.</p> <p>For AI-generated MicroSims, output validation is particularly critical because generative AI systems can produce code that looks correct but contains subtle logical errors. Consider a physics simulation where gravity is accidentally set to a positive value instead of negative\u2014the objects float upward instead of falling. The code runs fine; the physics is just hilariously wrong.</p> <p>Key output validation checks include:</p> <ul> <li>Visual correctness: Do elements render in expected positions and colors?</li> <li>Mathematical accuracy: Do calculations produce correct results?</li> <li>Behavioral consistency: Does the simulation respond predictably to user inputs?</li> <li>Edge case handling: What happens with extreme values or unexpected inputs?</li> <li>State management: Does the simulation reset properly?</li> </ul>"},{"location":"chapters/10-quality-evaluation-frameworks/#functionality-testing","title":"Functionality Testing","text":"<p>Functionality testing systematically verifies that every interactive element performs its intended function.</p> Test Category What to Check Common Failures Controls Sliders respond, buttons trigger actions Range limits not enforced Animations Start/stop/reset work correctly Animation loops don't reset Data Display Values update in real-time Display lag or stale values User Input Text fields validate input No input sanitization Audio/Visual Feedback triggers appropriately Missing confirmation signals"},{"location":"chapters/10-quality-evaluation-frameworks/#responsiveness-testing","title":"Responsiveness Testing","text":"<p>In 2024, learners access content on everything from 27-inch desktop monitors to 5-inch smartphone screens. A MicroSim that only works on the developer's MacBook Pro is a MicroSim that fails most learners.</p> <p>Responsiveness testing verifies that:</p> <ul> <li>Canvas dimensions adapt to viewport size</li> <li>Control panels reorganize appropriately on smaller screens</li> <li>Touch targets are large enough for mobile interaction (minimum 44x44 pixels)</li> <li>Text remains readable at all breakpoints</li> <li>No horizontal scrolling is required</li> </ul>"},{"location":"chapters/10-quality-evaluation-frameworks/#diagram-responsive-breakpoint-testing","title":"Diagram: Responsive Breakpoint Testing","text":"Responsive Breakpoint Testing <p>Type: diagram</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Students will apply responsive design testing criteria to evaluate MicroSim adaptability across device sizes.</p> <p>Components to show: - Four device mockups side by side: Desktop (1920px), Laptop (1366px), Tablet (768px), Mobile (375px) - Each mockup shows same MicroSim at that breakpoint - Callout annotations showing what changes at each breakpoint:   - Control panel moves from side to bottom   - Fonts scale proportionally   - Touch targets increase size on mobile - Traffic light indicators: green (pass), yellow (issues), red (fail)</p> <p>Visual style: Device mockup comparison with annotations</p> <p>Color scheme: Device frames in dark gray, content areas in white, annotations in blue</p> <p>Implementation: HTML/CSS or static diagram</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#bug-identification-finding-the-gremlins","title":"Bug Identification: Finding the Gremlins","text":"<p>Bugs in educational software aren't just annoying\u2014they can actively harm learning. A student who encounters a crash during a \"eureka moment\" may lose motivation entirely. A calculation error can reinforce misconceptions rather than correcting them.</p> <p>Common bug categories in MicroSims include:</p> <ul> <li>Logic errors: Calculations produce wrong results</li> <li>State corruption: Variables retain values when they shouldn't</li> <li>Race conditions: Timing-dependent behavior causes inconsistency</li> <li>Memory leaks: Performance degrades over extended use</li> <li>Browser incompatibilities: Works in Chrome, fails in Safari</li> <li>Accessibility failures: Screen readers can't parse content</li> </ul> <p>Pro tip</p> <p>If you ever want to find bugs quickly, hand your MicroSim to a middle schooler.   They will click everything, in every order, as fast as possible\u2014and find edge cases you never imagined.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#pedagogical-evaluation-teaching-what-you-mean-to-teach","title":"Pedagogical Evaluation: Teaching What You Mean to Teach","text":"<p>Technical perfection means nothing if a MicroSim doesn't effectively teach its intended concepts. Pedagogical evaluation examines whether the simulation aligns with learning objectives, targets appropriate cognitive levels, and demonstrably improves student understanding.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#objective-alignment-staying-on-target","title":"Objective Alignment: Staying on Target","text":"<p>Objective alignment verifies that every element of a MicroSim serves its stated learning objective. This sounds obvious, but scope creep is a constant temptation. A simulation designed to teach supply and demand might gradually accumulate features for market equilibrium, price elasticity, government intervention, and international trade\u2014until it teaches nothing well.</p> <p>Questions to ask during objective alignment review:</p> <ul> <li>Does the simulation directly address the stated learning objective?</li> <li>Are there features that distract from the core concept?</li> <li>Can a learner complete the interaction without engaging with the key insight?</li> <li>Is the objective achievable within a reasonable time frame?</li> </ul> Alignment Issue Symptom Solution Scope creep Too many concepts, unfocused Remove tangential features Hidden complexity Core concept buried in interface Simplify to essential elements False completion Can \"finish\" without understanding Add understanding checks Missing scaffolding Jumps to complex without building Add progressive difficulty"},{"location":"chapters/10-quality-evaluation-frameworks/#cognitive-level-match","title":"Cognitive Level Match","text":"<p>Not all learning objectives are created equal. Bloom's Taxonomy (2001 revision) distinguishes six cognitive levels, from simple recall to creative synthesis. A MicroSim designed for \"Remember\" level learning (vocabulary flashcards) requires different design patterns than one targeting \"Analyze\" level (comparing multiple data sets).</p> <p>Cognitive Level Matching Checklist:</p> <ul> <li>Remember (L1): Does the sim support fact recall? (flashcards, matching games)</li> <li>Understand (L2): Does it enable explanation of concepts? (labeled diagrams, cause-effect demos)</li> <li>Apply (L3): Can learners use knowledge to solve problems? (parameter manipulation, scenario testing)</li> <li>Analyze (L4): Does it reveal relationships and patterns? (data exploration, network visualization)</li> <li>Evaluate (L5): Can learners make judgments based on criteria? (comparison tools, trade-off analysis)</li> <li>Create (L6): Does it support generating original work? (model builders, design tools)</li> </ul> <p>A common mismatch occurs when a learning objective targets \"Analyze\" but the MicroSim only supports \"Understand\"\u2014the simulation explains concepts beautifully but never challenges learners to discover relationships themselves.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#diagram-blooms-taxonomy-and-microsim-types","title":"Diagram: Bloom's Taxonomy and MicroSim Types","text":"Bloom's Taxonomy and MicroSim Types <p>Type: infographic</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Students will analyze the relationship between Bloom's Taxonomy cognitive levels and appropriate MicroSim types for each level.</p> <p>Layout: Vertical pyramid with six levels, each containing example MicroSim types</p> <p>Content: - Level 6 - Create: Model editors, free-form design tools, hypothesis builders - Level 5 - Evaluate: Comparison simulators, trade-off analyzers, judgment exercises - Level 4 - Analyze: Network graphs, data explorers, pattern identification tools - Level 3 - Apply: Parameter sliders, scenario testers, problem solvers - Level 2 - Understand: Animated explanations, cause-effect demos, concept maps - Level 1 - Remember: Flashcard games, matching exercises, term sorters</p> <p>Interactive elements: - Hover over each level to see detailed examples and design considerations - Click to expand with case studies of successful MicroSims at each level</p> <p>Color scheme: Gradient from light blue (Remember) to dark purple (Create)</p> <p>Implementation: HTML/CSS/JavaScript with progressive disclosure</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#effectiveness-measurement","title":"Effectiveness Measurement","text":"<p>The ultimate test of pedagogical quality is whether students actually learn. Effectiveness measurement goes beyond \"Did they complete the simulation?\" to ask \"Did their understanding improve?\"</p> <p>Methods for measuring MicroSim effectiveness include:</p> <ul> <li>Pre/post assessments: Test understanding before and after MicroSim use</li> <li>Embedded checkpoints: Questions within the simulation that verify understanding</li> <li>Transfer tasks: Can learners apply concepts to new situations?</li> <li>Long-term retention: Does understanding persist over time?</li> <li>Comparison studies: How does learning compare to alternative methods?</li> </ul>"},{"location":"chapters/10-quality-evaluation-frameworks/#ux-evaluation-the-human-factor","title":"UX Evaluation: The Human Factor","text":"<p>A MicroSim can work perfectly and teach effectively, yet still frustrate users with confusing navigation, overwhelming interfaces, or interactions that feel like wrestling with an angry octopus.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#intuitiveness-can-they-figure-it-out","title":"Intuitiveness: Can They Figure It Out?","text":"<p>Intuitiveness measures whether users can accomplish their goals without extensive instruction. The gold standard is the \"Grandma Test\"\u2014can someone with minimal technical background figure out how to use it within 30 seconds?</p> <p>Indicators of poor intuitiveness:</p> <ul> <li>Users click randomly hoping something happens</li> <li>The \"obvious\" next step isn't actually obvious</li> <li>Critical controls are hidden or unlabeled</li> <li>Feedback doesn't match user expectations</li> <li>Reset functions are hard to find</li> </ul>"},{"location":"chapters/10-quality-evaluation-frameworks/#engagement-balance-the-goldilocks-zone","title":"Engagement Balance: The Goldilocks Zone","text":"<p>There's a sweet spot between \"so boring I'm falling asleep\" and \"so gamified I forgot I'm supposed to be learning.\" Engagement balance seeks that Goldilocks zone where learners are sufficiently motivated to persist without being distracted by bells, whistles, and virtual confetti.</p> Too Little Engagement Balanced Too Much Engagement Plain text only Subtle animations Constant motion No feedback Meaningful responses Points for everything Static displays Interactive controls Addictive mechanics No progress indication Clear milestones Leaderboards dominating <p>The key question: Does the engagement serve the learning, or does learning serve the engagement?</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#diagram-engagement-vs-learning-trade-off","title":"Diagram: Engagement vs. Learning Trade-off","text":"Engagement vs. Learning Trade-off <p>Type: chart</p> <p>Bloom Taxonomy: Evaluate (L5)</p> <p>Learning Objective: Students will evaluate the relationship between engagement features and learning effectiveness to identify optimal design approaches.</p> <p>Chart type: Scatter plot with trend curve</p> <p>X-axis: Engagement Features (0-10 scale) Y-axis: Learning Effectiveness (0-10 scale)</p> <p>Data points showing: - Low engagement/low learning: Static diagram (1,3) - Moderate engagement/high learning: Interactive simulation (5,8) - High engagement/moderate learning: Game-heavy design (9,5) - Optimal zone highlighted in green (4-6 on X, 7-9 on Y)</p> <p>Annotations: - \"Boring\" zone (left side) - \"Optimal Balance\" zone (center-top) - \"Distraction Risk\" zone (right side) - Curve showing diminishing returns on engagement beyond optimal point</p> <p>Color scheme: Blue dots, green optimal zone, trend line in dark gray</p> <p>Implementation: Chart.js or Plotly</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#the-completeness-quality-score-a-rubric-llms-can-follow","title":"The Completeness Quality Score: A Rubric LLMs Can Follow","text":"<p>Here's where the rubber meets the road. How do you translate these evaluation dimensions into a concrete, measurable score that both humans and AI systems can consistently apply?</p> <p>The Completeness Quality Score is a 100-point rubric that evaluates whether a MicroSim includes all required components for standardization. Note that this score measures completeness, not usability\u2014a MicroSim can achieve a perfect 100 and still be pedagogically questionable. Think of it as a checklist for structural integrity, not artistic merit.</p> <p>Why Completeness Matters for AI</p> <p>Large Language Models are remarkably precise at following structured rules. By encoding quality criteria as a rubric with clear checkpoints, we enable automated quality assessment during and after MicroSim generation.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#the-100-point-completeness-rubric","title":"The 100-Point Completeness Rubric","text":"Test Name Description Points Title index.md file has a title in markdown level 1 header 2 main.html The file main.html is present in the MicroSim directory 10 Metadata 1 index.md has title and description metadata in YAML 3 Metadata 2 index.md has image references for social preview 5 metadata.json present A metadata.json file is present 10 metadata.json valid The MicroSim JSON Schema passes validation with no errors 20 iframe An iframe that uses src=\"main.html\" is present 10 Fullscreen Link A button to view the MicroSim in fullscreen is present 5 iframe example An iframe example in an HTML source block is present for embedding 5 image A screenshot image is present and referenced in header metadata 5 Overview A description of the MicroSim and how to use it is present 5 Lesson Plan A detailed lesson plan is present 10 References A list of references in markdown format 5 Type-Specific Format Varies by type (e.g., p5.js editor link for p5.js MicroSims) 5 Total 100"},{"location":"chapters/10-quality-evaluation-frameworks/#quality-score-interpretation","title":"Quality Score Interpretation","text":"<ul> <li>90-100: Production ready. All components present, excellent documentation.</li> <li>70-89: Good quality. Most components present, may lack lesson plan.</li> <li>50-69: Needs work. Core components present but minimal documentation.</li> <li>Below 50: Incomplete. Missing critical components; do not deploy.</li> </ul> <p>The quality score is stored in the YAML frontmatter of the index.md file:</p> <pre><code>---\ntitle: Bouncing Ball Simulation\ndescription: Interactive demonstration of gravity and elastic collisions\nquality_score: 86\n---\n</code></pre>"},{"location":"chapters/10-quality-evaluation-frameworks/#the-standardization-process-a-step-by-step-walkthrough","title":"The Standardization Process: A Step-by-Step Walkthrough","text":"<p>Let's walk through the complete standardization process for bringing a MicroSim up to quality standards. This process is designed to be followed by humans or automated by AI agents.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#step-1-locate-the-microsim-directory","title":"Step 1: Locate the MicroSim Directory","text":"<p>MicroSim directories follow the standard structure: <pre><code>docs/sims/[microsim-name]/\n</code></pre></p> <p>The <code>microsim-name</code> must use kebab-case (lowercase letters and dashes only). No spaces, underscores, or uppercase characters.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#step-2-check-existing-quality-score","title":"Step 2: Check Existing Quality Score","text":"<p>Before diving into evaluation, check if a quality score already exists in the index.md YAML frontmatter:</p> <pre><code>---\nquality_score: 86\n---\n</code></pre> <p>If the score is 85 or above, the MicroSim meets standards. Consider whether your time is better spent creating new simulations rather than perfecting existing ones.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#step-3-verify-core-files-exist","title":"Step 3: Verify Core Files Exist","text":"<p>At minimum, a MicroSim directory must contain:</p> <ul> <li><code>main.html</code> - The core simulation file (required)</li> <li><code>index.md</code> - The MkDocs documentation page (required for integration)</li> <li><code>metadata.json</code> - Dublin Core metadata for discoverability (required for searchability)</li> </ul>"},{"location":"chapters/10-quality-evaluation-frameworks/#step-4-validate-indexmd-structure","title":"Step 4: Validate index.md Structure","text":"<p>The index.md file must include:</p> <p>YAML Frontmatter (between <code>---</code> delimiters): <pre><code>---\ntitle: Simulation Name\ndescription: Brief description for SEO and social previews\nquality_score: 75\nimage: /sims/simulation-name/simulation-name.png\nog:image: /sims/simulation-name/simulation-name.png\n---\n</code></pre></p> <p>Level 1 Header: <pre><code># Simulation Name\n</code></pre></p> <p>Iframe Embed: <pre><code>&lt;iframe src=\"main.html\" width=\"100%\" height=\"600px\"&gt;&lt;/iframe&gt;\n</code></pre></p> <p>Copy-Paste Iframe Example: <pre><code>```html\n&lt;iframe src=\"https://yourdomain.github.io/path/sims/name/main.html\" width=\"100%\" height=\"600px\"&gt;&lt;/iframe&gt;\n```\n</code></pre></p> <p>Fullscreen Link: <pre><code>[Run MicroSim in Fullscreen](main.html){ .md-button .md-button--primary }\n</code></pre></p>"},{"location":"chapters/10-quality-evaluation-frameworks/#step-5-validate-type-specific-requirements","title":"Step 5: Validate Type-Specific Requirements","text":"<p>Different MicroSim types have additional requirements:</p> <p>p5.js MicroSims must include a link to the p5.js editor: <pre><code>[Edit in the p5.js Editor](https://editor.p5js.org/username/sketches/SKETCH_ID)\n</code></pre></p> <p>To detect if a MicroSim uses p5.js, check for: - Import statements for p5.js CDN in main.html - Use of p5.js functions like <code>setup()</code>, <code>draw()</code>, <code>createCanvas()</code></p> <p>Other library types (vis-network, Chart.js, D3.js) currently have no type-specific requirements, but this may change as the ecosystem evolves.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#step-6-validate-content-sections","title":"Step 6: Validate Content Sections","text":"<p>Check for presence of documentation sections:</p> <ul> <li>Description/How to Use: Explains the MicroSim's purpose and interaction patterns</li> <li>Lesson Plan (10 points): Learning objectives, target audience, prerequisites, activities, assessment</li> <li>References (5 points): Links to relevant resources, papers, or library documentation</li> </ul>"},{"location":"chapters/10-quality-evaluation-frameworks/#step-7-validate-metadatajson","title":"Step 7: Validate metadata.json","text":"<p>The metadata.json file must pass schema validation. More on this in the next section.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#step-8-calculate-and-record-quality-score","title":"Step 8: Calculate and Record Quality Score","text":"<p>Sum points from the rubric and write the score to the index.md YAML frontmatter.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#diagram-standardization-workflow","title":"Diagram: Standardization Workflow","text":"Standardization Workflow <p>Type: workflow</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Students will apply the standardization workflow to evaluate and upgrade MicroSim quality.</p> <p>Visual style: Flowchart with decision diamonds and process rectangles</p> <p>Steps: 1. Start: \"Receive MicroSim Path\"    Hover text: \"User provides path to MicroSim directory\"</p> <ol> <li>Decision: \"Existing Score &gt;= 85?\"    Hover text: \"Check YAML frontmatter for quality_score field\"</li> </ol> <p>3a. Process: \"Skip Standardization\" (if Yes)     Hover text: \"MicroSim already meets quality standards\"</p> <p>3b. Process: \"Begin Evaluation Checklist\" (if No)     Hover text: \"Proceed with full standardization process\"</p> <ol> <li> <p>Process: \"Check Core Files\"    Hover text: \"Verify main.html, index.md, metadata.json exist\"</p> </li> <li> <p>Process: \"Validate index.md Structure\"    Hover text: \"Check YAML, headers, iframes, links\"</p> </li> <li> <p>Decision: \"Is p5.js MicroSim?\"    Hover text: \"Detect p5.js library usage in main.html\"</p> </li> </ol> <p>7a. Process: \"Check p5.js Editor Link\" (if Yes)     Hover text: \"Verify link to p5.js editor exists\"</p> <p>7b. Process: \"Continue to Content\" (if No)     Hover text: \"Skip type-specific checks\"</p> <ol> <li> <p>Process: \"Validate Content Sections\"    Hover text: \"Check description, lesson plan, references\"</p> </li> <li> <p>Process: \"Validate metadata.json\"    Hover text: \"Run JSON Schema validation\"</p> </li> <li> <p>Process: \"Calculate Quality Score\"     Hover text: \"Sum points from rubric\"</p> </li> <li> <p>Process: \"Write Score to index.md\"     Hover text: \"Update YAML frontmatter with quality_score\"</p> </li> <li> <p>End: \"Standardization Complete\"     Hover text: \"MicroSim meets documentation standards\"</p> </li> </ol> <p>Color coding: - Blue: File operations - Yellow: Decision points - Green: Success outcomes - Purple: Validation steps</p> <p>Implementation: Mermaid or HTML/CSS/JavaScript flowchart</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#microsim-metadata-the-key-to-discoverability","title":"MicroSim Metadata: The Key to Discoverability","text":"<p>Here's a truth that should be embroidered on every instructional designer's laptop bag:</p> <p>\"You can't reuse what you can't find.\"</p> <p>A brilliant MicroSim that's buried in an undocumented folder, lacking keywords and categorization, might as well not exist. Metadata is the bridge between creation and reuse\u2014it makes simulations findable, comparable, and adaptable.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#the-power-of-structured-metadata","title":"The Power of Structured Metadata","text":"<p>The MicroSim ecosystem uses a comprehensive JSON Schema based on Dublin Core metadata standards, extended with educational and technical specifications. When every MicroSim includes a properly structured <code>metadata.json</code> file:</p> <ul> <li>Search tools like https://dmccreary.github.io/search-microsims/ can quickly narrow down databases by subject, grade level, JavaScript library, or complexity</li> <li>AI agents can find similar MicroSims to use as templates for new designs</li> <li>Educators can filter simulations by Bloom's Taxonomy level, prerequisite concepts, or curriculum standards</li> </ul>"},{"location":"chapters/10-quality-evaluation-frameworks/#the-microsim-json-schema","title":"The MicroSim JSON Schema","text":"<p>The schema defines several major sections:</p> <p>Dublin Core Metadata (dublinCore): <pre><code>{\n  \"dublinCore\": {\n    \"title\": \"Gravity Simulator\",\n    \"creator\": [\"Dan McCreary\"],\n    \"subject\": [\"physics\", \"gravity\", \"mechanics\"],\n    \"description\": \"Interactive simulation demonstrating gravitational acceleration\",\n    \"date\": \"2024-01-15T10:30:00Z\",\n    \"type\": \"Interactive Simulation\",\n    \"format\": \"text/html\",\n    \"rights\": \"CC BY 4.0\"\n  }\n}\n</code></pre></p> <p>Search Metadata (search): <pre><code>{\n  \"search\": {\n    \"tags\": [\"physics\", \"gravity\", \"freefall\", \"STEM\"],\n    \"visualizationType\": [\"simulation\", \"animation\"],\n    \"interactionLevel\": \"moderate\",\n    \"complexity\": 4,\n    \"relatedConcepts\": [\"acceleration\", \"mass\", \"velocity\"]\n  }\n}\n</code></pre></p> <p>Educational Metadata (educational): <pre><code>{\n  \"educational\": {\n    \"gradeLevel\": [\"9\", \"10\", \"11\", \"12\"],\n    \"subjectArea\": [\"Physics\"],\n    \"topic\": [\"Newtonian mechanics\", \"Gravitational acceleration\"],\n    \"learningObjectives\": [\n      \"Demonstrate that gravitational acceleration is independent of mass\"\n    ],\n    \"bloomsTaxonomy\": [\"Understand\", \"Apply\"],\n    \"prerequisites\": [\"Basic algebra\", \"Concept of velocity\"],\n    \"difficulty\": \"Intermediate\"\n  }\n}\n</code></pre></p> <p>Technical Metadata (technical): <pre><code>{\n  \"technical\": {\n    \"framework\": \"p5.js\",\n    \"version\": \"1.2.0\",\n    \"canvasDimensions\": {\n      \"width\": 800,\n      \"height\": 600,\n      \"responsive\": true\n    },\n    \"accessibility\": {\n      \"screenReader\": true,\n      \"keyboardNavigation\": true,\n      \"colorContrast\": true\n    }\n  }\n}\n</code></pre></p>"},{"location":"chapters/10-quality-evaluation-frameworks/#why-generative-ai-loves-json-schemas","title":"Why Generative AI Loves JSON Schemas","text":"<p>Here's an optimistic insight: Large Language Models are remarkably good at following JSON Schema specifications. When you provide a well-defined schema, AI systems can:</p> <ul> <li>Generate valid metadata.json files for new MicroSims</li> <li>Validate existing metadata against schema requirements</li> <li>Suggest missing fields based on content analysis</li> <li>Maintain consistency across large MicroSim libraries</li> </ul> <p>This precision is a gift for quality automation. Instead of hoping an AI \"gets it right,\" you define exactly what \"right\" means in a machine-readable format.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#metadata-quality-scoring","title":"Metadata Quality Scoring","text":"<p>Beyond structural validation, metadata can be evaluated for quality and completeness:</p> Criterion Description Weight Required fields present All required Dublin Core and educational fields 40% Keyword richness Multiple relevant subject tags and search keywords 15% Educational detail Learning objectives, Bloom's levels, prerequisites 20% Technical completeness Framework, dimensions, accessibility info 15% Usage data Popularity metrics, effectiveness studies (if available) 10% <p>Future metadata extensions may include:</p> <ul> <li>Popularity metrics: View counts, embedding frequency, user ratings</li> <li>Effectiveness data: Links to research studies demonstrating learning outcomes</li> <li>Version history: Tracking iterations and improvements over time</li> <li>Remix relationships: Which MicroSims were derived from this one?</li> </ul>"},{"location":"chapters/10-quality-evaluation-frameworks/#diagram-metadata-enabled-search","title":"Diagram: Metadata-Enabled Search","text":"Metadata-Enabled Search <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Students will apply metadata filtering to efficiently search a MicroSim database by subject, grade level, and JavaScript library.</p> <p>Canvas layout: - Left panel (300px): Filter controls - Right panel (remaining): Search results grid</p> <p>Visual elements: - Dropdown filters: Subject Area, Grade Level, JS Library, Bloom's Level - Tag cloud showing popular search terms - Results displayed as cards with thumbnail, title, description - Quality score badge on each result card - Click card to view full metadata</p> <p>Interactive controls: - Dropdown: Subject Area (Math, Science, Physics, etc.) - Dropdown: Grade Level (K through Graduate) - Dropdown: JS Library (p5.js, vis-network, Chart.js, etc.) - Checkbox group: Bloom's Taxonomy levels - Slider: Minimum quality score (0-100) - Search button - Reset filters button</p> <p>Default parameters: - No filters applied initially - Results sorted by quality score descending</p> <p>Behavior: - Filter changes update results in real-time - Empty state shows \"No results match your criteria\" - Click result card to view full metadata modal - Shows count of matching results</p> <p>Implementation: HTML/CSS/JavaScript with mock data or API connection</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#automated-vs-human-evaluation","title":"Automated vs. Human Evaluation","text":"<p>Not all evaluation tasks require human judgment. Understanding when to automate and when to rely on human expertise is key to efficient quality assurance.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#what-can-be-automated","title":"What Can Be Automated","text":"<p>Automated evaluation excels at objective, rule-based checks:</p> <ul> <li>Structural validation: File existence, YAML syntax, JSON schema compliance</li> <li>Responsiveness testing: Viewport simulation across breakpoints</li> <li>Performance metrics: Load time, memory usage, frame rate</li> <li>Accessibility checks: Color contrast ratios, ARIA labels, keyboard navigation</li> <li>Link verification: Broken links, missing images</li> <li>Code quality: Linting, syntax errors, deprecated function usage</li> </ul> <p>Automated tools can run these checks consistently across hundreds of MicroSims without fatigue or variation.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#what-requires-human-evaluation","title":"What Requires Human Evaluation","text":"<p>Humans remain essential for subjective, context-dependent judgments:</p> <ul> <li>Pedagogical alignment: Does this actually teach the concept?</li> <li>Engagement quality: Is it engaging without being distracting?</li> <li>Visual aesthetics: Does it look professional and inviting?</li> <li>Cultural appropriateness: Are examples inclusive and sensitive?</li> <li>Intuitive design: Can users figure it out without instructions?</li> <li>Misconception handling: Does it address common student errors?</li> </ul> <p>Think of it this way: Computers can verify that a fullscreen button exists; only humans can judge whether students actually understand what they're supposed to learn.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#the-hybrid-approach","title":"The Hybrid Approach","text":"<p>The most effective quality assurance combines automated pre-checks with human review:</p> <ol> <li>Automated tier: Run all rule-based checks; flag issues for human review</li> <li>Human tier: Focus expert attention on pedagogical and UX evaluation</li> <li>Crowd tier: Gather feedback from actual learners through user testing</li> </ol> <p>This pipeline ensures that human cognitive resources are spent on judgments that require human cognition, while machines handle the tedious verification work.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#diagram-automated-vs-human-evaluation-matrix","title":"Diagram: Automated vs. Human Evaluation Matrix","text":"Automated vs. Human Evaluation Matrix <p>Type: diagram</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Students will analyze evaluation criteria to determine appropriate use of automated versus human evaluation methods.</p> <p>Layout: 2x2 matrix with automation potential (high/low) on X-axis and judgment complexity (objective/subjective) on Y-axis</p> <p>Quadrants: - Top-left (High automation, Objective): \"Fully Automate\"   - File structure checks   - JSON validation   - Responsiveness testing   - Link verification</p> <ul> <li>Top-right (Low automation, Objective): \"Semi-Automate\"</li> <li>Accessibility testing (some aspects)</li> <li>Code quality checks</li> <li> <p>Performance benchmarks</p> </li> <li> <p>Bottom-left (High automation, Subjective): \"AI-Assisted\"</p> </li> <li>Content summarization</li> <li>Keyword extraction</li> <li> <p>Similarity detection</p> </li> <li> <p>Bottom-right (Low automation, Subjective): \"Human Required\"</p> </li> <li>Pedagogical effectiveness</li> <li>Visual aesthetics</li> <li>Cultural sensitivity</li> <li>Engagement quality</li> </ul> <p>Color coding: - Green: Fully automate - Blue: Semi-automate - Yellow: AI-assisted - Purple: Human required</p> <p>Implementation: HTML/CSS grid or SVG diagram</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#documentation-standards-writing-for-reuse","title":"Documentation Standards: Writing for Reuse","text":"<p>Good documentation transforms a one-time creation into a reusable asset. Documentation standards ensure that anyone\u2014including your future self\u2014can understand, maintain, and adapt a MicroSim.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#the-documentation-checklist","title":"The Documentation Checklist","text":"<p>Every MicroSim should include:</p> <ol> <li>Purpose statement: What does this simulation teach? (1-2 sentences)</li> <li>Usage instructions: How do learners interact with it? (step-by-step)</li> <li>Learning objectives: What should learners be able to do afterward?</li> <li>Target audience: Grade level, prerequisites, assumed knowledge</li> <li>Technical requirements: Browser compatibility, device recommendations</li> <li>Customization guide: How can educators adapt it for different contexts?</li> <li>Known limitations: What doesn't this simulation cover or do well?</li> </ol>"},{"location":"chapters/10-quality-evaluation-frameworks/#lesson-plan-section","title":"Lesson Plan Section","text":"<p>The lesson plan transforms a MicroSim from \"neat demo\" to \"classroom-ready tool\":</p> <pre><code>## Lesson Plan\n\n### Learning Objectives\nAfter completing this simulation, students will be able to:\n1. Explain that gravitational acceleration is independent of object mass\n2. Predict the fall time of objects of different masses\n\n### Target Audience\n- Grade level: 9-12 Physics\n- Prerequisites: Basic understanding of velocity and acceleration\n\n### Activities\n1. **Prediction phase** (5 minutes): Ask students to predict which ball falls faster\n2. **Exploration phase** (10 minutes): Students manipulate parameters and observe\n3. **Explanation phase** (10 minutes): Discuss observations, introduce formula\n4. **Extension** (optional): Introduce air resistance toggle\n\n### Assessment\n- Formative: Embedded prediction prompts in simulation\n- Summative: Exit ticket with transfer question\n</code></pre>"},{"location":"chapters/10-quality-evaluation-frameworks/#rubric-development-building-your-own-evaluation-tools","title":"Rubric Development: Building Your Own Evaluation Tools","text":"<p>The completeness rubric provided in this chapter is a starting point, not a final destination. Different contexts may require different evaluation criteria.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#principles-of-effective-rubric-design","title":"Principles of Effective Rubric Design","text":"<ol> <li>Specific criteria: Vague items like \"good quality\" can't be measured</li> <li>Observable indicators: What evidence demonstrates each criterion?</li> <li>Consistent scoring: Different evaluators should arrive at similar scores</li> <li>Appropriate weighting: Not all criteria are equally important</li> <li>Actionable feedback: Scores should indicate what to improve</li> </ol>"},{"location":"chapters/10-quality-evaluation-frameworks/#customization-example-physics-microsim-rubric","title":"Customization Example: Physics MicroSim Rubric","text":"<p>A physics department might extend the standard rubric with domain-specific criteria:</p> Criterion Description Points Physical accuracy Equations match textbook formulations 15 Unit labels All quantities display appropriate units 10 Edge case behavior Simulation handles extreme values correctly 10 Misconception targeting Addresses documented student misconceptions 15"},{"location":"chapters/10-quality-evaluation-frameworks/#diagram-evaluation-rubric-builder","title":"Diagram: Evaluation Rubric Builder","text":"Evaluation Rubric Builder <p>Type: microsim</p> <p>Bloom Taxonomy: Create (L6)</p> <p>Learning Objective: Students will create custom evaluation rubrics by selecting and weighting criteria appropriate to their educational context.</p> <p>Canvas layout: - Left panel (350px): Criterion bank organized by category - Center panel (300px): Active rubric being built - Right panel (250px): Weighting and scoring controls</p> <p>Visual elements: - Categorized lists of criteria (Technical, Pedagogical, UX, Domain-specific) - Drag-and-drop interface for adding criteria to rubric - Weight sliders for each criterion (0-100%) - Total weight indicator (must sum to 100%) - Preview of rubric as checklist</p> <p>Interactive controls: - Checkbox lists for selecting criteria from each category - Sliders to adjust weights - Text input for custom criteria - Button: Add Custom Criterion - Button: Export Rubric as Markdown - Button: Export Rubric as JSON - Button: Reset to Default</p> <p>Default parameters: - Standard completeness rubric pre-loaded - Weights set to default values</p> <p>Behavior: - Drag criteria from bank to active rubric - Weight sliders automatically rebalance to sum to 100% - Preview updates in real-time - Export generates formatted rubric document</p> <p>Implementation: HTML/CSS/JavaScript with drag-and-drop library</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#designing-for-reusability-future-proofing-your-work","title":"Designing for Reusability: Future-Proofing Your Work","text":"<p>The final pillar of quality is reusability. A MicroSim that works today but can't be adapted, extended, or integrated tomorrow has limited value.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#reusability-principles","title":"Reusability Principles","text":"<ol> <li>Separation of concerns: Keep content, logic, and presentation separate</li> <li>Parameterization: Make key values adjustable without code changes</li> <li>Documentation: Enable others to understand and modify</li> <li>Standard formats: Use common file types and coding conventions</li> <li>Open licensing: Choose licenses that permit educational reuse</li> <li>Metadata completeness: Enable discovery through rich metadata</li> </ol>"},{"location":"chapters/10-quality-evaluation-frameworks/#the-metadata-reusability-connection","title":"The Metadata-Reusability Connection","text":"<p>Complete metadata directly enables reuse:</p> <ul> <li>Search and discovery: Others can find your MicroSim when they need it</li> <li>Adaptation: Metadata reveals what prerequisites and concepts are involved</li> <li>Quality signals: Quality scores help educators choose reliable resources</li> <li>Version tracking: Future authors know which version to reference</li> <li>Attribution: Clear creator information enables proper credit</li> </ul> <p>Remember: You can't reuse what you can't find. Invest in metadata now, and your work pays dividends across the entire educational community.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#summary-the-quality-mindset","title":"Summary: The Quality Mindset","text":"<p>Quality evaluation isn't a one-time checkpoint\u2014it's a mindset that infuses every stage of MicroSim development. From the first specification through final deployment, asking \"Is this technically sound? Pedagogically effective? User-friendly? Findable and reusable?\" keeps your work aligned with its educational mission.</p> <p>The frameworks in this chapter give you:</p> <ul> <li>Three-Lens Evaluation Model: Technical, Pedagogical, and UX perspectives</li> <li>100-Point Completeness Rubric: Measurable standards for structural quality</li> <li>Standardization Workflow: Step-by-step process for bringing MicroSims to standard</li> <li>Metadata Schema: Structured format for discoverability and search</li> <li>Automated/Human Balance: Efficient allocation of evaluation resources</li> <li>Documentation Standards: Templates for reusable lesson planning</li> <li>Rubric Development: Tools for creating context-specific evaluation criteria</li> </ul> <p>Armed with these tools, you're ready not just to create MicroSims, but to create good MicroSims\u2014simulations that work reliably, teach effectively, engage appropriately, and remain useful long after their first deployment.</p> <p>And that's the whole point, isn't it? We're not just making interactive widgets. We're building tools that help people understand the world better. That's worth doing well.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Quality evaluation examines three dimensions: Technical (does it work?), Pedagogical (does it teach?), and UX (is it usable?)</li> <li>The Completeness Quality Score provides a 100-point rubric that LLMs and humans can consistently apply</li> <li>Standardization follows a defined workflow: check existing score, validate structure, verify type-specific requirements, calculate and record quality</li> <li>Metadata is essential for discoverability\u2014\"You can't reuse what you can't find\"</li> <li>The MicroSim JSON Schema provides comprehensive structure for Dublin Core, educational, technical, and search metadata</li> <li>Automated evaluation handles objective checks; human evaluation handles subjective judgments</li> <li>Documentation standards transform one-time creations into reusable educational assets</li> <li>Custom rubrics can extend the standard framework for domain-specific needs</li> </ul>"},{"location":"chapters/10-quality-evaluation-frameworks/#reflection-questions","title":"Reflection Questions","text":"How would you prioritize evaluation dimensions for a time-limited review? <p>If you only had 15 minutes to evaluate a MicroSim, you might focus on: 1. Does it load without errors? (Technical pass/fail) 2. Does it address its stated learning objective? (Pedagogical alignment) 3. Can a first-time user figure out how to interact with it? (Intuitiveness)</p> What metadata fields would be most valuable for your teaching context? <p>Consider your specific needs: - Do you need to filter by curriculum standards? - Is accessibility information critical for your learners? - Would Bloom's Taxonomy levels help you select appropriate complexity?</p> When would human evaluation be worth the additional time investment? <p>Human evaluation is essential when: - The MicroSim addresses common misconceptions (requires domain expertise) - Cultural sensitivity is important (requires contextual judgment) - The simulation will be widely deployed (higher stakes justify investment)</p>"},{"location":"chapters/10-quality-evaluation-frameworks/#references","title":"References","text":"<ol> <li> <p>Anderson, L. W., &amp; Krathwohl, D. R. (Eds.). (2001). A taxonomy for learning, teaching, and assessing: A revision of Bloom's taxonomy of educational objectives. Longman.</p> </li> <li> <p>Dublin Core Metadata Initiative. (2020). DCMI Metadata Terms. https://www.dublincore.org/specifications/dublin-core/dcmi-terms/</p> </li> <li> <p>Nielsen, J. (1994). Usability engineering. Morgan Kaufmann.</p> </li> <li> <p>Web Content Accessibility Guidelines (WCAG) 2.1. (2018). W3C Recommendation. https://www.w3.org/TR/WCAG21/</p> </li> <li> <p>JSON Schema. (2020). Understanding JSON Schema. https://json-schema.org/understanding-json-schema/</p> </li> </ol>"},{"location":"chapters/10-quality-evaluation-frameworks/quiz/","title":"Quiz: Quality Evaluation Frameworks","text":"<p>Test your understanding of MicroSim quality evaluation across technical, pedagogical, and user experience dimensions, including rubrics, metadata standards, and strategies for search and reuse.</p>"},{"location":"chapters/10-quality-evaluation-frameworks/quiz/#1-what-are-the-three-dimensions-of-the-three-lens-evaluation-model","title":"1. What are the three dimensions of the Three-Lens Evaluation Model?","text":"<ol> <li>Speed, Cost, and Quality</li> <li>Technical, Pedagogical, and User Experience (UX)</li> <li>Design, Development, and Deployment</li> <li>Input, Process, and Output</li> </ol> Show Answer <p>The correct answer is B. The Three-Lens Evaluation Model examines MicroSim quality across three dimensions: Technical (does it work reliably across devices and browsers?), Pedagogical (does it actually teach what it claims to teach?), and User Experience (is it intuitive and engaging without being distracting?). The magic happens at the intersection of all three\u2014a MicroSim must succeed in all dimensions.</p> <p>Concept Tested: Technical Evaluation, Pedagogical Evaluation, UX Evaluation</p> <p>See: Chapter Content</p>"},{"location":"chapters/10-quality-evaluation-frameworks/quiz/#2-what-is-output-validation-in-the-context-of-microsim-quality","title":"2. What is output validation in the context of MicroSim quality?","text":"<ol> <li>Checking that the MicroSim file size meets requirements</li> <li>Verifying that a MicroSim produces expected results given specific inputs</li> <li>Validating user credentials before allowing access</li> <li>Confirming that output files are saved to the correct location</li> </ol> Show Answer <p>The correct answer is B. Output validation is the process of verifying that a MicroSim produces expected results given specific inputs. It includes visual correctness (elements render properly), mathematical accuracy (calculations are correct), behavioral consistency (responds predictably), edge case handling (handles extreme values), and state management (resets properly). This is critical for AI-generated code which can contain subtle logical errors.</p> <p>Concept Tested: Output Validation</p> <p>See: Chapter Content</p>"},{"location":"chapters/10-quality-evaluation-frameworks/quiz/#3-what-must-be-disabled-when-embedding-vis-network-in-a-textbook-via-iframe","title":"3. What must be disabled when embedding vis-network in a textbook via iframe?","text":"<ol> <li>Node colors and edge labels</li> <li>Physics simulation and layout algorithms</li> <li>zoomView (mouse zoom) to prevent accidental zooming when scrolling</li> <li>All interactive features</li> </ol> Show Answer <p>The correct answer is C. When embedding vis-network in an iframe, you MUST disable zoomView and dragView in the interaction options. Otherwise, users scrolling through the textbook will accidentally zoom the diagram instead of scrolling the page. This is one of the most common usability mistakes in educational visualizations.</p> <p>Concept Tested: Responsiveness Testing, Bug Identification</p> <p>See: Chapter Content</p>"},{"location":"chapters/10-quality-evaluation-frameworks/quiz/#4-what-does-objective-alignment-verify-in-pedagogical-evaluation","title":"4. What does objective alignment verify in pedagogical evaluation?","text":"<ol> <li>That the MicroSim file structure follows naming conventions</li> <li>That every element of a MicroSim serves its stated learning objective without scope creep</li> <li>That learning objectives are written in Bloom's Taxonomy format</li> <li>That multiple objectives can be achieved simultaneously</li> </ol> Show Answer <p>The correct answer is B. Objective alignment verifies that every element of a MicroSim serves its stated learning objective. Scope creep is a constant temptation\u2014a simulation designed to teach supply and demand might accumulate features for market equilibrium, price elasticity, and international trade until it teaches nothing well. Key questions include: does it directly address the objective? Are there distracting features? Can learners complete it without engaging with the key insight?</p> <p>Concept Tested: Objective Alignment</p> <p>See: Chapter Content</p>"},{"location":"chapters/10-quality-evaluation-frameworks/quiz/#5-what-is-the-purpose-of-the-completeness-quality-score-rubric","title":"5. What is the purpose of the Completeness Quality Score rubric?","text":"<ol> <li>To measure how entertaining the MicroSim is</li> <li>To evaluate whether a MicroSim includes all required components for standardization, providing a measurable 100-point score</li> <li>To determine the MicroSim's loading speed</li> <li>To calculate how many users can access it simultaneously</li> </ol> Show Answer <p>The correct answer is B. The Completeness Quality Score is a 100-point rubric that evaluates whether a MicroSim includes all required components for standardization. Note that it measures completeness, not usability\u2014a MicroSim can achieve a perfect 100 and still be pedagogically questionable. It's a checklist for structural integrity, with scores interpreted as: 90-100 (production ready), 70-89 (good quality), 50-69 (needs work), below 50 (incomplete).</p> <p>Concept Tested: Quality Score, Evaluation Rubric</p> <p>See: Chapter Content</p>"},{"location":"chapters/10-quality-evaluation-frameworks/quiz/#6-what-is-the-golden-rule-of-microsim-discoverability-mentioned-in-the-chapter","title":"6. What is the golden rule of MicroSim discoverability mentioned in the chapter?","text":"<ol> <li>\"The best MicroSim is the one you create yourself\"</li> <li>\"You can't reuse what you can't find\"</li> <li>\"Quality is more important than quantity\"</li> <li>\"Simple designs are always better\"</li> </ol> Show Answer <p>The correct answer is B. \"You can't reuse what you can't find\" is the golden rule of MicroSim discoverability. A brilliant MicroSim buried in an undocumented folder, lacking keywords and categorization, might as well not exist. Metadata is the bridge between creation and reuse\u2014it makes simulations findable, comparable, and adaptable through search tools, AI agents, and educator filters.</p> <p>Concept Tested: MicroSim Metadata, Search and Reuse</p> <p>See: Chapter Content</p>"},{"location":"chapters/10-quality-evaluation-frameworks/quiz/#7-what-metadata-standard-does-the-microsim-json-schema-use-as-its-foundation","title":"7. What metadata standard does the MicroSim JSON Schema use as its foundation?","text":"<ol> <li>XML Schema Definition (XSD)</li> <li>Dublin Core metadata standards, extended with educational and technical specifications</li> <li>YAML Ain't Markup Language (YAML)</li> <li>Resource Description Framework (RDF)</li> </ol> Show Answer <p>The correct answer is B. The MicroSim ecosystem uses a comprehensive JSON Schema based on Dublin Core metadata standards, extended with educational and technical specifications. Dublin Core provides the foundation (title, creator, subject, description, date, type, format, rights), while extensions add search metadata, educational metadata (grade level, Bloom's taxonomy, prerequisites), and technical metadata (framework, dimensions, accessibility).</p> <p>Concept Tested: MicroSim Metadata, Documentation Standard</p> <p>See: Chapter Content</p>"},{"location":"chapters/10-quality-evaluation-frameworks/quiz/#8-what-is-the-key-difference-between-automated-and-human-evaluation","title":"8. What is the key difference between automated and human evaluation?","text":"<ol> <li>Automated is always better than human evaluation</li> <li>Automated handles objective, rule-based checks while human evaluation handles subjective, context-dependent judgments</li> <li>Human evaluation is faster than automated evaluation</li> <li>Automated evaluation can assess pedagogical effectiveness</li> </ol> Show Answer <p>The correct answer is B. Automated evaluation excels at objective, rule-based checks (structural validation, responsiveness testing, performance metrics, accessibility checks, link verification, code quality). Human evaluation remains essential for subjective, context-dependent judgments (pedagogical alignment, engagement quality, visual aesthetics, cultural appropriateness, intuitive design, misconception handling). The hybrid approach uses automated pre-checks and reserves human attention for judgments requiring human cognition.</p> <p>Concept Tested: Automated Evaluation, Human Evaluation</p> <p>See: Chapter Content</p>"},{"location":"chapters/10-quality-evaluation-frameworks/quiz/#9-what-does-cognitive-level-match-verify-in-pedagogical-evaluation","title":"9. What does cognitive level match verify in pedagogical evaluation?","text":"<ol> <li>That the MicroSim matches the user's cognitive abilities</li> <li>That the MicroSim's complexity matches the targeted Bloom's Taxonomy level for the learning objective</li> <li>That all cognitive load is eliminated from the interface</li> <li>That learners of all ages can use the MicroSim equally well</li> </ol> Show Answer <p>The correct answer is B. Cognitive level match verifies that a MicroSim's design patterns match the targeted Bloom's Taxonomy level. A simulation designed for \"Remember\" level (vocabulary flashcards) requires different patterns than one targeting \"Analyze\" level (comparing data sets). A common mismatch occurs when a learning objective targets \"Analyze\" but the MicroSim only supports \"Understand\"\u2014explaining concepts beautifully but never challenging learners to discover relationships.</p> <p>Concept Tested: Cognitive Level Match</p> <p>See: Chapter Content</p>"},{"location":"chapters/10-quality-evaluation-frameworks/quiz/#10-what-is-engagement-balance-in-ux-evaluation","title":"10. What is engagement balance in UX evaluation?","text":"<ol> <li>Balancing the number of users who can access the MicroSim simultaneously</li> <li>Finding the sweet spot between \"so boring I'm falling asleep\" and \"so gamified I forgot I'm learning\"</li> <li>Ensuring equal engagement across all age groups</li> <li>Balancing visual elements evenly across the screen</li> </ol> Show Answer <p>The correct answer is B. Engagement balance seeks the Goldilocks zone where learners are sufficiently motivated to persist without being distracted by bells, whistles, and virtual confetti. Too little engagement means plain text and no feedback; too much means constant motion and points for everything. The key question: does the engagement serve the learning, or does learning serve the engagement?</p> <p>Concept Tested: Engagement Balance</p> <p>See: Chapter Content</p>"},{"location":"chapters/11-user-testing-iteration/","title":"User Testing and Iteration","text":""},{"location":"chapters/11-user-testing-iteration/#summary","title":"Summary","text":"<p>This chapter covers user testing methodologies and the iterative refinement process for MicroSims. You will learn think-aloud protocols, A/B testing design, and techniques for gathering learner feedback across different ages and abilities. The chapter addresses observation techniques, test interpretation, ethical research considerations, and the complete design-test-refine cycle. You will also master change prioritization, distinguishing critical from nice-to-have changes, deciding between fundamental redesign and incremental improvement, preventing scope creep, defining completion criteria, maintaining change logs, and documenting design rationale.</p>"},{"location":"chapters/11-user-testing-iteration/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following concepts from the learning graph:</p> <ol> <li>Iterative Refinement</li> <li>Conversation Prompting</li> <li>Think-Aloud Protocol</li> <li>A/B Testing</li> <li>Learner Feedback</li> <li>Age-Based Feedback</li> <li>Ability-Based Feedback</li> <li>Observation Technique</li> <li>Test Interpretation</li> <li>Change Prioritization</li> <li>Ethical Research</li> <li>Design-Test-Refine Cycle</li> <li>Critical Changes</li> <li>Nice-to-Have Changes</li> <li>Fundamental Redesign</li> <li>Incremental Improvement</li> <li>Scope Creep Prevention</li> <li>Completion Criteria</li> <li>Change Log</li> <li>Design Rationale</li> <li>Google Analytics</li> <li>Simulation Tracking</li> <li>xAPI</li> <li>Tracking Who-What-When</li> <li>Learning Record Store</li> <li>xAPI events to a LRS</li> <li>IEEE Learning Standards</li> <li>Total Learning Architecture</li> </ol>"},{"location":"chapters/11-user-testing-iteration/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Prerequisite Analysis and MicroSim Fundamentals</li> <li>Chapter 6: Adapting for Audience Levels</li> <li>Chapter 10: Quality Evaluation Frameworks</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#introduction-the-testing-mindset","title":"Introduction: The Testing Mindset","text":"<p>Here's a truth that separates good instructional designers from great ones: the MicroSim you ship is never the MicroSim you first built.</p> <p>No matter how brilliant your initial design, real learners will interact with it in ways you never anticipated. They'll miss the obvious button, ignore your carefully crafted instructions, and somehow discover that dragging the slider to -47 causes the entire simulation to explode (metaphorically, we hope). This isn't a failure\u2014it's valuable data.</p> <p>User testing and iteration transform your \"pretty good\" MicroSim into an \"actually works for real humans\" MicroSim. Think of it as the difference between a rehearsal and opening night. Rehearsals are where you discover that the trapdoor sticks, the lighting cue is three seconds late, and the lead actor is allergic to the prop flowers. Better to discover these things before the audience arrives.</p> <p>This chapter equips you with the methodologies to gather meaningful feedback, the analytics infrastructure to track what learners actually do (not just what you hope they do), and the iteration frameworks to systematically improve your work. Along the way, we'll explore the cutting-edge world of xAPI and Learning Record Stores\u2014technologies that make \"who did what when\" not just a philosophical question but an answerable one.</p> <p>Let's dive in. Your learners are waiting, and they have opinions.</p>"},{"location":"chapters/11-user-testing-iteration/#two-testing-contexts-standalone-vs-intelligent-textbook","title":"Two Testing Contexts: Standalone vs. Intelligent Textbook","text":"<p>Before we get into specific methodologies, it's crucial to understand that MicroSims live in two very different habitats, and each requires its own testing approach.</p>"},{"location":"chapters/11-user-testing-iteration/#standalone-microsim-testing","title":"Standalone MicroSim Testing","text":"<p>In the standalone context, a teacher presents a MicroSim during a class lecture, projecting it on the screen while students follow along on their own devices. Testing in this environment emphasizes:</p> <ul> <li>Synchronization: Can all students keep up with the teacher's pace?</li> <li>Technical reliability: Does it work on the mix of devices in the classroom?</li> <li>Attention management: Does the MicroSim enhance or compete with the lecture?</li> <li>Discussion prompts: Does it generate productive class conversation?</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#intelligent-textbook-integration-testing","title":"Intelligent Textbook Integration Testing","text":"<p>When a MicroSim lives inside an interactive intelligent textbook, the testing challenges multiply. Here's the key insight that changes everything:</p> <p>Learning is Non-Linear</p> <p>In an interactive textbook, students \"choose their own adventure.\" They hop around driven by curiosity, skip sections they think they know, and revisit concepts they found confusing. Your MicroSim must work regardless of what path brought the student there.</p> <p>Integration testing must verify:</p> <ul> <li>Context independence: Does the MicroSim make sense without reading the preceding paragraphs?</li> <li>Navigation flow: Can students easily return to where they were?</li> <li>Cross-reference validity: Do links to other sections still work?</li> <li>Progressive disclosure: Does complexity unfold appropriately for different entry points?</li> </ul> Testing Context Primary Focus Key Challenge Standalone Live classroom dynamics Synchronization with instruction Intelligent Textbook Non-linear navigation Context-independent comprehension Both Technical reliability Cross-device compatibility"},{"location":"chapters/11-user-testing-iteration/#think-aloud-protocol-getting-inside-learners-heads","title":"Think-Aloud Protocol: Getting Inside Learners' Heads","text":"<p>The think-aloud protocol is the Swiss Army knife of user testing. It's simple, powerful, and reveals insights that no amount of analytics can capture. The basic premise: ask learners to verbalize their thoughts while using your MicroSim.</p>"},{"location":"chapters/11-user-testing-iteration/#how-it-works","title":"How It Works","text":"<ol> <li>Set up: Provide the learner with the MicroSim and a recording method (audio, video, or notes)</li> <li>Instruct: \"Please say out loud everything you're thinking as you work through this simulation\"</li> <li>Observe: Watch and listen without interrupting (this is harder than it sounds)</li> <li>Follow up: Ask clarifying questions after they've finished</li> </ol>"},{"location":"chapters/11-user-testing-iteration/#what-youll-hear-and-what-it-means","title":"What You'll Hear (And What It Means)","text":"Verbalization What It Reveals \"I wonder if I should click this...\" Unclear affordances \"Wait, what just happened?\" Missing feedback \"I think this means...\" Testing understanding \"Oh! That makes sense now!\" Effective visualization [Long silence] Confusion or deep thought \"This is annoying\" UX friction"},{"location":"chapters/11-user-testing-iteration/#the-art-of-not-intervening","title":"The Art of Not Intervening","text":"<p>The hardest part of think-aloud testing is keeping your mouth shut. When a participant struggles with something obvious (to you), the instinct to help is overwhelming. Resist it. Their struggle is your data.</p> <p>That said, some intervention guidelines:</p> <ul> <li>Do intervene if they're about to give up entirely</li> <li>Don't intervene if they're merely frustrated</li> <li>Do intervene if technical issues unrelated to your MicroSim cause problems</li> <li>Don't intervene to \"explain what you meant\"</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#diagram-think-aloud-protocol-workflow","title":"Diagram: Think-Aloud Protocol Workflow","text":"Think-Aloud Protocol Workflow <p>Type: workflow</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Students will apply the think-aloud protocol to gather qualitative feedback on MicroSim usability.</p> <p>Visual style: Flowchart with phases and decision points</p> <p>Phases: 1. Start: \"Prepare Testing Session\"    Hover text: \"Set up recording equipment, prepare consent forms, ready the MicroSim\"</p> <ol> <li> <p>Process: \"Brief Participant\"    Hover text: \"Explain think-aloud method, reassure them the MicroSim is being tested, not them\"</p> </li> <li> <p>Process: \"Begin Recording\"    Hover text: \"Start audio/video capture, note start time\"</p> </li> <li> <p>Process: \"Participant Uses MicroSim\"    Hover text: \"Observe silently, take notes on behaviors and verbalizations\"</p> </li> <li> <p>Decision: \"Participant Stuck?\"    Hover text: \"Are they making no progress for extended period?\"</p> </li> </ol> <p>6a. Process: \"Provide Minimal Hint\" (if Yes)     Hover text: \"Give smallest possible nudge to continue\"</p> <p>6b. Process: \"Continue Observing\" (if No)     Hover text: \"Let them work through challenges\"</p> <ol> <li> <p>Decision: \"Task Complete?\"    Hover text: \"Has participant finished or given up?\"</p> </li> <li> <p>Process: \"Conduct Debrief\"    Hover text: \"Ask follow-up questions about their experience\"</p> </li> <li> <p>Process: \"Analyze Recording\"    Hover text: \"Review footage, code behaviors, identify patterns\"</p> </li> <li> <p>End: \"Document Insights\"     Hover text: \"Write up findings with specific examples\"</p> </li> </ol> <p>Color coding: - Blue: Preparation steps - Green: Active testing - Yellow: Decision points - Purple: Analysis steps</p> <p>Implementation: Mermaid or HTML/CSS/JavaScript flowchart</p>"},{"location":"chapters/11-user-testing-iteration/#observation-techniques-watching-without-interfering","title":"Observation Techniques: Watching Without Interfering","text":"<p>While think-aloud protocols capture what learners are thinking, systematic observation captures what they're doing. Sometimes these tell very different stories.</p>"},{"location":"chapters/11-user-testing-iteration/#structured-observation-categories","title":"Structured Observation Categories","text":"<p>Professional observers typically track:</p> <ul> <li>Navigation patterns: Where do they click first? What path do they take?</li> <li>Time allocation: How long on each section? Where do they linger or rush?</li> <li>Error recovery: When they make mistakes, how do they recover?</li> <li>Attention indicators: Eye tracking, cursor hovering, scrolling patterns</li> <li>Emotional responses: Facial expressions, posture, sighs of frustration or delight</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#creating-an-observation-checklist","title":"Creating an Observation Checklist","text":"<p>Before testing, create a structured checklist of specific behaviors to watch for:</p> <pre><code>## Observation Checklist: Gravity Simulator MicroSim\n\n### Initial Engagement (First 30 seconds)\n- [ ] Notices title and description\n- [ ] Reads instructions before interacting\n- [ ] Immediately starts clicking controls\n- [ ] Appears confused about purpose\n\n### Control Usage\n- [ ] Uses mass slider\n- [ ] Uses height slider\n- [ ] Clicks reset button\n- [ ] Attempts to drag objects directly\n\n### Understanding Indicators\n- [ ] Predicts outcome before running simulation\n- [ ] Compares different parameter combinations\n- [ ] Tests edge cases (maximum/minimum values)\n- [ ] Articulates correct relationship\n</code></pre>"},{"location":"chapters/11-user-testing-iteration/#remote-observation-tools","title":"Remote Observation Tools","text":"<p>When in-person observation isn't possible, remote tools can capture much of the same data:</p> <ul> <li>Screen recording software: Captures exactly what learners see and do</li> <li>Click heatmaps: Aggregate data showing where many users click</li> <li>Scroll depth tracking: How far down the page do learners actually go?</li> <li>Session replay tools: Watch recordings of individual sessions</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#gathering-learner-feedback-age-and-ability-considerations","title":"Gathering Learner Feedback: Age and Ability Considerations","text":"<p>Different learners require different feedback collection approaches. A method that works beautifully with graduate students might completely fail with fourth graders.</p>"},{"location":"chapters/11-user-testing-iteration/#age-based-feedback-strategies","title":"Age-Based Feedback Strategies","text":"Age Group Effective Methods What to Avoid K-5 Drawing responses, emoji ratings, short verbal answers Long surveys, abstract questions 6-8 Quick polls, comparison choices, peer discussion Complex rating scales 9-12 Written reflections, rating scales, focus groups Overly childish approaches Undergraduate Surveys, interviews, analytics review Unpaid lengthy studies Graduate/Professional Detailed critiques, comparative analysis Oversimplification"},{"location":"chapters/11-user-testing-iteration/#ability-based-feedback-accommodations","title":"Ability-Based Feedback Accommodations","text":"<p>Learners with different abilities may need modified feedback methods:</p> <ul> <li>Visual impairments: Verbal interviews, audio recordings, screen reader compatibility testing</li> <li>Hearing impairments: Written surveys, chat-based feedback, visual rating scales</li> <li>Motor impairments: Extended time, alternative input methods, verbal responses</li> <li>Cognitive differences: Simplified questions, frequent breaks, visual supports</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#the-conversation-prompting-technique","title":"The Conversation Prompting Technique","text":"<p>Conversation prompting uses natural dialogue to extract feedback without the formality of structured interviews. It works especially well with younger learners who clam up when they feel \"tested.\"</p> <p>Instead of: \"On a scale of 1-5, how would you rate the usability of this simulation?\"</p> <p>Try: \"If you were telling a friend about this, what would you say?\"</p> <p>Instead of: \"Did you understand the learning objective?\"</p> <p>Try: \"What do you think this was trying to teach you?\"</p>"},{"location":"chapters/11-user-testing-iteration/#ab-testing-the-scientific-approach-to-design-decisions","title":"A/B Testing: The Scientific Approach to Design Decisions","text":"<p>When you have a design question that reasonable people disagree on, A/B testing provides an objective answer. Should the reset button be red or gray? Does adding sound effects improve learning? Is the horizontal or vertical layout more effective?</p>"},{"location":"chapters/11-user-testing-iteration/#ab-testing-fundamentals","title":"A/B Testing Fundamentals","text":"<p>An A/B test randomly assigns learners to one of two (or more) versions of your MicroSim and measures which performs better on a defined metric.</p> <p>Key components:</p> <ul> <li>Control (A): The current or baseline version</li> <li>Treatment (B): The version with your proposed change</li> <li>Metric: What you're measuring (completion rate, time to understanding, quiz scores)</li> <li>Sample size: Enough participants to detect meaningful differences</li> <li>Randomization: Each learner has equal chance of seeing either version</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#the-page-views-vs-simulation-tracking-problem","title":"The Page Views vs. Simulation Tracking Problem","text":"<p>Here's where many instructional designers hit a wall. Standard web analytics tools like Google Analytics are fantastic for tracking page views\u2014which pages visitors see, how long they stay, where they click. But for MicroSim A/B testing, page views aren't enough.</p> <p>Consider what Google Analytics can tell you:</p> <ul> <li>Student visited the Gravity Simulator page</li> <li>They stayed for 4 minutes</li> <li>They left to the \"Next Chapter\" page</li> </ul> <p>And what it cannot tell you:</p> <ul> <li>Did they actually interact with the simulation?</li> <li>Which sliders did they adjust?</li> <li>How many times did they reset?</li> <li>Did they test the hypothesis the simulation was designed to demonstrate?</li> </ul> <p>This is the fundamental difference between page-view tracking and simulation tracking. Page-view tracking knows you were in the room; simulation tracking knows what you did while you were there.</p>"},{"location":"chapters/11-user-testing-iteration/#diagram-page-view-vs-simulation-tracking","title":"Diagram: Page-View vs. Simulation Tracking","text":"Page-View vs. Simulation Tracking Comparison <p>Type: diagram</p> <p>Bloom Taxonomy: Analyze (L4)</p> <p>Learning Objective: Students will analyze the differences between page-view tracking and simulation tracking to select appropriate analytics for MicroSim evaluation.</p> <p>Layout: Side-by-side comparison with two columns</p> <p>Left Column - Page-View Tracking (Google Analytics): - Title: \"What Traditional Analytics See\" - Metrics shown:   - Page URL visited   - Time on page (aggregate)   - Referrer (where they came from)   - Device/browser information   - Geographic location   - Bounce rate - Limitation callout: \"Cannot see interactions within the page\" - Icon: Eye looking at a page outline</p> <p>Right Column - Simulation Tracking (xAPI): - Title: \"What xAPI Sees\" - Metrics shown:   - Each control interaction   - Slider value changes over time   - Button clicks with timestamps   - Prediction accuracy (if applicable)   - Time spent on specific activities   - Sequence of interactions - Advantage callout: \"Fine-grained learning behavior data\" - Icon: Magnifying glass looking at detailed interaction stream</p> <p>Center comparison: - Arrow pointing from left to right: \"More Detail \u2192\" - Text: \"The difference between knowing someone was in the room vs. knowing exactly what they did\"</p> <p>Color scheme: - Left: Orange/yellow (surface level) - Right: Blue/purple (deep analysis) - Center: Gray (neutral)</p> <p>Implementation: HTML/CSS or SVG diagram</p>"},{"location":"chapters/11-user-testing-iteration/#xapi-the-experience-api-for-fine-grained-tracking","title":"xAPI: The Experience API for Fine-Grained Tracking","text":"<p>This brings us to xAPI (the Experience API, sometimes called Tin Can API)\u2014the technology that bridges the gap between \"they visited the page\" and \"here's exactly how they learned.\"</p>"},{"location":"chapters/11-user-testing-iteration/#the-elegant-simplicity-of-who-what-when","title":"The Elegant Simplicity of Who-What-When","text":"<p>At its core, xAPI is beautifully simple. Every learning experience is recorded as a statement with three parts:</p> <p>Actor (Who) \u2192 Verb (What) \u2192 Object (When + Context)</p> <p>For example:</p> <ul> <li>Dan \u2192 completed \u2192 the Gravity Simulator at 2:34 PM</li> <li>Sarah \u2192 adjusted \u2192 the mass slider to 50kg</li> <li>Alex \u2192 predicted \u2192 that both balls would fall at the same speed</li> <li>Jordan \u2192 answered incorrectly \u2192 quiz question 3 about terminal velocity</li> </ul> <p>These statements are machine-readable, standardized, and can be aggregated across millions of learners to reveal patterns no individual observation could uncover.</p>"},{"location":"chapters/11-user-testing-iteration/#xapi-statement-structure","title":"xAPI Statement Structure","text":"<p>A complete xAPI statement includes:</p> <pre><code>{\n  \"actor\": {\n    \"name\": \"Dan McCreary\",\n    \"mbox\": \"mailto:dan@example.com\"\n  },\n  \"verb\": {\n    \"id\": \"http://adlnet.gov/expapi/verbs/interacted\",\n    \"display\": {\"en-US\": \"interacted\"}\n  },\n  \"object\": {\n    \"id\": \"https://example.com/microsims/gravity-simulator\",\n    \"definition\": {\n      \"name\": {\"en-US\": \"Gravity Simulator\"},\n      \"type\": \"http://adlnet.gov/expapi/activities/simulation\"\n    }\n  },\n  \"result\": {\n    \"extensions\": {\n      \"https://example.com/xapi/slider-value\": 50\n    }\n  },\n  \"timestamp\": \"2024-03-15T14:34:22.345Z\"\n}\n</code></pre>"},{"location":"chapters/11-user-testing-iteration/#why-this-matters-for-ab-testing","title":"Why This Matters for A/B Testing","text":"<p>With xAPI tracking in place, your A/B tests can measure things like:</p> <ul> <li>Average number of interactions before achieving understanding</li> <li>Sequence patterns that correlate with quiz success</li> <li>Time spent in exploration vs. targeted practice</li> <li>Whether students who adjust certain parameters score higher</li> <li>Drop-off points where learners abandon the simulation</li> </ul> <p>This is the fine-grained data needed to make evidence-based design decisions.</p>"},{"location":"chapters/11-user-testing-iteration/#the-learning-record-store-where-xapi-events-live","title":"The Learning Record Store: Where xAPI Events Live","text":"<p>xAPI events don't just float into the void\u2014they need somewhere to go. That's where the Learning Record Store (LRS) comes in.</p>"},{"location":"chapters/11-user-testing-iteration/#what-is-an-lrs","title":"What Is an LRS?","text":"<p>A Learning Record Store is a database specifically designed to receive, store, and retrieve xAPI statements. Think of it as the filing cabinet for all learning experiences across your educational ecosystem.</p> <p>LRS implementations range from:</p> <ul> <li>Browser-based (local storage): For prototyping and privacy-sensitive contexts</li> <li>Server-based (self-hosted): Full control, requires technical infrastructure</li> <li>Cloud-based (SaaS): Easy setup, ongoing subscription costs</li> <li>LMS-integrated: Built into Learning Management Systems like Moodle or Canvas</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#the-privacy-and-security-consideration","title":"The Privacy and Security Consideration","text":"<p>Because xAPI tracks individual learner behavior in fine detail, LRS security is heavily regulated. The data reveals:</p> <ul> <li>What each student struggles with</li> <li>How fast or slow they learn</li> <li>Patterns that might indicate learning disabilities</li> <li>Engagement levels and attention spans</li> </ul> <p>This sensitivity means LRS systems must comply with:</p> <ul> <li>FERPA (in the US) for student data protection</li> <li>GDPR (in Europe) for personal data rights</li> <li>Institutional IRB approval for research uses</li> </ul> <p>We won't go into all the security details here\u2014the Graph LMS intelligent textbook covers this in depth. The key takeaway: hyper-personalization requires data, and that data comes with responsibility.</p>"},{"location":"chapters/11-user-testing-iteration/#diagram-xapi-data-flow","title":"Diagram: xAPI Data Flow","text":"xAPI Data Flow Architecture <p>Type: diagram</p> <p>Bloom Taxonomy: Understand (L2)</p> <p>Learning Objective: Students will understand how xAPI events flow from MicroSim interactions to Learning Record Store storage and analytics.</p> <p>Components to show: - Left side: Student interacting with MicroSim on device (laptop, tablet, phone) - Center-left: MicroSim application with xAPI instrumentation code - Center: xAPI statements (Actor-Verb-Object boxes) - Center-right: Learning Record Store (database icon) - Right side: Analytics dashboard showing patterns</p> <p>Data flow arrows: 1. Student interactions \u2192 MicroSim (clicks, drags, inputs) 2. MicroSim \u2192 xAPI Statement Generator (event triggers) 3. xAPI Statements \u2192 LRS via HTTP POST (secure connection indicated) 4. LRS \u2192 Analytics Engine (query and aggregation) 5. Analytics \u2192 Dashboard (visualizations)</p> <p>Annotations: - At MicroSim: \"Every interactive event generates xAPI statement\" - At LRS: \"Stores millions of statements, queryable\" - At Analytics: \"Pattern recognition, A/B test results\"</p> <p>Security indicators: - Lock icon on LRS - HTTPS notation on connections - \"Anonymization possible\" note</p> <p>Color scheme: - User interaction: Green - Technical components: Blue - Data flow: Gray arrows - Security: Red accents</p> <p>Implementation: SVG or vis-network</p>"},{"location":"chapters/11-user-testing-iteration/#ieee-learning-standards-and-total-learning-architecture","title":"IEEE Learning Standards and Total Learning Architecture","text":"<p>xAPI and LRS don't exist in isolation\u2014they're part of a broader ecosystem of learning technology standards maintained by IEEE (the Institute of Electrical and Electronics Engineers) and the ADL Initiative (Advanced Distributed Learning).</p>"},{"location":"chapters/11-user-testing-iteration/#the-standards-landscape","title":"The Standards Landscape","text":"Standard Purpose Key Features xAPI Experience tracking Who-what-when statements, verb registry LRS Data storage Statement storage, query API, conformance testing cmi5 LMS integration xAPI + traditional LMS features SCORM (legacy) Course packaging Single-file courses, completion tracking LTI Tool interoperability Embed tools across LMS platforms"},{"location":"chapters/11-user-testing-iteration/#total-learning-architecture-tla","title":"Total Learning Architecture (TLA)","text":"<p>The Total Learning Architecture is the IEEE's vision for how all these standards work together. It encompasses:</p> <ul> <li>Content delivery: How learning materials reach learners</li> <li>Experience tracking: How interactions are recorded (xAPI)</li> <li>Competency management: How skills and knowledge are assessed</li> <li>Learner profiles: How individual learning histories inform personalization</li> <li>Recommendation engines: How AI suggests next learning steps</li> </ul> <p>For a deep dive into TLA implementation, see the Graph LMS book.</p>"},{"location":"chapters/11-user-testing-iteration/#the-golden-rule-of-microsim-tracking","title":"The Golden Rule of MicroSim Tracking","text":"<p>Here's the most important takeaway from all these standards:</p> <p>Every MicroSim Must Have At Least One Interactive Event</p> <p>Without interactivity, you cannot track whether a student actually used a MicroSim. It is the instructional designer's responsibility to ensure that every MicroSim has at least one trackable interaction\u2014even if it's just detecting when a student hovers their mouse over regions of an infographic.</p> <p>A MicroSim that students can passively view without any interaction is, from an analytics perspective, invisible. You'll know they loaded the page. You won't know if they learned anything.</p>"},{"location":"chapters/11-user-testing-iteration/#the-design-test-refine-cycle","title":"The Design-Test-Refine Cycle","text":"<p>Now that we have our testing methodologies and analytics infrastructure, let's put them together into a systematic improvement process.</p>"},{"location":"chapters/11-user-testing-iteration/#the-cycle-phases","title":"The Cycle Phases","text":"<p>The design-test-refine cycle is exactly what it sounds like\u2014a loop that you repeat until your MicroSim meets quality standards:</p> <ol> <li>Design: Create or modify the MicroSim based on specifications</li> <li>Test: Gather data through user testing and analytics</li> <li>Analyze: Interpret results, identify issues</li> <li>Prioritize: Decide which changes to make first</li> <li>Refine: Implement changes</li> <li>Return to Step 2: Test again to verify improvements</li> </ol>"},{"location":"chapters/11-user-testing-iteration/#when-to-stop-iterating","title":"When to Stop Iterating","text":"<p>One of the hardest questions: when is a MicroSim \"done enough\"?</p> <p>Completion criteria might include:</p> <ul> <li>Usability test success rate exceeds 85%</li> <li>Average task completion time under target threshold</li> <li>No critical bugs in last two testing rounds</li> <li>Quality score meets minimum standard (see Chapter 10)</li> <li>A/B test shows no significant difference from best alternative</li> <li>Stakeholder approval obtained</li> </ul> <p>The enemy of done is perfect. At some point, your time is better spent creating new MicroSims rather than polishing existing ones.</p>"},{"location":"chapters/11-user-testing-iteration/#diagram-design-test-refine-cycle","title":"Diagram: Design-Test-Refine Cycle","text":"Design-Test-Refine Cycle <p>Type: diagram</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Students will apply the design-test-refine cycle to systematically improve MicroSim quality through iterative development.</p> <p>Visual style: Circular flow diagram with feedback loops</p> <p>Center: \"High-Quality MicroSim\" (the goal)</p> <p>Cycle phases arranged clockwise: 1. DESIGN (Top)    - Create initial implementation    - Apply design principles    - Instrument for xAPI tracking</p> <ol> <li>TEST (Right)</li> <li>Run user testing sessions</li> <li>Collect analytics data</li> <li> <p>Gather qualitative feedback</p> </li> <li> <p>ANALYZE (Bottom-right)</p> </li> <li>Interpret test results</li> <li>Identify patterns and issues</li> <li> <p>Compare against criteria</p> </li> <li> <p>PRIORITIZE (Bottom-left)</p> </li> <li>Categorize issues (critical vs. nice-to-have)</li> <li>Estimate effort for each fix</li> <li> <p>Create ordered improvement list</p> </li> <li> <p>REFINE (Left)</p> </li> <li>Implement high-priority changes</li> <li>Update documentation</li> <li>Log changes for tracking</li> </ol> <p>Arrows connecting each phase clockwise Additional arrow from ANALYZE to center: \"If criteria met, exit cycle\" Exit indicator: \"Deploy\" arrow leaving the cycle</p> <p>Color scheme: - Design: Blue (#3B82F6) - Test: Green (#10B981) - Analyze: Yellow (#F59E0B) - Prioritize: Orange (#F97316) - Refine: Purple (#8B5CF6) - Center goal: Gold</p> <p>Implementation: SVG or p5.js animation showing cycle rotation</p>"},{"location":"chapters/11-user-testing-iteration/#change-prioritization-critical-vs-nice-to-have","title":"Change Prioritization: Critical vs. Nice-to-Have","text":"<p>Not all feedback is created equal. A crash bug and a slightly-too-small font are both \"issues,\" but they demand very different responses.</p>"},{"location":"chapters/11-user-testing-iteration/#the-priority-matrix","title":"The Priority Matrix","text":"Priority Level Criteria Examples Response Time Critical Blocks learning, crashes, produces wrong results Calculation errors, data loss, broken core functionality Immediate High Significantly degrades experience or effectiveness Confusing interface, missed learning objectives Next iteration Medium Noticeable but workaroundable Slow performance, minor visual glitches When time permits Low (Nice-to-Have) Polishing, preferences, edge cases Animation smoothness, color preferences If extra time"},{"location":"chapters/11-user-testing-iteration/#critical-changes-fix-these-first","title":"Critical Changes: Fix These First","text":"<p>Critical changes are non-negotiable. They include:</p> <ul> <li>Factual errors: Physics formulas that give wrong answers</li> <li>Accessibility blockers: Cannot be used by learners with disabilities</li> <li>Platform failures: Crashes on common devices/browsers</li> <li>Data corruption: Loses or incorrectly saves learner progress</li> <li>Security vulnerabilities: Exposes learner data</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#nice-to-have-changes-the-parking-lot","title":"Nice-to-Have Changes: The Parking Lot","text":"<p>Nice-to-have changes are the features and fixes that would be lovely but aren't essential. Keep a \"parking lot\" list of these ideas:</p> <ul> <li>Animation could be smoother</li> <li>Would be nice to have sound effects</li> <li>User suggested a dark mode</li> <li>Could add more preset scenarios</li> </ul> <p>Review the parking lot periodically. Some ideas improve with age (you understand their value better); others become obsolete (the underlying design changed).</p>"},{"location":"chapters/11-user-testing-iteration/#fundamental-redesign-vs-incremental-improvement","title":"Fundamental Redesign vs. Incremental Improvement","text":"<p>Sometimes testing reveals that small fixes won't suffice. The core design is flawed, and no amount of iteration will fix it. Recognizing this moment\u2014and having the courage to start over\u2014is a crucial skill.</p>"},{"location":"chapters/11-user-testing-iteration/#signs-you-need-fundamental-redesign","title":"Signs You Need Fundamental Redesign","text":"<ul> <li>Users consistently misunderstand the core concept</li> <li>Multiple unrelated issues all trace to the same design decision</li> <li>The simulation metaphor doesn't match the learning objective</li> <li>Technical debt makes every fix create new problems</li> <li>Stakeholders agree the direction is wrong</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#signs-incremental-improvement-will-work","title":"Signs Incremental Improvement Will Work","text":"<ul> <li>Issues are isolated and independent</li> <li>Core functionality works; problems are at the edges</li> <li>Users \"get it\" but stumble on specific interactions</li> <li>Fixes are straightforward to implement and verify</li> <li>Each iteration shows measurable improvement</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#the-sunk-cost-trap","title":"The Sunk Cost Trap","text":"<p>Beware the sunk cost fallacy: \"We've invested so much in this design, we can't throw it away.\" If the design is fundamentally broken, the investment is already lost. Continuing to iterate on a flawed foundation wastes more resources than starting fresh.</p> <p>That said, don't be too quick to scrap and restart. Distinguish between \"this approach can't work\" and \"this approach hasn't worked yet.\"</p>"},{"location":"chapters/11-user-testing-iteration/#scope-creep-prevention-staying-focused","title":"Scope Creep Prevention: Staying Focused","text":"<p>Scope creep is the gradual expansion of a project's goals beyond its original boundaries. In MicroSim development, it often sounds like:</p> <ul> <li>\"While we're at it, could we also add...\"</li> <li>\"It would be really cool if this also demonstrated...\"</li> <li>\"The students would love it if we included...\"</li> </ul> <p>Each addition seems small, but they accumulate into bloated, unfocused, never-finished projects.</p>"},{"location":"chapters/11-user-testing-iteration/#scope-creep-prevention-strategies","title":"Scope Creep Prevention Strategies","text":"<ol> <li>Document original scope: Write down exactly what the MicroSim will and won't do</li> <li>Parking lot additions: New ideas go to the list, not into the current iteration</li> <li>Change control process: Additions require explicit approval and scope adjustment</li> <li>Time boxing: Set deadlines and ship what's ready</li> <li>Feature freeze dates: After a certain point, no new features until next version</li> </ol>"},{"location":"chapters/11-user-testing-iteration/#the-mvp-mindset","title":"The MVP Mindset","text":"<p>Embrace the Minimum Viable Product (MVP) approach:</p> <ul> <li>What's the smallest version that achieves the learning objective?</li> <li>Ship that first</li> <li>Iterate based on evidence, not speculation</li> <li>Version 2.0 can add the cool features\u2014if testing shows they're needed</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#change-logs-and-design-rationale-documenting-your-decisions","title":"Change Logs and Design Rationale: Documenting Your Decisions","text":"<p>Future you (and future collaborators) will thank present you for documenting what changed and why.</p>"},{"location":"chapters/11-user-testing-iteration/#maintaining-a-change-log","title":"Maintaining a Change Log","text":"<p>A change log records every modification to a MicroSim:</p> <pre><code>## Change Log: Gravity Simulator MicroSim\n\n### Version 1.3.0 (2024-03-15)\n**Changed:**\n- Increased slider range from 0-100 to 0-200 (user feedback indicated 100 was too limiting)\n- Moved reset button to more prominent position (60% of users couldn't find it)\n\n**Fixed:**\n- Corrected calculation error in terminal velocity (was using wrong drag coefficient)\n- Fixed animation stutter on Safari browsers\n\n**Added:**\n- xAPI tracking for all slider interactions\n- Prediction prompt before running simulation\n\n### Version 1.2.0 (2024-02-28)\n...\n</code></pre>"},{"location":"chapters/11-user-testing-iteration/#documenting-design-rationale","title":"Documenting Design Rationale","text":"<p>Beyond what changed, document why:</p> <pre><code>## Design Rationale: Gravity Simulator\n\n### Why Two Balls Instead of One?\nThe misconception that heavier objects fall faster is deeply ingrained.\nA single ball simulation would allow students to \"see\" confirmation of\ntheir misconception (heavy ball \"looks\" like it should fall faster).\nSide-by-side comparison makes the equivalence undeniable.\n\n### Why No Air Resistance by Default?\nInitial testing showed students became confused by air resistance\nbefore understanding basic gravitational acceleration. Progressive\ndisclosure: unlock air resistance only after demonstrating understanding\nof the base case.\n\n### Why This Color Scheme?\nRed/blue balls tested better than other color combinations for:\n- Colorblind accessibility (distinguishable in all common types)\n- Contrast against white background\n- Association (red often = \"heavier\" in student mental models,\n  useful for confronting misconceptions)\n</code></pre>"},{"location":"chapters/11-user-testing-iteration/#ethical-research-considerations","title":"Ethical Research Considerations","text":"<p>User testing with learners, especially children, involves ethical considerations that go beyond technical quality.</p>"},{"location":"chapters/11-user-testing-iteration/#informed-consent","title":"Informed Consent","text":"<p>Before any testing:</p> <ul> <li>Explain what you're testing (the MicroSim, not the learner)</li> <li>Describe what data will be collected</li> <li>Clarify how data will be used and stored</li> <li>For minors: obtain parental/guardian consent</li> <li>Allow withdrawal at any time without penalty</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#power-dynamics","title":"Power Dynamics","text":"<p>Learners may feel pressure to please the tester, especially if:</p> <ul> <li>The tester is their teacher</li> <li>They think their grade depends on it</li> <li>They don't want to seem \"stupid\"</li> </ul> <p>Mitigate this by:</p> <ul> <li>Using neutral observers rather than instructors</li> <li>Emphasizing that you're testing the simulation, not them</li> <li>Celebrating when they find problems (\"That's exactly what we needed to know!\")</li> <li>Offering genuine appreciation for their time</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#data-protection","title":"Data Protection","text":"<p>Learning analytics data requires careful handling:</p> <ul> <li>Anonymize data when possible</li> <li>Store securely with appropriate access controls</li> <li>Comply with applicable regulations (FERPA, GDPR, etc.)</li> <li>Delete data when no longer needed</li> <li>Never share individual learner data without consent</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#test-interpretation-making-sense-of-the-data","title":"Test Interpretation: Making Sense of the Data","text":"<p>Collecting data is easy; interpreting it correctly is the art. Here are common pitfalls and how to avoid them.</p>"},{"location":"chapters/11-user-testing-iteration/#confirmation-bias","title":"Confirmation Bias","text":"<p>You built this MicroSim. You want it to work. This bias makes it easy to explain away negative feedback and amplify positive signals.</p> <p>Antidote: Have someone else review the test results before you. What do they see?</p>"},{"location":"chapters/11-user-testing-iteration/#overgeneralizing-from-small-samples","title":"Overgeneralizing from Small Samples","text":"<p>Three users is not a statistically significant sample, no matter how clearly they articulate their feedback.</p> <p>Antidote: Distinguish between \"strong signal from few users\" (investigate further) and \"statistically significant finding\" (act with confidence).</p>"},{"location":"chapters/11-user-testing-iteration/#confusing-correlation-with-causation","title":"Confusing Correlation with Causation","text":"<p>\"Students who used the slider more scored higher on the quiz\" doesn't mean using the slider caused higher scores. Maybe curious students both use sliders more and score higher.</p> <p>Antidote: When possible, use controlled experiments (A/B tests) rather than observational data.</p>"},{"location":"chapters/11-user-testing-iteration/#the-expert-blind-spot","title":"The Expert Blind Spot","text":"<p>You understand this concept deeply. You can't easily see what's confusing to someone who doesn't. This is the \"curse of knowledge.\"</p> <p>Antidote: Watch novice users. Really watch them. Their struggles reveal what you can no longer see.</p>"},{"location":"chapters/11-user-testing-iteration/#diagram-common-interpretation-pitfalls","title":"Diagram: Common Interpretation Pitfalls","text":"Test Interpretation Pitfalls <p>Type: infographic</p> <p>Bloom Taxonomy: Evaluate (L5)</p> <p>Learning Objective: Students will evaluate test results by identifying and avoiding common interpretation pitfalls.</p> <p>Layout: Four quadrant grid, each with a pitfall and its antidote</p> <p>Quadrant 1 (Top-left): \"Confirmation Bias\" - Icon: Rose-colored glasses - Description: \"Seeing what you want to see\" - Red flag: \"Only positive feedback feels 'real'\" - Antidote: \"Get outside perspectives before reviewing data\"</p> <p>Quadrant 2 (Top-right): \"Small Sample Overgeneralization\" - Icon: Three stick figures claiming to represent thousands - Description: \"N=3 is not statistical significance\" - Red flag: \"Three users said this, so it must be true\" - Antidote: \"Note patterns, but verify with larger samples\"</p> <p>Quadrant 3 (Bottom-left): \"Correlation \u2260 Causation\" - Icon: Ice cream and drowning correlation graph - Description: \"Just because A and B happen together...\" - Red flag: \"Users who did X also did Y, so X causes Y\" - Antidote: \"Design controlled experiments when possible\"</p> <p>Quadrant 4 (Bottom-right): \"Expert Blind Spot\" - Icon: Flashlight illuminating only part of scene - Description: \"Can't see what novices struggle with\" - Red flag: \"I don't understand why this is confusing\" - Antidote: \"Watch novices carefully; their struggles are data\"</p> <p>Center text: \"Good interpretation requires humility\"</p> <p>Color scheme: Each quadrant a different pastel color, antidotes in green boxes</p> <p>Implementation: HTML/CSS grid or SVG infographic</p>"},{"location":"chapters/11-user-testing-iteration/#building-a-microsim-testing-culture","title":"Building a MicroSim Testing Culture","text":"<p>Testing isn't a phase\u2014it's a mindset. The best MicroSim creators integrate testing throughout their process.</p>"},{"location":"chapters/11-user-testing-iteration/#test-early-and-often","title":"Test Early and Often","text":"<p>Don't wait until the MicroSim is \"done\" to test. Test:</p> <ul> <li>Paper prototypes (before any code is written)</li> <li>Core interaction (before adding polish)</li> <li>Near-complete versions (before deployment)</li> <li>Deployed versions (continuous improvement)</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#build-testing-infrastructure","title":"Build Testing Infrastructure","text":"<p>Invest in tools and processes that make testing easy:</p> <ul> <li>xAPI instrumentation templates</li> <li>Standard user testing scripts</li> <li>Feedback form templates</li> <li>Analytics dashboards</li> <li>Change log templates</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#celebrate-learning-from-failure","title":"Celebrate Learning from Failure","text":"<p>When testing reveals problems, that's success. The test worked. You learned something. You can now make something better.</p> <p>The only failure is shipping without testing and never learning what could have been improved.</p>"},{"location":"chapters/11-user-testing-iteration/#summary-the-iterative-mindset","title":"Summary: The Iterative Mindset","text":"<p>User testing and iteration transform MicroSims from theoretical designs into proven learning tools. The key insights from this chapter:</p> <p>Testing Methods: - Think-aloud protocols reveal what learners are thinking - Observation techniques capture what they're doing - Age-based and ability-based feedback strategies ensure inclusive data collection - A/B testing provides statistical confidence for design decisions</p> <p>Analytics Infrastructure: - Google Analytics tracks page views but not simulation interactions - xAPI tracks fine-grained who-what-when data - Learning Record Stores provide secure storage for xAPI events - IEEE standards (TLA) integrate these technologies into coherent systems - Every MicroSim must have at least one interactive event for tracking</p> <p>Iteration Framework: - The design-test-refine cycle systematically improves quality - Critical changes must be fixed immediately; nice-to-haves can wait - Recognize when fundamental redesign beats incremental improvement - Prevent scope creep with documented boundaries and parking lots - Maintain change logs and design rationale for future collaborators</p> <p>Ethical Considerations: - Obtain informed consent, especially for minors - Protect learner data according to applicable regulations - Interpret results humbly, watching for common pitfalls</p> <p>The ultimate goal: MicroSims that genuinely help people learn. That's worth the effort of rigorous testing and continuous improvement.</p> <p>Now go test something. Your learners are waiting.</p>"},{"location":"chapters/11-user-testing-iteration/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Think-aloud protocols reveal learner thought processes that analytics cannot capture</li> <li>Observation techniques complement verbal feedback with behavioral data</li> <li>A/B testing requires fine-grained simulation tracking, not just page views</li> <li>xAPI provides who-what-when tracking: Actor \u2192 Verb \u2192 Object</li> <li>Learning Record Stores securely store xAPI events for analysis</li> <li>IEEE Learning Standards including the Total Learning Architecture integrate learning technologies</li> <li>Every MicroSim must have at least one interactive event to enable tracking</li> <li>The design-test-refine cycle systematically improves quality through iteration</li> <li>Change prioritization distinguishes critical issues from nice-to-haves</li> <li>Scope creep prevention keeps projects focused and deliverable</li> <li>Change logs and design rationale document decisions for future reference</li> <li>Ethical research protects learner rights and data</li> </ul>"},{"location":"chapters/11-user-testing-iteration/#reflection-questions","title":"Reflection Questions","text":"How would you design an xAPI tracking strategy for a specific MicroSim? <p>Consider what interactions are meaningful: - Which control changes indicate engagement vs. random clicking? - What sequence of actions suggests understanding? - How would you structure Actor-Verb-Object statements for key moments? - What aggregations would reveal A/B test results?</p> When should you choose fundamental redesign over incremental improvement? <p>Signs pointing toward redesign: - Multiple unrelated issues trace to the same design decision - Users consistently misunderstand the core concept - Each fix creates new problems - Testing shows no improvement over iterations</p> <p>Signs supporting incremental improvement: - Issues are isolated and independent - Core functionality works - Each iteration shows measurable progress</p> What ethical considerations apply when testing MicroSims with children? <p>Key considerations include: - Parental consent requirements - Age-appropriate testing methods - Power dynamics with adult testers - Data protection and privacy regulations - Making the experience positive regardless of outcomes</p>"},{"location":"chapters/11-user-testing-iteration/#references","title":"References","text":"<ol> <li> <p>Nielsen, J. (1994). Usability Engineering. Morgan Kaufmann. \u2014 Classic text on think-aloud testing and usability methods.</p> </li> <li> <p>Advanced Distributed Learning (ADL) Initiative. (2023). xAPI Specification. https://github.com/adlnet/xAPI-Spec \u2014 Official xAPI technical specification.</p> </li> <li> <p>IEEE Learning Technology Standards Committee. (2022). Total Learning Architecture Reference Implementation. https://adlnet.gov/tla \u2014 Overview of integrated learning standards.</p> </li> <li> <p>McCreary, D. (2024). Graph LMS: Intelligent Learning Management Systems. https://dmccreary.github.io/graph-lms/ \u2014 Deep dive into LRS implementation and learning analytics.</p> </li> <li> <p>Ericsson, K. A., &amp; Simon, H. A. (1993). Protocol Analysis: Verbal Reports as Data. MIT Press. \u2014 Foundational text on think-aloud methodology.</p> </li> <li> <p>U.S. Department of Education. (2021). Family Educational Rights and Privacy Act (FERPA). https://www.ed.gov/ferpa \u2014 Student data protection requirements.</p> </li> </ol>"},{"location":"chapters/11-user-testing-iteration/quiz/","title":"Quiz: User Testing and Iteration","text":"<p>Test your understanding of user testing methodologies, xAPI analytics, Learning Record Stores, and the iterative refinement process for MicroSims.</p>"},{"location":"chapters/11-user-testing-iteration/quiz/#1-what-is-the-think-aloud-protocol","title":"1. What is the think-aloud protocol?","text":"<ol> <li>A method where testers explain the code structure verbally</li> <li>A user testing method where learners verbalize their thoughts while using a MicroSim</li> <li>A technique for AI systems to explain their reasoning</li> <li>A protocol for recording audio narration in MicroSims</li> </ol> Show Answer <p>The correct answer is B. The think-aloud protocol is a user testing method where learners verbalize their thoughts while using your MicroSim. The basic premise: ask learners to say out loud everything they're thinking as they work through the simulation. This reveals insights that no amount of analytics can capture, including confusion, understanding, frustration, and \"aha\" moments.</p> <p>Concept Tested: Think-Aloud Protocol</p> <p>See: Chapter Content</p>"},{"location":"chapters/11-user-testing-iteration/quiz/#2-what-is-the-fundamental-difference-between-page-view-tracking-and-simulation-tracking","title":"2. What is the fundamental difference between page-view tracking and simulation tracking?","text":"<ol> <li>Page-view tracking is more accurate than simulation tracking</li> <li>Page-view tracking knows you were in the room; simulation tracking knows what you did while there</li> <li>Simulation tracking only works with p5.js MicroSims</li> <li>Page-view tracking requires user consent while simulation tracking does not</li> </ol> Show Answer <p>The correct answer is B. Page-view tracking (like Google Analytics) tells you a student visited the page and how long they stayed, but cannot tell you if they actually interacted with the simulation, which sliders they adjusted, or whether they tested the intended hypothesis. Simulation tracking knows what the user did while there\u2014which controls were used, what values were tested, and the sequence of interactions.</p> <p>Concept Tested: Simulation Tracking, Google Analytics</p> <p>See: Chapter Content</p>"},{"location":"chapters/11-user-testing-iteration/quiz/#3-what-is-the-core-structure-of-an-xapi-statement","title":"3. What is the core structure of an xAPI statement?","text":"<ol> <li>Input \u2192 Process \u2192 Output</li> <li>Actor (Who) \u2192 Verb (What) \u2192 Object (When + Context)</li> <li>Subject \u2192 Predicate \u2192 Complement</li> <li>Event \u2192 Handler \u2192 Response</li> </ol> Show Answer <p>The correct answer is B. At its core, xAPI is beautifully simple. Every learning experience is recorded as a statement with three parts: Actor (Who) \u2192 Verb (What) \u2192 Object (When + Context). Examples include \"Dan completed the Gravity Simulator at 2:34 PM\" or \"Sarah adjusted the mass slider to 50kg.\" These statements are machine-readable, standardized, and can be aggregated to reveal learning patterns.</p> <p>Concept Tested: xAPI, Tracking Who-What-When</p> <p>See: Chapter Content</p>"},{"location":"chapters/11-user-testing-iteration/quiz/#4-what-is-a-learning-record-store-lrs","title":"4. What is a Learning Record Store (LRS)?","text":"<ol> <li>A physical storage facility for educational materials</li> <li>A database specifically designed to receive, store, and retrieve xAPI statements</li> <li>A file system for organizing MicroSim source code</li> <li>A retail store that sells educational technology</li> </ol> Show Answer <p>The correct answer is B. A Learning Record Store (LRS) is a database specifically designed to receive, store, and retrieve xAPI statements. Think of it as the filing cabinet for all learning experiences across your educational ecosystem. LRS implementations range from browser-based (local storage), server-based (self-hosted), cloud-based (SaaS), to LMS-integrated (built into Moodle, Canvas, etc.).</p> <p>Concept Tested: Learning Record Store, xAPI events to a LRS</p> <p>See: Chapter Content</p>"},{"location":"chapters/11-user-testing-iteration/quiz/#5-what-is-the-golden-rule-of-microsim-tracking-stated-in-the-chapter","title":"5. What is the golden rule of MicroSim tracking stated in the chapter?","text":"<ol> <li>Track as much data as possible about every user</li> <li>Every MicroSim must have at least one interactive event to enable tracking</li> <li>Tracking should only be enabled for research purposes</li> <li>User consent is optional for educational tracking</li> </ol> Show Answer <p>The correct answer is B. The chapter states: \"Every MicroSim Must Have At Least One Interactive Event.\" Without interactivity, you cannot track whether a student actually used a MicroSim. A MicroSim that students can passively view without any interaction is, from an analytics perspective, invisible. You'll know they loaded the page, but you won't know if they learned anything.</p> <p>Concept Tested: Interaction Tracking, Simulation Tracking</p> <p>See: Chapter Content</p>"},{"location":"chapters/11-user-testing-iteration/quiz/#6-what-are-the-phases-of-the-design-test-refine-cycle","title":"6. What are the phases of the design-test-refine cycle?","text":"<ol> <li>Plan, Build, Ship, Support</li> <li>Design, Test, Analyze, Prioritize, Refine, then return to Test</li> <li>Requirements, Development, Testing, Deployment</li> <li>Create, Review, Approve, Publish</li> </ol> Show Answer <p>The correct answer is B. The design-test-refine cycle has these phases: Design (create or modify the MicroSim), Test (gather data through user testing and analytics), Analyze (interpret results, identify issues), Prioritize (decide which changes to make first), Refine (implement changes), then return to Test to verify improvements. This loop repeats until the MicroSim meets quality standards.</p> <p>Concept Tested: Design-Test-Refine Cycle, Iterative Refinement</p> <p>See: Chapter Content</p>"},{"location":"chapters/11-user-testing-iteration/quiz/#7-how-should-changes-be-prioritized-after-user-testing","title":"7. How should changes be prioritized after user testing?","text":"<ol> <li>Alphabetically by feature name</li> <li>By severity: Critical (blocks learning), High (degrades experience), Medium (workaroundable), Low (nice-to-have)</li> <li>By how easy they are to implement</li> <li>By who requested them (stakeholders first)</li> </ol> Show Answer <p>The correct answer is B. Changes should be triaged by severity: Critical (blocks learning, crashes, produces wrong results\u2014fix immediately), High (significantly degrades experience\u2014next iteration), Medium (noticeable but workaroundable\u2014when time permits), and Low/Nice-to-Have (polishing, preferences\u2014if extra time). Critical changes include factual errors, accessibility blockers, platform failures, data corruption, and security vulnerabilities.</p> <p>Concept Tested: Change Prioritization, Critical Changes, Nice-to-Have Changes</p> <p>See: Chapter Content</p>"},{"location":"chapters/11-user-testing-iteration/quiz/#8-what-is-scope-creep-and-how-should-it-be-prevented","title":"8. What is scope creep and how should it be prevented?","text":"<ol> <li>Software bugs that spread to other files; prevent with automated testing</li> <li>Gradual expansion of project goals beyond original boundaries; prevent with documented scope and parking lot lists</li> <li>When MicroSims take too long to load; prevent with file compression</li> <li>When users access features they shouldn't; prevent with authentication</li> </ol> Show Answer <p>The correct answer is B. Scope creep is the gradual expansion of a project's goals beyond its original boundaries, often through requests like \"While we're at it, could we also add...\" Each addition seems small but they accumulate into bloated, unfocused projects. Prevention strategies include documenting original scope, using a parking lot for new ideas, having a change control process, time boxing with deadlines, and feature freeze dates.</p> <p>Concept Tested: Scope Creep Prevention</p> <p>See: Chapter Content</p>"},{"location":"chapters/11-user-testing-iteration/quiz/#9-what-distinguishes-situations-requiring-fundamental-redesign-from-those-suitable-for-incremental-improvement","title":"9. What distinguishes situations requiring fundamental redesign from those suitable for incremental improvement?","text":"<ol> <li>Fundamental redesign is needed when budget is unlimited; incremental when budget is tight</li> <li>Fundamental redesign is needed when users consistently misunderstand the core concept; incremental when issues are isolated and core functionality works</li> <li>Fundamental redesign is needed for small projects; incremental for large projects</li> <li>There is no difference\u2014always choose incremental improvement</li> </ol> Show Answer <p>The correct answer is B. Signs you need fundamental redesign include: users consistently misunderstand the core concept, multiple unrelated issues trace to the same design decision, the simulation metaphor doesn't match the learning objective, and technical debt makes every fix create new problems. Signs incremental improvement will work include: issues are isolated and independent, core functionality works, users \"get it\" but stumble on specific interactions, and each iteration shows measurable improvement.</p> <p>Concept Tested: Fundamental Redesign, Incremental Improvement</p> <p>See: Chapter Content</p>"},{"location":"chapters/11-user-testing-iteration/quiz/#10-why-is-documenting-design-rationale-important-and-what-should-it-include","title":"10. Why is documenting design rationale important, and what should it include?","text":"<ol> <li>It's only needed for legal compliance; include copyright information</li> <li>It explains why design decisions were made so future collaborators understand the reasoning; include the decision, alternatives considered, and justification</li> <li>It's optional documentation for marketing purposes; include user testimonials</li> <li>It's only needed for commercial products; include pricing information</li> </ol> Show Answer <p>The correct answer is B. Design rationale documentation explains why decisions were made, not just what changed. This helps future collaborators (including your future self) understand the reasoning. Examples include explaining \"Why two balls instead of one?\" (to make side-by-side comparison undeniable for misconception confrontation) or \"Why no air resistance by default?\" (students became confused before understanding basic gravitational acceleration). Include the decision, alternatives considered, and justification.</p> <p>Concept Tested: Design Rationale, Change Log</p> <p>See: Chapter Content</p>"},{"location":"chapters/12-accessibility-deployment-completion/","title":"Accessibility, Deployment, and Course Completion","text":""},{"location":"chapters/12-accessibility-deployment-completion/#summary","title":"Summary","text":"<p>This final chapter covers three essential areas for completing your MicroSim development journey. First, you will learn accessibility principles including universal design, UDL principles, screen reader support, keyboard navigation, color blindness design, reduced motion preferences, multilingual support, vocabulary considerations, cultural sensitivity, prior knowledge support, differentiation strategies, accessibility auditing, and constraint simulation. Second, the deployment section covers LMS integration, intelligent textbooks, learning analytics, interaction tracking, maintenance planning, library organization, educator collaboration, and content sharing. Finally, you will complete your portfolio project, maintain a reflection journal, engage in peer feedback, and conduct self-evaluation to demonstrate mastery.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 27 concepts from the learning graph:</p> <ol> <li>Color Accessibility</li> <li>Contrast Design</li> <li>Universal Design</li> <li>UDL Principles</li> <li>Screen Reader Support</li> <li>Keyboard Navigation</li> <li>Color Blindness Design</li> <li>Reduced Motion</li> <li>Multilingual Support</li> <li>Vocabulary Level</li> <li>Cultural Sensitivity</li> <li>Prior Knowledge Support</li> <li>Differentiation Strategy</li> <li>Accessibility Audit</li> <li>Constraint Simulation</li> <li>LMS Integration</li> <li>Intelligent Textbook</li> <li>Learning Analytics</li> <li>Interaction Tracking</li> <li>Maintenance Planning</li> <li>Library Organization</li> <li>Educator Collaboration</li> <li>Content Sharing</li> <li>Portfolio Project</li> <li>Reflection Journal</li> <li>Peer Feedback</li> <li>Self-Evaluation</li> </ol>"},{"location":"chapters/12-accessibility-deployment-completion/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 7: Cognitive Load and Visual Design</li> <li>Chapter 10: Quality Evaluation Frameworks</li> <li>Chapter 11: User Testing and Iteration</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#introduction-the-final-mile-and-what-a-journey-its-been","title":"Introduction: The Final Mile (And What a Journey It's Been!)","text":"<p>Congratulations! You've made it to the final chapter. If this were a marathon, you'd be rounding the corner into the stadium with the crowd on their feet. If it were a cooking show, you'd be plating the dish with thirty seconds on the clock. If it were a space mission... well, you get the idea.</p> <p>You've learned to analyze learning objectives, select visualization paradigms, write specifications, generate MicroSims with AI, evaluate quality, conduct user testing, and iterate toward excellence. Now comes the part that determines whether all that work actually reaches learners: making your MicroSims accessible to everyone and deploying them where educators can find and use them.</p> <p>Here's a truth that should guide everything in this chapter: the most brilliantly designed MicroSim is worthless if some learners can't use it. A color-blind student shouldn't miss critical information. A student using a screen reader shouldn't encounter an impenetrable wall of silence. A learner with motor impairments shouldn't be locked out because your simulation requires pixel-perfect mouse movements.</p> <p>Accessibility isn't a checkbox at the end\u2014it's a mindset that should inform every design decision from the beginning. The good news? Building accessible MicroSims often makes them better for everyone.</p> <p>Let's make sure your MicroSims reach every learner who needs them.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#universal-design-building-for-everyone-from-the-start","title":"Universal Design: Building for Everyone from the Start","text":"<p>Universal Design is the philosophy that products should be usable by the widest possible range of people without requiring adaptation or specialized design. It originated in architecture (think: automatic doors that help wheelchair users, parents with strollers, and tired shoppers alike) but applies beautifully to educational technology.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#the-seven-principles-of-universal-design","title":"The Seven Principles of Universal Design","text":"Principle Description MicroSim Application Equitable Use Useful for people with diverse abilities Works with screen readers, keyboard, touch Flexibility in Use Accommodates wide range of preferences Multiple input methods, adjustable speed Simple and Intuitive Easy to understand regardless of experience Clear labels, obvious controls Perceptible Information Communicates effectively across sensory abilities Visual + audio + text feedback Tolerance for Error Minimizes hazards and adverse consequences Undo, reset, forgiving controls Low Physical Effort Efficient and comfortable Keyboard shortcuts, minimal precision required Size and Space Appropriate for body sizes and mobility Large touch targets, zoom support"},{"location":"chapters/12-accessibility-deployment-completion/#universal-design-vs-accessibility-retrofitting","title":"Universal Design vs. Accessibility Retrofitting","text":"<p>There's a crucial difference between designing for accessibility from the start and bolting it on at the end:</p> <ul> <li>Universal Design: \"How can I make this work for everyone?\" (Day 1 question)</li> <li>Accessibility Retrofitting: \"How can I make this work for people I forgot?\" (Deadline question)</li> </ul> <p>Retrofitting is expensive, often incomplete, and sometimes impossible. A MicroSim designed entirely around precise mouse dragging cannot be easily adapted for keyboard users\u2014you'd need to fundamentally rethink the interaction model.</p> <p>Start with Constraints</p> <p>Design your MicroSim as if you couldn't use a mouse. What interactions would still work? That's your accessibility foundation.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#udl-principles-the-educational-lens-on-accessibility","title":"UDL Principles: The Educational Lens on Accessibility","text":"<p>While Universal Design comes from architecture and product design, Universal Design for Learning (UDL) specifically addresses how people learn. UDL recognizes that learners vary in:</p> <ul> <li>How they perceive and comprehend information (Recognition networks)</li> <li>How they navigate the learning environment and express what they know (Strategic networks)</li> <li>How they get engaged and stay motivated (Affective networks)</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#the-three-udl-principles","title":"The Three UDL Principles","text":"<p>1. Multiple Means of Representation</p> <p>Present information in multiple formats so learners can choose what works for them:</p> <ul> <li>Text + audio narration</li> <li>Static diagram + animated demonstration</li> <li>Concrete examples + abstract principles</li> <li>Multiple languages</li> </ul> <p>2. Multiple Means of Action and Expression</p> <p>Let learners interact and demonstrate understanding in various ways:</p> <ul> <li>Mouse + keyboard + touch</li> <li>Typed responses + spoken responses</li> <li>Predefined options + open-ended exploration</li> </ul> <p>3. Multiple Means of Engagement</p> <p>Offer different ways to motivate and sustain interest:</p> <ul> <li>Challenge levels (easy \u2192 hard)</li> <li>Choice in what to explore first</li> <li>Progress indicators</li> <li>Connections to learner interests</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#udl-in-practice-the-gravity-simulator-example","title":"UDL in Practice: The Gravity Simulator Example","text":"<p>Consider how a gravity simulator could embody UDL principles:</p> UDL Principle Implementation Multiple Representation Visual animation + numerical readouts + verbal description via describe() Multiple Action/Expression Sliders (mouse) + number inputs (keyboard) + voice commands (future) Multiple Engagement Preset scenarios for guided learners + open exploration for curious ones"},{"location":"chapters/12-accessibility-deployment-completion/#screen-reader-support-when-vision-isnt-an-option","title":"Screen Reader Support: When Vision Isn't an Option","text":"<p>Screen readers are software that converts on-screen content to speech or Braille. For learners who are blind or have low vision, screen readers are their window into your MicroSim. Making that window clear and informative is your responsibility.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#the-p5js-describe-function-non-negotiable-for-quality","title":"The p5.js describe() Function: Non-Negotiable for Quality","text":"<p>If your MicroSim uses p5.js, the <code>describe()</code> function is your primary tool for screen reader accessibility. It creates a description of the canvas content that screen readers can announce.</p> <pre><code>function draw() {\n  background(220);\n\n  // Your drawing code here\n  ellipse(ball.x, ball.y, 50, 50);\n\n  // REQUIRED: Describe what's happening\n  describe(`A ${ball.color} ball at position ${Math.round(ball.x)},\n            ${Math.round(ball.y)} falling under gravity.\n            Current velocity: ${Math.round(ball.vy)} pixels per frame.`);\n}\n</code></pre> <p>describe() is Required for Quality Scores</p> <p>MicroSims without proper <code>describe()</code> implementation cannot achieve high quality scores. This isn't bureaucratic box-checking\u2014it's the difference between \"usable by everyone\" and \"excludes blind learners entirely.\"</p>"},{"location":"chapters/12-accessibility-deployment-completion/#describe-best-practices","title":"describe() Best Practices","text":"<ol> <li>Update dynamically: Change the description when the simulation state changes</li> <li>Include key data: What values are learners supposed to observe?</li> <li>Be concise but complete: Screen reader users don't want to hear a novel every frame</li> <li>Use the second parameter: <code>describe(text, LABEL)</code> for always-visible descriptions vs. <code>describe(text, FALLBACK)</code> for screen-reader-only</li> </ol> <pre><code>// Good: Dynamic, informative description\ndescribe(`Simulation running. Two balls falling:\n         red ball (5kg) at height ${redHeight}m,\n         blue ball (50kg) at height ${blueHeight}m.\n         Both falling at the same rate, demonstrating\n         that mass doesn't affect gravitational acceleration.`);\n\n// Bad: Static, uninformative description\ndescribe(\"A physics simulation\"); // Tells user nothing about current state\n</code></pre>"},{"location":"chapters/12-accessibility-deployment-completion/#beyond-describe-full-screen-reader-strategy","title":"Beyond describe(): Full Screen Reader Strategy","text":"<p>The <code>describe()</code> function handles the canvas, but a complete MicroSim includes HTML controls that also need accessibility:</p> <ul> <li>Labels: Every control needs a visible or screen-reader-accessible label</li> <li>ARIA attributes: Use <code>aria-label</code>, <code>aria-describedby</code> for complex controls</li> <li>Live regions: Announce changes with <code>aria-live=\"polite\"</code> for important updates</li> <li>Focus management: Ensure screen reader users can navigate logically</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#keyboard-navigation-freedom-from-mouse-dependency","title":"Keyboard Navigation: Freedom from Mouse Dependency","text":"<p>Here's a hard truth about many MicroSims: if you can only control them with a mouse, you've excluded learners who can't use a mouse.</p> <p>This includes:</p> <ul> <li>Users with motor impairments affecting fine motor control</li> <li>Users with tremors or conditions affecting precision</li> <li>Blind users who can't see where to click</li> <li>Power users who prefer keyboard efficiency</li> <li>Anyone with a broken mouse (it happens!)</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#the-microsim-control-architecture-solution","title":"The MicroSim Control Architecture Solution","text":"<p>Our recommended MicroSim architecture places controls below the drawing area rather than overlapping it. This isn't just an aesthetic choice\u2014it's an accessibility decision:</p> Architecture Mouse Users Keyboard Users Screen Reader Users Controls overlapping canvas \u2713 Can click anywhere \u2717 Can't tab to canvas regions \u2717 Canvas is one opaque block Controls below canvas \u2713 Clear control area \u2713 Tab through controls \u2713 Controls are focusable Hybrid (canvas + separate controls) \u2713 Both options \u2713 Controls accessible \u2713 Partial access"},{"location":"chapters/12-accessibility-deployment-completion/#standard-control-elements-for-accessibility","title":"Standard Control Elements for Accessibility","text":"<p>Replace mouse-dependent interactions with accessible HTML controls:</p> <pre><code>&lt;!-- Instead of: \"drag to adjust\" --&gt;\n&lt;label for=\"mass-slider\"&gt;Mass (kg):&lt;/label&gt;\n&lt;input type=\"range\" id=\"mass-slider\" min=\"1\" max=\"100\" value=\"50\"\n       aria-describedby=\"mass-help\"&gt;\n&lt;span id=\"mass-help\"&gt;Use arrow keys to adjust mass&lt;/span&gt;\n\n&lt;!-- Instead of: \"click to start\" --&gt;\n&lt;button id=\"start-btn\" onclick=\"startSimulation()\"&gt;\n  Start Simulation\n&lt;/button&gt;\n</code></pre>"},{"location":"chapters/12-accessibility-deployment-completion/#keyboard-shortcuts-for-power-users","title":"Keyboard Shortcuts for Power Users","text":"<p>For MicroSims with many controls, consider adding keyboard shortcuts:</p> <pre><code>function keyPressed() {\n  if (key === 'r' || key === 'R') {\n    resetSimulation();\n    describe('Simulation reset to initial state');\n  }\n  if (key === ' ') {  // Spacebar\n    togglePause();\n    describe(isPaused ? 'Simulation paused' : 'Simulation running');\n  }\n  if (keyCode === UP_ARROW) {\n    increaseSpeed();\n    describe(`Animation speed increased to ${speed}x`);\n  }\n}\n</code></pre>"},{"location":"chapters/12-accessibility-deployment-completion/#diagram-accessible-microsim-control-layout","title":"Diagram: Accessible MicroSim Control Layout","text":"Accessible MicroSim Control Layout <p>Type: diagram</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Students will apply accessible layout principles to design MicroSim interfaces that work for mouse, keyboard, and screen reader users.</p> <p>Layout: Vertical stack showing recommended MicroSim structure</p> <p>Top section - Drawing Canvas: - Full-width responsive canvas area - Contains visual simulation elements - Has describe() output for screen readers - Focus indicator when canvas has focus - Label: \"Canvas Area (visual simulation)\"</p> <p>Middle section - Status/Output: - Current simulation values in text form - aria-live region for dynamic updates - Label: \"Status Area (announces changes)\"</p> <p>Bottom section - Control Panel: - Row of sliders with labels - Row of buttons (Start, Pause, Reset) - Each control has visible label and focus indicator - Tab order flows left-to-right, top-to-bottom - Label: \"Control Panel (keyboard accessible)\"</p> <p>Annotations: - Arrow showing Tab key navigation order - Note: \"Controls placed BELOW canvas, not overlapping\" - Note: \"All controls reachable via keyboard Tab\" - Note: \"describe() provides canvas content to screen readers\"</p> <p>Color scheme: - Canvas: Light gray background - Controls: Distinct from canvas - Focus indicators: Bold blue outline - Annotations: Green</p> <p>Implementation: HTML/CSS diagram or static image</p>"},{"location":"chapters/12-accessibility-deployment-completion/#color-accessibility-and-contrast-design","title":"Color Accessibility and Contrast Design","text":"<p>Color is a powerful design tool, but approximately 8% of men and 0.5% of women have some form of color vision deficiency. Designing with color accessibility in mind ensures your MicroSim works for everyone.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#color-blindness-types","title":"Color Blindness Types","text":"Type Affects Confusing Colors Prevalence Deuteranomaly Green sensitivity Red/green 6% of males Protanomaly Red sensitivity Red/green 2% of males Tritanomaly Blue sensitivity Blue/yellow 0.01% Monochromacy All color Everything Very rare"},{"location":"chapters/12-accessibility-deployment-completion/#design-strategies-for-color-blindness","title":"Design Strategies for Color Blindness","text":"<p>Never rely on color alone to convey information:</p> <pre><code>// Bad: Color is the only differentiator\nfill(isCorrect ? 'green' : 'red');\nellipse(x, y, 50, 50);\n\n// Good: Color + shape + label\nif (isCorrect) {\n  fill('green');\n  ellipse(x, y, 50, 50);\n  text('\u2713', x, y);  // Checkmark reinforces meaning\n} else {\n  fill('red');\n  rect(x - 25, y - 25, 50, 50);  // Different shape\n  text('\u2717', x, y);  // X mark reinforces meaning\n}\n</code></pre>"},{"location":"chapters/12-accessibility-deployment-completion/#contrast-requirements","title":"Contrast Requirements","text":"<p>WCAG (Web Content Accessibility Guidelines) specifies minimum contrast ratios:</p> <ul> <li>4.5:1 for normal text</li> <li>3:1 for large text (18pt+) and graphics</li> </ul> <p>Use tools like WebAIM's Contrast Checker to verify your color choices.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#colorblind-safe-palettes","title":"Colorblind-Safe Palettes","text":"<p>When you need distinct colors that work for colorblind users, consider:</p> <ul> <li>Blue and Orange: Distinguishable by almost everyone</li> <li>Blue and Red: Works for most color blindness types</li> <li>Adding patterns: Stripes, dots, or textures supplement color</li> <li>Using saturation differences: Light vs. dark versions of same hue</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#diagram-color-blindness-simulation","title":"Diagram: Color Blindness Simulation","text":"Color Blindness Simulation Tool <p>Type: microsim</p> <p>Bloom Taxonomy: Apply (L3)</p> <p>Learning Objective: Students will apply color accessibility principles by testing MicroSim color schemes under simulated color vision deficiencies.</p> <p>Canvas layout: - Left half: Original color palette display - Right half: Simulated color vision view - Bottom: Controls</p> <p>Visual elements: - Grid of colored squares showing a sample MicroSim palette - Common combinations: red/green, blue/yellow, traffic light colors - Before/after comparison - Labels for each color</p> <p>Interactive controls: - Dropdown: Select color blindness type (Normal, Deuteranopia, Protanopia, Tritanopia) - Upload area: Upload image or enter colors to test - Preset buttons: \"Traffic Light\", \"Heat Map\", \"Safe Palette\"</p> <p>Default parameters: - Normal vision selected - Sample palette displayed</p> <p>Behavior: - Selecting color blindness type updates right panel with simulated view - Problematic colors highlighted with warning icon - Suggestions for accessible alternatives displayed</p> <p>Accessibility: - describe() announces: \"Color simulation showing [type]. Left panel shows original colors. Right panel shows how colors appear with [type] color blindness.\" - All controls keyboard accessible</p> <p>Implementation: p5.js with color transformation matrices</p>"},{"location":"chapters/12-accessibility-deployment-completion/#reduced-motion-respecting-vestibular-sensitivities","title":"Reduced Motion: Respecting Vestibular Sensitivities","text":"<p>Animations can be delightful\u2014or they can trigger vestibular disorders, causing dizziness, nausea, or migraines. Users can set a system preference for reduced motion, and your MicroSim should respect it.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#detecting-reduced-motion-preference","title":"Detecting Reduced Motion Preference","text":"<pre><code>/* CSS approach */\n@media (prefers-reduced-motion: reduce) {\n  * {\n    animation: none !important;\n    transition: none !important;\n  }\n}\n</code></pre> <pre><code>// JavaScript approach\nconst prefersReducedMotion = window.matchMedia(\n  '(prefers-reduced-motion: reduce)'\n).matches;\n\nfunction draw() {\n  if (prefersReducedMotion) {\n    // Show static or step-by-step view\n    drawStaticState();\n  } else {\n    // Show full animation\n    animatePhysics();\n  }\n}\n</code></pre>"},{"location":"chapters/12-accessibility-deployment-completion/#reduced-motion-alternatives","title":"Reduced Motion Alternatives","text":"<p>Instead of removing animations entirely, provide alternatives:</p> Full Animation Reduced Motion Alternative Continuous movement Step-by-step with button clicks Smooth transitions Instant state changes Auto-play animation Play on demand Parallax scrolling Static layout"},{"location":"chapters/12-accessibility-deployment-completion/#multilingual-support-and-vocabulary-level","title":"Multilingual Support and Vocabulary Level","text":"<p>MicroSims travel. A simulation created in English might be used by Spanish-speaking students in Mexico, Mandarin-speaking students in China, or Arabic-speaking students in Jordan. Even within one language, vocabulary level matters: a graduate physics student and a middle schooler need different terminology.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#vocabulary-level-considerations","title":"Vocabulary Level Considerations","text":"Audience Vocabulary Approach Example Elementary Common words, no jargon \"how fast it falls\" Middle School Introduce terms with definitions \"acceleration (how quickly speed changes)\" High School Technical terms expected \"gravitational acceleration\" College+ Full jargon \"uniform gravitational field approximation\""},{"location":"chapters/12-accessibility-deployment-completion/#internationalization-best-practices","title":"Internationalization Best Practices","text":"<ol> <li>Externalize strings: Don't hardcode text in JavaScript</li> <li>Use language files: JSON or similar for translations</li> <li>Consider text expansion: German and Spanish often need 30% more space than English</li> <li>Handle RTL languages: Arabic and Hebrew read right-to-left</li> <li>Format numbers and units: Different locales use different decimal separators</li> </ol> <pre><code>// Externalized strings approach\nconst strings = {\n  en: {\n    startButton: \"Start Simulation\",\n    resetButton: \"Reset\",\n    massLabel: \"Mass (kg)\"\n  },\n  es: {\n    startButton: \"Iniciar Simulaci\u00f3n\",\n    resetButton: \"Reiniciar\",\n    massLabel: \"Masa (kg)\"\n  }\n};\n\nfunction getLabel(key) {\n  return strings[currentLanguage][key] || strings.en[key];\n}\n</code></pre>"},{"location":"chapters/12-accessibility-deployment-completion/#cultural-sensitivity-design-that-travels-well","title":"Cultural Sensitivity: Design That Travels Well","text":"<p>Beyond language, cultures differ in ways that affect how educational content is perceived. What's an obvious metaphor in one culture may be meaningless or offensive in another.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#cultural-considerations-in-microsim-design","title":"Cultural Considerations in MicroSim Design","text":"Element Potential Issue Mitigation Colors Red = luck (China), danger (West), mourning (S. Africa) Use color + symbols; allow customization Icons Thumbs up offensive in some cultures Use universal symbols when possible Examples Sports, holidays, foods vary by culture Use universally relatable scenarios Reading order Left-to-right vs. right-to-left Design layouts that adapt Gestures Pointing, hand signals vary Avoid culturally-specific gestures Names \"John and Jane\" are anglicentric Use diverse, culturally neutral names"},{"location":"chapters/12-accessibility-deployment-completion/#universal-scenarios","title":"Universal Scenarios","text":"<p>Some scenarios translate across cultures:</p> <ul> <li>Physics: Gravity, motion, light work the same everywhere</li> <li>Math: Numbers and geometric relationships are universal</li> <li>Nature: Plants growing, water flowing, seasons changing</li> <li>Abstract: Shapes, colors, patterns without cultural meaning</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#prior-knowledge-support-and-differentiation-strategies","title":"Prior Knowledge Support and Differentiation Strategies","text":"<p>Learners arrive at your MicroSim with vastly different backgrounds. A differentiation strategy ensures that neither struggling nor advanced learners are left behind.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#supporting-different-prior-knowledge-levels","title":"Supporting Different Prior Knowledge Levels","text":"Learner Type Challenge Solution in MicroSim Novice Overwhelmed by options Guided mode with step-by-step instructions Intermediate Needs some scaffolding Optional hints, preset scenarios Advanced Bored by slow pace Skip intro, full parameter access Expert Wants edge cases Unlock advanced options, raw data access"},{"location":"chapters/12-accessibility-deployment-completion/#implementing-differentiation","title":"Implementing Differentiation","text":"<pre><code>const modes = {\n  guided: {\n    showInstructions: true,\n    limitedControls: true,\n    autoProgress: true\n  },\n  explore: {\n    showInstructions: false,\n    limitedControls: false,\n    autoProgress: false\n  },\n  expert: {\n    showInstructions: false,\n    limitedControls: false,\n    autoProgress: false,\n    showAdvanced: true,\n    showRawData: true\n  }\n};\n\nfunction setMode(modeName) {\n  const settings = modes[modeName];\n  applySettings(settings);\n  describe(`Switched to ${modeName} mode`);\n}\n</code></pre>"},{"location":"chapters/12-accessibility-deployment-completion/#accessibility-audit-and-constraint-simulation","title":"Accessibility Audit and Constraint Simulation","text":"<p>How do you know if your MicroSim is truly accessible? You audit it\u2014systematically testing against accessibility criteria.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#the-accessibility-audit-checklist","title":"The Accessibility Audit Checklist","text":"Category Test Pass Criteria Keyboard Tab through all controls All interactive elements reachable Screen Reader Listen with NVDA/VoiceOver All content announced, describe() works Color View in grayscale Information still distinguishable Motion Enable reduced motion Usable without animation Zoom Zoom to 200% Layout remains usable Text Increase font size Text remains readable Touch Test on tablet Touch targets \u2265 44\u00d744 pixels"},{"location":"chapters/12-accessibility-deployment-completion/#constraint-simulation-experience-your-learners-challenges","title":"Constraint Simulation: Experience Your Learners' Challenges","text":"<p>The best way to understand accessibility needs is to experience constraints yourself:</p> <ul> <li>Use your MicroSim keyboard-only for 10 minutes (no mouse!)</li> <li>Turn on a screen reader and close your eyes</li> <li>Install a color blindness simulator browser extension</li> <li>Turn on high contrast mode in your OS</li> <li>Enable reduced motion in system preferences</li> </ul> <p>This isn't about feeling sorry for learners with disabilities\u2014it's about understanding what \"usable\" actually means for them.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#diagram-accessibility-audit-workflow","title":"Diagram: Accessibility Audit Workflow","text":"Accessibility Audit Workflow <p>Type: workflow</p> <p>Bloom Taxonomy: Evaluate (L5)</p> <p>Learning Objective: Students will evaluate MicroSim accessibility by conducting systematic audits across multiple accessibility dimensions.</p> <p>Visual style: Checklist flowchart with pass/fail branches</p> <p>Audit phases:</p> <ol> <li> <p>Start: \"Begin Accessibility Audit\"</p> </li> <li> <p>Test: \"Keyboard Navigation\"</p> </li> <li>Can you Tab to all controls?</li> <li>Can you activate all buttons with Enter/Space?</li> <li> <p>Is focus visible at all times?    \u2192 Pass: Move to next test    \u2192 Fail: Log issue, continue</p> </li> <li> <p>Test: \"Screen Reader Compatibility\"</p> </li> <li>Does describe() provide current state?</li> <li>Are all controls labeled?</li> <li> <p>Are live regions announcing changes?    \u2192 Pass: Move to next test    \u2192 Fail: Log issue, continue</p> </li> <li> <p>Test: \"Color and Contrast\"</p> </li> <li>Does it pass WCAG contrast ratios?</li> <li>Is color not the only differentiator?</li> <li> <p>Is it usable in grayscale?    \u2192 Pass: Move to next test    \u2192 Fail: Log issue, continue</p> </li> <li> <p>Test: \"Motion and Animation\"</p> </li> <li>Does it respect prefers-reduced-motion?</li> <li>Can animations be paused?</li> <li> <p>Is there a static alternative?    \u2192 Pass: Move to next test    \u2192 Fail: Log issue, continue</p> </li> <li> <p>Test: \"Touch and Motor\"</p> </li> <li>Are touch targets \u2265 44\u00d744px?</li> <li>Is precision not required?</li> <li> <p>Are there keyboard alternatives to drag?    \u2192 Pass: Move to next test    \u2192 Fail: Log issue, continue</p> </li> <li> <p>Summary: \"Generate Audit Report\"</p> </li> <li>List all passed tests</li> <li>List all failed tests with severity</li> <li> <p>Prioritize fixes</p> </li> <li> <p>End: \"Audit Complete\"</p> </li> </ol> <p>Color coding: - Pass: Green - Fail: Red - In progress: Yellow - Summary: Blue</p> <p>Implementation: Mermaid or HTML/CSS/JavaScript flowchart</p>"},{"location":"chapters/12-accessibility-deployment-completion/#lms-integration-getting-your-microsims-to-learners","title":"LMS Integration: Getting Your MicroSims to Learners","text":"<p>You've built an accessible, well-tested MicroSim. Now how do you get it into learners' hands? For most educational contexts, that means integration with a Learning Management System (LMS).</p>"},{"location":"chapters/12-accessibility-deployment-completion/#common-lms-platforms","title":"Common LMS Platforms","text":"LMS Common Context Integration Method Canvas Higher Ed, K-12 LTI, embed iframe Moodle Global, open source LTI, H5P, iframe Blackboard Higher Ed LTI, Building Blocks Google Classroom K-12 Link sharing, embed D2L Brightspace Higher Ed, Corporate LTI, widget"},{"location":"chapters/12-accessibility-deployment-completion/#integration-methods","title":"Integration Methods","text":"<p>1. Simple Embedding (iframe)</p> <p>The simplest approach: host your MicroSim on a web server and embed via iframe:</p> <pre><code>&lt;iframe src=\"https://yoursite.github.io/microsims/gravity/main.html\"\n        width=\"100%\"\n        height=\"600px\"\n        title=\"Gravity Simulator\"&gt;\n&lt;/iframe&gt;\n</code></pre> <p>Pros: Simple, works anywhere that allows iframes Cons: Limited integration, no grade passback</p> <p>2. LTI (Learning Tools Interoperability)</p> <p>LTI is the standard for deep LMS integration. It enables:</p> <ul> <li>Single sign-on (students don't create new accounts)</li> <li>Grade passback (MicroSim can report scores to gradebook)</li> <li>Context information (which course, which student)</li> </ul> <p>3. SCORM/xAPI Packaging</p> <p>For tracking learning activities (see Chapter 11), SCORM or xAPI can package your MicroSim with analytics capabilities.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#intelligent-textbook-integration","title":"Intelligent Textbook Integration","text":"<p>Beyond traditional LMS integration, MicroSims shine when embedded in intelligent textbooks\u2014interactive digital texts that adapt to learner needs.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#what-makes-a-textbook-intelligent","title":"What Makes a Textbook \"Intelligent\"?","text":"<ul> <li>Embedded interactivity: MicroSims, quizzes, and exercises inline with content</li> <li>Adaptive sequencing: Different paths based on learner performance</li> <li>Progress tracking: Knowing what each learner has mastered</li> <li>Personalized recommendations: Suggesting what to study next</li> <li>Non-linear navigation: Learners choose their own path (as discussed in Chapter 11)</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#mkdocs-material-our-intelligent-textbook-platform","title":"MkDocs Material: Our Intelligent Textbook Platform","text":"<p>This course uses MkDocs Material, which supports:</p> <ul> <li>Markdown-based content for easy authoring</li> <li>Embedded iframes for MicroSims</li> <li>Navigation and search</li> <li>Mobile-responsive design</li> <li>Progressive Web App capabilities</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#embedding-microsims-in-mkdocs","title":"Embedding MicroSims in MkDocs","text":"<pre><code>## The Gravity Simulator\n\nExplore how mass affects falling objects:\n\n&lt;iframe src=\"../sims/gravity-simulator/main.html\"\n        width=\"100%\"\n        height=\"600px\"&gt;\n&lt;/iframe&gt;\n\n[Run in Fullscreen](../sims/gravity-simulator/main.html){ .md-button }\n[Edit in p5.js Editor](https://editor.p5js.org/yourname/sketches/abc123)\n</code></pre>"},{"location":"chapters/12-accessibility-deployment-completion/#learning-analytics-and-interaction-tracking","title":"Learning Analytics and Interaction Tracking","text":"<p>Once deployed, your MicroSim can generate valuable data about how learners interact with it. This learning analytics data informs both immediate interventions and long-term design improvements.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#what-to-track","title":"What to Track","text":"Metric What It Reveals Design Implication Time on simulation Engagement level Too short: confusing? Too long: stuck? Controls used Exploration patterns Unused controls: remove or highlight? Parameter values tested Hypothesis testing behavior Are they finding the key insight? Reset frequency Experimentation vs. confusion High resets might indicate frustration Completion rate Overall success Low completion: too hard? Too long? Sequence of actions Learning pathway Optimal sequences vs. struggling patterns"},{"location":"chapters/12-accessibility-deployment-completion/#privacy-and-ethics-revisited","title":"Privacy and Ethics (Revisited)","text":"<p>As discussed in Chapter 11, learning analytics involves sensitive data. Ensure:</p> <ul> <li>Clear consent mechanisms</li> <li>Data minimization (collect only what you need)</li> <li>Secure storage and transmission</li> <li>Compliance with FERPA, GDPR, institutional policies</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#maintenance-planning-the-long-game","title":"Maintenance Planning: The Long Game","text":"<p>Launching a MicroSim isn't the end\u2014it's the beginning of ongoing maintenance. Technology changes, bugs emerge, and educational standards evolve.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#maintenance-categories","title":"Maintenance Categories","text":"Type Frequency Examples Bug fixes As needed Crash fixes, calculation errors Dependency updates Quarterly p5.js version updates Browser compatibility Annually New browser features/deprecations Content updates As needed Correcting factual errors Accessibility improvements Ongoing Responding to user feedback Feature enhancements Roadmap New capabilities based on user needs"},{"location":"chapters/12-accessibility-deployment-completion/#maintenance-planning-checklist","title":"Maintenance Planning Checklist","text":"<ul> <li>Version control: All MicroSims in Git repository</li> <li>Issue tracking: GitHub Issues or similar for bug reports</li> <li>Dependency monitoring: Alerts for library vulnerabilities</li> <li>Analytics review: Monthly check for usage patterns</li> <li>User feedback channel: Way for educators to report problems</li> <li>Documentation updates: Keep README current</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#library-organization-building-your-microsim-collection","title":"Library Organization: Building Your MicroSim Collection","text":"<p>As you create more MicroSims, organization becomes crucial. You can't reuse what you can't find (remember Chapter 10's golden rule!).</p>"},{"location":"chapters/12-accessibility-deployment-completion/#organizational-strategies","title":"Organizational Strategies","text":"<p>1. By Subject Area</p> <pre><code>microsims/\n\u251c\u2500\u2500 physics/\n\u2502   \u251c\u2500\u2500 gravity/\n\u2502   \u251c\u2500\u2500 pendulum/\n\u2502   \u2514\u2500\u2500 waves/\n\u251c\u2500\u2500 math/\n\u2502   \u251c\u2500\u2500 fractions/\n\u2502   \u251c\u2500\u2500 graphing/\n\u2502   \u2514\u2500\u2500 probability/\n\u2514\u2500\u2500 chemistry/\n    \u251c\u2500\u2500 periodic-table/\n    \u2514\u2500\u2500 molecular-bonds/\n</code></pre> <p>2. By Visualization Type</p> <pre><code>microsims/\n\u251c\u2500\u2500 p5js-animations/\n\u251c\u2500\u2500 network-graphs/\n\u251c\u2500\u2500 timelines/\n\u251c\u2500\u2500 charts/\n\u2514\u2500\u2500 infographics/\n</code></pre> <p>3. By Cognitive Level</p> <pre><code>microsims/\n\u251c\u2500\u2500 remember/\n\u251c\u2500\u2500 understand/\n\u251c\u2500\u2500 apply/\n\u251c\u2500\u2500 analyze/\n\u251c\u2500\u2500 evaluate/\n\u2514\u2500\u2500 create/\n</code></pre>"},{"location":"chapters/12-accessibility-deployment-completion/#metadata-for-discoverability","title":"Metadata for Discoverability","text":"<p>Each MicroSim should have a <code>metadata.json</code> file (see Chapter 10) that enables filtering and search:</p> <ul> <li>Subject area</li> <li>Grade level</li> <li>Bloom's taxonomy level</li> <li>JavaScript library used</li> <li>Prerequisites</li> <li>Learning objectives</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#educator-collaboration-and-content-sharing","title":"Educator Collaboration and Content Sharing","text":"<p>MicroSims become more valuable when educators collaborate\u2014sharing, adapting, and improving each other's work.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#collaboration-models","title":"Collaboration Models","text":"Model Description Pros Cons Open repository All MicroSims publicly available Maximum sharing Quality control challenges Curated gallery Reviewed MicroSims selected for inclusion Quality assured Slower updates Institutional sharing Within school/district/university Controlled access Limited reach Commercial marketplace Paid MicroSims Author compensation Access barriers"},{"location":"chapters/12-accessibility-deployment-completion/#licensing-considerations","title":"Licensing Considerations","text":"<p>Choose licenses that reflect your sharing intent:</p> <ul> <li>CC BY: Anyone can use/modify with attribution</li> <li>CC BY-SA: Same as above, derivatives must use same license</li> <li>CC BY-NC: Non-commercial use only</li> <li>MIT License: Very permissive for code</li> <li>All Rights Reserved: No sharing without permission</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#building-a-sharing-culture","title":"Building a Sharing Culture","text":"<ul> <li>Document thoroughly (others must understand your design)</li> <li>Use consistent structure (follow standards like our MicroSim architecture)</li> <li>Respond to feedback (engage with users who report issues)</li> <li>Acknowledge contributors (give credit for improvements)</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#portfolio-project-demonstrating-your-mastery","title":"Portfolio Project: Demonstrating Your Mastery","text":"<p>Throughout this course, you've learned principles, practiced skills, and created MicroSims. The portfolio project brings it all together into a comprehensive demonstration of your capabilities.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#portfolio-requirements","title":"Portfolio Requirements","text":"<p>Your final portfolio should include:</p> <p>1. Learning Objective Analysis Document - Original learning objective - Bloom's taxonomy classification - Decomposition into testable components - Simulation readiness assessment</p> <p>2. Detailed MicroSim Specification - Visual design mockup - Interaction specification - Accessibility requirements - Target audience profile</p> <p>3. Working MicroSim - Functional implementation - Responsive design - Full accessibility (describe(), keyboard, color) - Quality score \u2265 85</p> <p>4. Evaluation Rubric - Technical criteria - Pedagogical criteria - UX criteria - Self-assessment against rubric</p> <p>5. User Testing Report - Testing methodology - Participant demographics - Findings (what worked, what didn't) - Changes made in response</p> <p>6. Iteration Log - Version history - Rationale for each change - Before/after comparisons</p> <p>7. Accessibility Audit - Audit methodology - Results by category - Remediation actions</p> <p>8. Deployment Documentation - Where the MicroSim is deployed - Integration method - Maintenance plan</p>"},{"location":"chapters/12-accessibility-deployment-completion/#diagram-portfolio-project-components","title":"Diagram: Portfolio Project Components","text":"Portfolio Project Components <p>Type: infographic</p> <p>Bloom Taxonomy: Create (L6)</p> <p>Learning Objective: Students will create a comprehensive portfolio demonstrating mastery of MicroSim development across all course competencies.</p> <p>Layout: Central hub with eight radiating components</p> <p>Center: \"Portfolio Project\" with graduation cap icon</p> <p>Eight components arranged in circle:</p> <ol> <li>Learning Objective Analysis (top)</li> <li>Icon: Magnifying glass over document</li> <li> <p>Key elements: Bloom's level, decomposition, readiness</p> </li> <li> <p>MicroSim Specification (top-right)</p> </li> <li>Icon: Blueprint/wireframe</li> <li> <p>Key elements: Visual mockup, interactions, accessibility</p> </li> <li> <p>Working MicroSim (right)</p> </li> <li>Icon: Play button / simulation</li> <li> <p>Key elements: Functional, responsive, accessible</p> </li> <li> <p>Evaluation Rubric (bottom-right)</p> </li> <li>Icon: Checklist with scores</li> <li> <p>Key elements: Technical, pedagogical, UX criteria</p> </li> <li> <p>User Testing Report (bottom)</p> </li> <li>Icon: People with speech bubbles</li> <li> <p>Key elements: Methodology, findings, changes</p> </li> <li> <p>Iteration Log (bottom-left)</p> </li> <li>Icon: Version branches / timeline</li> <li> <p>Key elements: Versions, rationale, comparisons</p> </li> <li> <p>Accessibility Audit (left)</p> </li> <li>Icon: Universal access symbol</li> <li> <p>Key elements: Tests, results, fixes</p> </li> <li> <p>Deployment Documentation (top-left)</p> </li> <li>Icon: Cloud with arrow</li> <li>Key elements: Platform, integration, maintenance</li> </ol> <p>Connecting lines show dependencies: - Analysis \u2192 Specification \u2192 MicroSim - Testing \u2192 Iteration \u2192 Quality improvement - All feed into final portfolio</p> <p>Color scheme: - Design phase: Blue - Implementation phase: Green - Evaluation phase: Orange - Documentation phase: Purple</p> <p>Implementation: SVG or HTML/CSS infographic</p>"},{"location":"chapters/12-accessibility-deployment-completion/#reflection-journal-capturing-your-learning-journey","title":"Reflection Journal: Capturing Your Learning Journey","text":"<p>A reflection journal documents your growth as a MicroSim designer. It's both a learning tool (reflection deepens understanding) and a portfolio artifact (demonstrates metacognitive awareness).</p>"},{"location":"chapters/12-accessibility-deployment-completion/#reflection-prompts-by-phase","title":"Reflection Prompts by Phase","text":"<p>Design Phase: - What was challenging about translating the learning objective? - What design decisions felt uncertain? - What would you do differently next time?</p> <p>Implementation Phase: - Where did the specification prove insufficient? - What accessibility challenges emerged? - How did AI-generated code need refinement?</p> <p>Testing Phase: - What surprised you in user testing? - What did you learn about your assumptions? - How did real learners differ from your expectations?</p> <p>Iteration Phase: - Which feedback was hardest to act on? - What trade-offs did you make? - When did you decide the MicroSim was \"done enough\"?</p>"},{"location":"chapters/12-accessibility-deployment-completion/#reflection-best-practices","title":"Reflection Best Practices","text":"<ul> <li>Be specific: \"The slider was confusing\" \u2192 \"Three users expected the slider to control speed, not mass\"</li> <li>Be honest: Acknowledge what didn't work, not just successes</li> <li>Connect to principles: Link experiences to course concepts</li> <li>Plan forward: What will you do differently next time?</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#peer-feedback-learning-from-each-other","title":"Peer Feedback: Learning from Each Other","text":"<p>Peer feedback provides perspectives you can't get on your own. Other designers see things you miss, bring different expertise, and ask questions that reveal blind spots.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#effective-peer-feedback-structure","title":"Effective Peer Feedback Structure","text":"<p>For the giver: - Start with what works well (specific, not generic praise) - Identify areas for improvement (specific, actionable) - Ask clarifying questions (understand before judging) - Offer suggestions, not demands (\"Consider...\" not \"You must...\")</p> <p>For the receiver: - Listen fully before responding - Ask for clarification when needed - Thank feedback givers sincerely - Decide which feedback to act on (you don't have to accept everything)</p>"},{"location":"chapters/12-accessibility-deployment-completion/#peer-feedback-categories","title":"Peer Feedback Categories","text":"Category Questions to Ask Clarity Is the learning objective obvious? Usability Could you figure out the controls? Engagement Did you want to explore? Accessibility Did anything seem inaccessible? Effectiveness Did you learn what you were supposed to? Polish What feels unfinished?"},{"location":"chapters/12-accessibility-deployment-completion/#self-evaluation-honest-assessment-of-your-work","title":"Self-Evaluation: Honest Assessment of Your Work","text":"<p>The final step is honest self-assessment. Against the rubric you developed, how does your MicroSim measure up?</p>"},{"location":"chapters/12-accessibility-deployment-completion/#self-evaluation-framework","title":"Self-Evaluation Framework","text":"Criterion Exceeds (A) Meets (B) Developing (C) Needs Work (D) Learning objective alignment Directly and clearly addresses objective Mostly addresses objective Partially addresses objective Misses objective Technical quality Zero bugs, fully responsive Minor issues, mostly responsive Some bugs or responsiveness issues Major functionality problems Accessibility Full WCAG AA compliance Most accessibility requirements met Basic accessibility Accessibility gaps User experience Intuitive and engaging Usable with minor friction Confusing elements Frustrating experience Documentation Comprehensive and clear Complete but could improve Missing elements Inadequate"},{"location":"chapters/12-accessibility-deployment-completion/#the-growth-mindset-in-self-evaluation","title":"The Growth Mindset in Self-Evaluation","text":"<p>Self-evaluation isn't about proving you're perfect\u2014it's about identifying growth opportunities. The best MicroSim designers:</p> <ul> <li>Acknowledge what they don't know yet</li> <li>See critique as data, not attack</li> <li>Set specific improvement goals</li> <li>Celebrate progress, not just perfection</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#summary-youre-ready-to-change-the-world-one-microsim-at-a-time","title":"Summary: You're Ready to Change the World (One MicroSim at a Time)","text":"<p>You've completed a remarkable journey. From analyzing learning objectives in Chapter 1 to this moment of portfolio completion, you've built skills that will serve learners for years to come.</p> <p>Let's recap what you've accomplished:</p> <p>Accessibility Mastery: - Universal Design principles for building for everyone - UDL principles for educational accessibility - Screen reader support including p5.js describe() - Keyboard navigation to free learners from mouse dependency - Color accessibility and contrast design - Reduced motion for vestibular sensitivities - Multilingual and cultural considerations - Differentiation strategies for diverse learners - Accessibility auditing and constraint simulation</p> <p>Deployment Skills: - LMS integration methods (iframe, LTI, SCORM) - Intelligent textbook embedding - Learning analytics and interaction tracking - Maintenance planning for the long term - Library organization for findability - Educator collaboration and content sharing</p> <p>Professional Development: - Portfolio project demonstrating comprehensive skill - Reflection journal capturing growth - Peer feedback for continuous improvement - Self-evaluation for honest assessment</p>"},{"location":"chapters/12-accessibility-deployment-completion/#the-bigger-picture","title":"The Bigger Picture","text":"<p>Every accessible, well-designed MicroSim you create has the potential to:</p> <ul> <li>Make abstract concepts click for a struggling student</li> <li>Bring joy and curiosity to someone dreading a subject</li> <li>Bridge learning gaps that traditional methods miss</li> <li>Reach learners who would otherwise be excluded</li> <li>Scale quality education beyond what any single teacher could provide</li> </ul> <p>That's not just professional skill\u2014that's making the world a better place, one simulation at a time.</p> <p>Now go forth and build something wonderful. Your learners are waiting.</p>"},{"location":"chapters/12-accessibility-deployment-completion/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Universal Design means building for everyone from the start, not retrofitting accessibility</li> <li>UDL principles provide multiple means of representation, action/expression, and engagement</li> <li>p5.js describe() is required for screen reader accessibility and quality scores</li> <li>Controls below the canvas ensures keyboard accessibility; mouse-only interactions exclude users</li> <li>Color alone should never convey information\u2014always add shape, text, or pattern</li> <li>Reduced motion preferences must be respected for vestibular-sensitive users</li> <li>Cultural sensitivity extends beyond translation to examples, icons, and metaphors</li> <li>Differentiation strategies accommodate different prior knowledge levels</li> <li>Accessibility audits systematically verify compliance</li> <li>LMS integration brings MicroSims to learners through established platforms</li> <li>Maintenance planning ensures MicroSims remain functional over time</li> <li>Library organization with good metadata enables reuse</li> <li>Portfolio projects demonstrate comprehensive mastery</li> <li>Reflection and self-evaluation drive continuous improvement</li> </ul>"},{"location":"chapters/12-accessibility-deployment-completion/#reflection-questions","title":"Reflection Questions","text":"How would you retrofit accessibility into an existing MicroSim that relied entirely on mouse-based drag interactions? <p>Consider these approaches: - Add number input fields that set the same values as dragging - Implement arrow key controls for adjustment - Create preset buttons for common configurations - Add describe() to announce state changes - Ensure all functionality is reachable via Tab navigation</p> What accessibility constraints would you prioritize for testing during development? <p>Key constraints to experience firsthand: - Keyboard-only navigation (no mouse for 10 minutes) - Screen reader testing (even just a few minutes reveals issues) - Color blindness simulation (browser extension) - Reduced motion (system preference) - Zoom to 200% (verify layout integrity)</p> How would you explain the importance of accessibility to a stakeholder focused only on 'core functionality'? <p>Key arguments: - Legal compliance (ADA, Section 508, WCAG requirements) - Larger audience (8%+ of users have color blindness alone) - Better for everyone (accessible design often improves general UX) - Institutional reputation (commitment to inclusion) - Ethical responsibility (education should be for everyone)</p>"},{"location":"chapters/12-accessibility-deployment-completion/#references","title":"References","text":"<ol> <li> <p>W3C Web Accessibility Initiative. (2018). Web Content Accessibility Guidelines (WCAG) 2.1. https://www.w3.org/TR/WCAG21/ \u2014 The standard for web accessibility.</p> </li> <li> <p>CAST. (2018). Universal Design for Learning Guidelines version 2.2. https://udlguidelines.cast.org/ \u2014 The authoritative UDL framework.</p> </li> <li> <p>p5.js Accessibility. (2023). p5.js Web Accessibility. https://p5js.org/learn/accessibility.html \u2014 Official guide to p5.js accessibility features.</p> </li> <li> <p>WebAIM. (2023). Color Contrast Checker. https://webaim.org/resources/contrastchecker/ \u2014 Tool for verifying contrast ratios.</p> </li> <li> <p>The Paciello Group. (2019). The Business Case for Accessibility. https://www.tpgi.com/the-business-case-for-accessibility/ \u2014 Why accessibility matters for organizations.</p> </li> <li> <p>IMS Global. (2023). Learning Tools Interoperability (LTI). https://www.imsglobal.org/lti \u2014 Standard for LMS integration.</p> </li> </ol>"},{"location":"chapters/12-accessibility-deployment-completion/quiz/","title":"Quiz: Accessibility, Deployment, and Course Completion","text":"<p>Test your understanding of accessibility principles, deployment strategies, and portfolio development for MicroSims.</p>"},{"location":"chapters/12-accessibility-deployment-completion/quiz/#1-what-is-universal-design","title":"1. What is Universal Design?","text":"<ol> <li>A design approach that creates one version for the most common user</li> <li>A philosophy that products should be usable by the widest possible range of people without requiring adaptation</li> <li>A design system for creating universally compatible file formats</li> <li>A method for translating designs into multiple languages</li> </ol> Show Answer <p>The correct answer is B. Universal Design is the philosophy that products should be usable by the widest possible range of people without requiring adaptation or specialized design. It originated in architecture (think: automatic doors that help wheelchair users, parents with strollers, and tired shoppers alike) but applies beautifully to educational technology. Building for everyone from the start is far more effective than retrofitting accessibility later.</p> <p>Concept Tested: Universal Design</p> <p>See: Chapter Content</p>"},{"location":"chapters/12-accessibility-deployment-completion/quiz/#2-what-are-the-three-principles-of-universal-design-for-learning-udl","title":"2. What are the three principles of Universal Design for Learning (UDL)?","text":"<ol> <li>Simplicity, Consistency, and Feedback</li> <li>Multiple Means of Representation, Action/Expression, and Engagement</li> <li>Visual, Auditory, and Kinesthetic learning styles</li> <li>Input, Processing, and Output modalities</li> </ol> Show Answer <p>The correct answer is B. The three UDL principles are: Multiple Means of Representation (present information in multiple formats), Multiple Means of Action and Expression (let learners interact and demonstrate understanding in various ways), and Multiple Means of Engagement (offer different ways to motivate and sustain interest). UDL recognizes that learners vary in how they perceive information, navigate environments, and get engaged.</p> <p>Concept Tested: UDL Principles</p> <p>See: Chapter Content</p>"},{"location":"chapters/12-accessibility-deployment-completion/quiz/#3-what-is-the-p5js-describe-function-and-why-is-it-required-for-quality-scores","title":"3. What is the p5.js describe() function, and why is it required for quality scores?","text":"<ol> <li>A function that describes the code structure for documentation purposes</li> <li>A function that creates a description of canvas content for screen readers, making the MicroSim accessible to blind learners</li> <li>A function that generates metadata about the MicroSim file</li> <li>A function that describes errors in the console for debugging</li> </ol> Show Answer <p>The correct answer is B. The describe() function creates a description of the canvas content that screen readers can announce. For learners who are blind or have low vision, screen readers are their window into your MicroSim. MicroSims without proper describe() implementation cannot achieve high quality scores\u2014this isn't bureaucratic box-checking; it's the difference between \"usable by everyone\" and \"excludes blind learners entirely.\"</p> <p>Concept Tested: Screen Reader Support</p> <p>See: Chapter Content</p>"},{"location":"chapters/12-accessibility-deployment-completion/quiz/#4-why-does-the-recommended-microsim-architecture-place-controls-below-the-drawing-area","title":"4. Why does the recommended MicroSim architecture place controls below the drawing area?","text":"<ol> <li>For aesthetic reasons\u2014it looks more professional</li> <li>To ensure keyboard users can Tab through focusable controls instead of the canvas being one opaque block</li> <li>To maximize the canvas size for larger animations</li> <li>To reduce memory usage by separating visual and control elements</li> </ol> Show Answer <p>The correct answer is B. Placing controls below the canvas rather than overlapping it is an accessibility decision. When controls overlap the canvas, keyboard users can't tab to canvas regions, and for screen reader users, the canvas is one opaque block. With controls below the canvas, Tab navigation flows through focusable controls logically, making the MicroSim accessible to users who can't use a mouse due to motor impairments, visual impairments, or other reasons.</p> <p>Concept Tested: Keyboard Navigation</p> <p>See: Chapter Content</p>"},{"location":"chapters/12-accessibility-deployment-completion/quiz/#5-what-is-the-key-principle-for-designing-for-color-blindness","title":"5. What is the key principle for designing for color blindness?","text":"<ol> <li>Use only grayscale colors</li> <li>Never rely on color alone to convey information\u2014always add shape, text, or pattern</li> <li>Avoid using red and green in any design</li> <li>Use the brightest colors possible for maximum visibility</li> </ol> Show Answer <p>The correct answer is B. Never rely on color alone to convey information. Approximately 8% of men have some form of color vision deficiency. Instead of using only color to distinguish elements (like green for correct, red for wrong), combine color with other visual cues: different shapes, checkmarks or X marks, labels, or patterns. Colorblind-safe palettes like blue/orange are distinguishable by almost everyone.</p> <p>Concept Tested: Color Blindness Design, Color Accessibility</p> <p>See: Chapter Content</p>"},{"location":"chapters/12-accessibility-deployment-completion/quiz/#6-how-should-a-microsim-respond-to-the-prefers-reduced-motion-system-preference","title":"6. How should a MicroSim respond to the prefers-reduced-motion system preference?","text":"<ol> <li>Ignore it\u2014animations are essential for learning</li> <li>Provide alternatives like step-by-step views or instant state changes instead of continuous animation</li> <li>Display a warning that the MicroSim requires motion</li> <li>Reduce the frame rate but keep animations running</li> </ol> Show Answer <p>The correct answer is B. Animations can trigger vestibular disorders, causing dizziness, nausea, or migraines. When users set a preference for reduced motion, MicroSims should provide alternatives: step-by-step with button clicks instead of continuous movement, instant state changes instead of smooth transitions, play on demand instead of auto-play, and static layouts instead of parallax scrolling.</p> <p>Concept Tested: Reduced Motion</p> <p>See: Chapter Content</p>"},{"location":"chapters/12-accessibility-deployment-completion/quiz/#7-what-is-the-purpose-of-an-accessibility-audit","title":"7. What is the purpose of an accessibility audit?","text":"<ol> <li>To verify compliance with copyright laws</li> <li>To systematically test a MicroSim against accessibility criteria including keyboard, screen reader, color, and motion</li> <li>To audit the development costs of accessibility features</li> <li>To check which users have accessed the MicroSim</li> </ol> Show Answer <p>The correct answer is B. An accessibility audit systematically tests against accessibility criteria: keyboard (all interactive elements reachable via Tab), screen reader (content announced, describe() works), color (information distinguishable in grayscale), motion (respects reduced-motion preference), zoom (layout usable at 200%), text (remains readable when enlarged), and touch (targets at least 44x44 pixels). The best way to understand accessibility needs is to experience constraints yourself.</p> <p>Concept Tested: Accessibility Audit</p> <p>See: Chapter Content</p>"},{"location":"chapters/12-accessibility-deployment-completion/quiz/#8-what-is-lti-learning-tools-interoperability-and-what-does-it-enable","title":"8. What is LTI (Learning Tools Interoperability) and what does it enable?","text":"<ol> <li>A programming language for creating learning tools</li> <li>A standard for deep LMS integration enabling single sign-on, grade passback, and context information</li> <li>A file format for packaging educational content</li> <li>A testing framework for learning management systems</li> </ol> Show Answer <p>The correct answer is B. LTI (Learning Tools Interoperability) is the standard for deep LMS integration. It enables single sign-on (students don't create new accounts), grade passback (MicroSim can report scores to the gradebook), and context information (which course, which student). This is more powerful than simple iframe embedding, which has limited integration and no grade passback.</p> <p>Concept Tested: LMS Integration</p> <p>See: Chapter Content</p>"},{"location":"chapters/12-accessibility-deployment-completion/quiz/#9-what-makes-a-textbook-intelligent-according-to-the-chapter","title":"9. What makes a textbook \"intelligent\" according to the chapter?","text":"<ol> <li>It uses artificial intelligence to grade student work</li> <li>It includes embedded interactivity, adaptive sequencing, progress tracking, and non-linear navigation</li> <li>It automatically updates its content from the internet</li> <li>It can read itself aloud to students</li> </ol> Show Answer <p>The correct answer is B. An intelligent textbook features: embedded interactivity (MicroSims, quizzes, and exercises inline with content), adaptive sequencing (different paths based on learner performance), progress tracking (knowing what each learner has mastered), personalized recommendations (suggesting what to study next), and non-linear navigation (learners choose their own path). MicroSims shine when embedded in these adaptive environments.</p> <p>Concept Tested: Intelligent Textbook</p> <p>See: Chapter Content</p>"},{"location":"chapters/12-accessibility-deployment-completion/quiz/#10-what-are-the-eight-components-required-in-a-portfolio-project-according-to-the-chapter","title":"10. What are the eight components required in a portfolio project according to the chapter?","text":"<ol> <li>Resume, cover letter, references, work samples, certifications, awards, publications, presentations</li> <li>Learning objective analysis, specification, working MicroSim, evaluation rubric, user testing report, iteration log, accessibility audit, deployment documentation</li> <li>Code files, documentation, tests, README, license, changelog, contributing guide, issue templates</li> <li>Design mockups, wireframes, prototypes, final designs, style guide, component library, documentation, presentation</li> </ol> Show Answer <p>The correct answer is B. The portfolio project requires: (1) Learning Objective Analysis Document, (2) Detailed MicroSim Specification, (3) Working MicroSim with quality score of 85 or higher, (4) Evaluation Rubric, (5) User Testing Report, (6) Iteration Log, (7) Accessibility Audit, and (8) Deployment Documentation. Together, these demonstrate comprehensive mastery of MicroSim development across all course competencies.</p> <p>Concept Tested: Portfolio Project</p> <p>See: Chapter Content</p>"},{"location":"chapters/13-capstone-projects/","title":"Capstone Projects","text":"<p>Now that you are an expert at automating instructional design you get to show your instructor and your classmates what you can do. This chapter only contains high level guidelines on what types of projects you might take on.  It also makes sense for larger projects to have teams of two or more people working on these projects.</p>"},{"location":"chapters/13-capstone-projects/#suggestions-for-success","title":"Suggestions for Success","text":"<p>For a capstone that combines Intelligent Textbooks + MicroSims, the biggest risk is not technical failure\u2014it\u2019s diffuse scope and shallow integration. The guidance below is designed to help teams of 3\u20135 students focus on learning impact, coherence, and evidence of thinking, not just shiny demos.</p>"},{"location":"chapters/13-capstone-projects/#1-start-with-a-learning-spine-not-a-an-entire-textbook","title":"1. Start With a Learning Spine, Not a an Entire Textbook","text":"<p>Before any writing or coding, require teams to define a learning spine:</p> <ul> <li>Target learner (age, background, constraints)</li> <li>One core transformation:   \u201cBefore this book, the learner could not ; after, they can .\u201d</li> </ul> <p>Have them express this as a single sentence and get it approved.</p> <p>Why: This prevents the \u201cCopy a Wikipedia article and add random sims\u201d failure mode.</p>"},{"location":"chapters/13-capstone-projects/#2-define-a-concept-dependency-graph-early","title":"2. Define a Concept Dependency Graph Early","text":"<p>Ask teams to explicitly create a concept dependency graph (even a rough one):</p> <ul> <li>15\u201330 concepts total (30-60 concepts for a course)</li> <li>Clear prerequisites</li> <li>No cycles</li> <li>One capstone concept at the far right</li> </ul> <p>This graph becomes the table of contents, not the other way around.</p> <p>Rule of thumb: If a concept does not enable another concept, question whether it belongs in the project.</p>"},{"location":"chapters/13-capstone-projects/#3-map-each-microsim-to-a-cognitive-purpose","title":"3. Map Each MicroSim to a Cognitive Purpose","text":"<p>Require that every MicroSim declare its instructional role, not just its behavior.</p> <p>For each MicroSim, teams should state:</p> <ul> <li>Which concept(s) it supports</li> <li>Bloom level(s) it targets (e.g., Understand \u2192 Analyze)</li> <li>What misconception it helps surface or correct</li> </ul> <p>Example:</p> <p>\u201cThis MicroSim targets Understanding and Applying Ohm\u2019s Law by allowing learners to vary resistance and observe non-linear power effects.\u201d</p> <p>Anti-pattern to avoid: \u201cFun but conceptually orphaned\u201d simulations.</p>"},{"location":"chapters/13-capstone-projects/#4-design-microsims-as-experiments-not-animations","title":"4. Design MicroSims as Experiments, Not Animations","text":"<p>Encourage teams to treat MicroSims as interactive experiments:</p> <ul> <li>At least one controllable variable</li> <li>One observable outcome</li> <li>One learner question the sim helps answer</li> </ul> <p>Ask teams to explicitly state:</p> <ul> <li>What should the learner predict before running the sim?</li> <li>What should surprise them?</li> </ul> <p>This aligns naturally with PRIMM-style learning without naming it explicitly.</p>"},{"location":"chapters/13-capstone-projects/#5-require-accessibility-as-a-first-class-constraint","title":"5. Require Accessibility as a First-Class Constraint","text":"<p>Before coding begins, teams should answer:</p> <ul> <li> <p>How can this MicroSim be explored with:</p> </li> <li> <p>Keyboard only?</p> </li> <li>Reduced motion?</li> <li>Screen reader narration?</li> <li>What textual feedback mirrors the visual state?</li> </ul> <p>This reframes accessibility as design intelligence, not compliance.</p>"},{"location":"chapters/13-capstone-projects/#6-limit-the-scope-on-purpose","title":"6. Limit the Scope on Purpose","text":"<p>Give teams explicit limits:</p> <ul> <li>5\u20137 chapters maximum</li> <li>1\u20132 MicroSims per chapter</li> <li>One \u201chero\u201d MicroSim they polish deeply</li> </ul> <p>Why: Depth beats coverage. A well-integrated MicroSim is more impressive than five shallow ones.</p> <p>Note</p> <p>You will not receive a lower grade for an incomplete textbook. We encourage teams to push themselves for depth, not breath.</p>"},{"location":"chapters/13-capstone-projects/#7-encourage-clear-team-roles-but-shared-ownership","title":"7. Encourage Clear Team Roles (but Shared Ownership)","text":"<p>Suggest\u2014but do not rigidly enforce\u2014roles:</p> <ul> <li>Learning Architect (concepts, flow, assessments)</li> <li>MicroSim Designer (interaction, experimentation)</li> <li>Content Curator (writing, examples, clarity)</li> <li>Integrator (MkDocs structure, navigation, polish)</li> </ul> <p>Make it clear:</p> <ul> <li>Everyone must contribute to at least one MicroSim</li> <li>Everyone must contribute to concept modeling</li> </ul>"},{"location":"chapters/13-capstone-projects/#8-require-an-explicit-ai-usage-disclosure","title":"8. Require an Explicit \u201cAI Usage Disclosure\u201d","text":"<p>Since this is an intelligent textbook, ask teams to document:</p> <ul> <li>Where AI was used (drafting, concept extraction, sim ideation, feedback)</li> <li>Where human judgment overrode AI output</li> <li>One mistake the AI made and how it was corrected</li> </ul> <p>This reinforces critical AI literacy, not passive use.</p>"},{"location":"chapters/13-capstone-projects/#9-define-success-in-terms-of-learner-evidence","title":"9. Define Success in Terms of Learner Evidence","text":"<p>Ask teams to include:</p> <ul> <li>3\u20135 learner tasks or challenges</li> <li>What success looks like for each</li> <li>How the MicroSims support those tasks</li> </ul> <p>Optional but powerful:</p> <ul> <li>A short \u201cIf we had more time\u201d reflection identifying the next MicroSim they would build.</li> </ul>"},{"location":"chapters/13-capstone-projects/#10-end-with-a-narrative-not-a-demo","title":"10. End With a Narrative, Not a Demo","text":"<p>For final presentations, encourage teams to:</p> <ul> <li>Walk the audience through a learner journey</li> <li>Show confusion \u2192 exploration \u2192 insight</li> <li>Use one MicroSim live, not many slides</li> </ul> <p>The goal is to demonstrate:</p> <p>\u201cWe understand how people learn with interactive systems.\u201d</p>"},{"location":"chapters/13-capstone-projects/#a-strong-framing-sentence-you-can-give-students","title":"A Strong Framing Sentence You Can Give Students","text":"<p>\u201cYour capstone is not a book with simulations attached\u2014it is a learning system where text, interaction, and feedback work together to change how someone thinks.\u201d</p> <p>If you\u2019d like, I can turn this into:</p> <ul> <li>A one-page capstone brief for students</li> <li>A grading rubric aligned to learning impact</li> <li>A starter MkDocs + MicroSim template that bakes these constraints in</li> </ul>"},{"location":"learning-graph/","title":"Learning Graph for Automating Instructional Design","text":"<p>This section contains the learning graph for this textbook. A learning graph is a graph of concepts used in this textbook. Each concept is represented by a node in a network graph. Concepts are connected by directed edges that indicate what concepts each node depends on before that concept is understood by the student.</p> <p>A learning graph is the foundational data structure for intelligent textbooks that can recommend learning paths. A learning graph is like a roadmap of concepts to help students arrive at their learning goals.</p> <p>At the left of the learning graph are prerequisite or foundational concepts. They have no outbound edges. They only have inbound edges for other concepts that depend on understanding these foundational prerequisite concepts. At the far right we have the most advanced concepts in the course. To master these concepts you must understand all the concepts that they point to.</p> <p>Here are other files used by the learning graph.</p>"},{"location":"learning-graph/#course-description","title":"Course Description","text":"<p>We use the Course Description as the source document for the concepts that are included in this course. The course description uses the 2001 Bloom taxonomy to order learning objectives.</p>"},{"location":"learning-graph/#list-of-concepts","title":"List of Concepts","text":"<p>We use generative AI to convert the course description into a Concept List. Each concept is in the form of a short Title Case label with most labels under 32 characters long.</p>"},{"location":"learning-graph/#concept-dependency-list","title":"Concept Dependency List","text":"<p>We next use generative AI to create a Directed Acyclic Graph (DAG). DAGs do not have cycles where concepts depend on themselves. We provide the DAG in two formats. One is a CSV file and the other format is a JSON file that uses the vis-network JavaScript library format. The vis-network format uses <code>nodes</code>, <code>edges</code> and <code>metadata</code> elements with edges containing <code>from</code> and <code>to</code> properties. This makes it easy for you to view and edit the learning graph using an editor built with the vis-network tools.</p>"},{"location":"learning-graph/#analysis-documentation","title":"Analysis &amp; Documentation","text":""},{"location":"learning-graph/#course-description-quality-assessment","title":"Course Description Quality Assessment","text":"<p>This report rates the overall quality of the course description for the purpose of generating a learning graph.</p> <ul> <li>Course description fields and content depth analysis</li> <li>Validates course description has sufficient depth for generating 200 concepts</li> <li>Compares course description against similar courses</li> <li>Identifies content gaps and strengths</li> <li>Suggests areas of improvement</li> </ul> <p>View the Course Description Quality Assessment</p>"},{"location":"learning-graph/#learning-graph-quality-validation","title":"Learning Graph Quality Validation","text":"<p>This report gives you an overall assessment of the quality of the learning graph. It uses graph algorithms to look for specific quality patterns in the graph.</p> <ul> <li>Graph structure validation - all concepts are connected</li> <li>DAG validation (no cycles detected)</li> <li>Foundational concepts: 7 entry points</li> <li>Indegree distribution analysis</li> <li>Longest dependency chains (12 concepts)</li> <li>Connectivity: 100% of nodes connected to the main cluster</li> </ul> <p>View the Learning Graph Quality Validation</p>"},{"location":"learning-graph/#concept-taxonomy","title":"Concept Taxonomy","text":"<p>In order to see patterns in the learning graph, it is useful to assign colors to each concept based on the concept type. We use generative AI to create about a dozen categories for our concepts and then place each concept into a single primary classifier.</p> <ul> <li>12 concept categories covering instructional design topics</li> <li>Category organization - foundational elements first, capstone projects last</li> <li>Balanced categories for visualization</li> <li>Clear 3-5 letter abbreviations for use in CSV file</li> </ul> <p>View the Concept Taxonomy</p>"},{"location":"learning-graph/#taxonomy-distribution","title":"Taxonomy Distribution","text":"<p>This report shows how many concepts fit into each category of the taxonomy. Our goal is a somewhat balanced taxonomy where each category holds a reasonable number of concepts. We also don't want any category to contain over 30% of our concepts.</p> <ul> <li>Statistical breakdown by category</li> <li>Detailed concept listing by category</li> <li>Visual distribution table</li> <li>Balance verification</li> </ul> <p>View the Taxonomy Distribution Report</p>"},{"location":"learning-graph/book-metrics/","title":"Book Metrics","text":"<p>Generated by: Book Metrics Python Program v0.05 Generated on: December 18, 2025 at 08:11 PM</p> <p>This file contains overall metrics for the intelligent textbook.</p> <p>Note: Student-facing content metrics exclude <code>prompts/</code> and <code>learning-graph/</code> directories. Chapter-only metrics show what students see in the main chapters.</p>"},{"location":"learning-graph/book-metrics/#overall-metrics","title":"Overall Metrics","text":"Metric Name Value Link Notes Chapters 13 Chapters Number of chapter directories Concepts 200 Concept List Concepts from learning graph Glossary Terms 198 Glossary Defined terms FAQs 60 FAQ Frequently asked questions Quiz Questions 120 - Questions across all chapters MicroSims 2 - Interactive MicroSims"},{"location":"learning-graph/book-metrics/#student-facing-content-metrics","title":"Student-Facing Content Metrics","text":"<p>Excludes administrative directories (<code>prompts/</code>, <code>learning-graph/</code>).</p> Metric Name All Content Chapters Only Notes Diagrams 90 90 H4 headers starting with '#### Diagram:' Equations 9 9 LaTeX expressions (inline and display) Total Words 106,703 84,286 Words in markdown files Links 215 162 Hyperlinks in markdown format Equivalent Pages 450 360 Estimated pages (250 words/page + visuals)"},{"location":"learning-graph/book-metrics/#metrics-explanation","title":"Metrics Explanation","text":""},{"location":"learning-graph/book-metrics/#structural-metrics","title":"Structural Metrics","text":"<ul> <li>Chapters: Count of chapter directories containing index.md files</li> <li>Concepts: Number of rows in learning-graph.csv</li> <li>Glossary Terms: H4 headers in glossary.md</li> <li>FAQs: H3 headers in faq.md</li> <li>Quiz Questions: H4 headers with numbered questions (e.g., '#### 1.') or H2 headers in quiz.md files</li> <li>MicroSims: Directories in docs/sims/ with index.md files</li> </ul>"},{"location":"learning-graph/book-metrics/#content-metrics","title":"Content Metrics","text":"<ul> <li>Diagrams: H4 headers starting with '#### Diagram:'</li> <li>Equations: LaTeX expressions using $ and $$ delimiters</li> <li>Total Words: All words in markdown files (excluding code blocks and URLs)</li> <li>Links: Markdown-formatted links <code>[text](url)</code></li> <li>Equivalent Pages: Based on 250 words/page + 0.25 page/diagram + 0.5 page/MicroSim</li> </ul>"},{"location":"learning-graph/book-metrics/#column-explanations","title":"Column Explanations","text":"<ul> <li>All Content: Includes all student-facing content (chapters, glossary, FAQ, sims, etc.) but excludes administrative directories</li> <li>Chapters Only: Aggregated from chapter directories only - represents the core textbook content students read</li> </ul> <p>Excluded Directories: <code>prompts/</code>, <code>learning-graph/</code> (administrative content not visible to students)</p>"},{"location":"learning-graph/chapter-metrics/","title":"Chapter Metrics","text":"<p>Generated by: Book Metrics Python Program v0.05 Generated on: December 18, 2025 at 08:11 PM</p> <p>This file contains chapter-by-chapter metrics for student-facing content.</p> Chapter Name Sections Diagrams Equations Words Links 1 Foundations of Learning Objective Analysis 23 6 0 6,603 11 2 Prerequisite Analysis and MicroSim Fundamentals 25 7 0 6,441 13 3 The MicroSim Pattern Library 40 11 1 7,748 12 4 Visualization Libraries and Tools 42 6 2 5,322 18 5 Writing Effective MicroSim Specifications 43 10 0 10,258 12 6 Adapting for Audience Levels 31 10 2 6,988 16 7 Cognitive Load and Visual Design 42 10 0 8,509 12 8 Anticipating Misconceptions 41 7 0 7,681 11 9 Generating MicroSims with AI Tools 99 6 0 4,768 12 10 Quality Evaluation Frameworks 56 8 0 6,058 15 11 User Testing and Iteration 78 5 0 6,704 15 12 Accessibility, Deployment, and Course Completion 75 4 4 6,452 15 13 Capstone Projects 12 0 0 754 0"},{"location":"learning-graph/chapter-metrics/#metrics-explanation","title":"Metrics Explanation","text":"<ul> <li>Chapter: Chapter number (leading zeros removed)</li> <li>Name: Chapter title from index.md</li> <li>Sections: Count of H2 and H3 headers in chapter markdown files</li> <li>Diagrams: Count of H4 headers starting with '#### Diagram:'</li> <li>Equations: LaTeX expressions using $ and $$ delimiters</li> <li>Words: Word count across all markdown files in the chapter</li> <li>Links: Markdown-formatted links <code>[text](url)</code></li> </ul>"},{"location":"learning-graph/concept-list/","title":"Concept List","text":"<p>This document contains the 200 concepts for the Automating Instructional Design course. Each concept is numbered with a unique ConceptID for use in the learning graph.</p>"},{"location":"learning-graph/concept-list/#concepts-1-200","title":"Concepts (1-200)","text":"<ol> <li>Instructional Design</li> <li>Learning Objective</li> <li>MicroSim</li> <li>Interactive Simulation</li> <li>Educational Technology</li> <li>AI-Assisted Design</li> <li>Bloom's Taxonomy</li> <li>Cognitive Complexity</li> <li>Remember Level</li> <li>Understand Level</li> <li>Apply Level</li> <li>Analyze Level</li> <li>Evaluate Level</li> <li>Create Level</li> <li>Action Verbs</li> <li>Measurable Outcomes</li> <li>Learning Outcome</li> <li>Objective Decomposition</li> <li>Atomic Concepts</li> <li>Compound Objectives</li> <li>Prerequisite Knowledge</li> <li>Assumed Knowledge</li> <li>Simulation Readiness</li> <li>Concept Dependencies</li> <li>Learning Pathway</li> <li>Visualization Paradigm</li> <li>p5.js Animation</li> <li>Network Graph</li> <li>vis-network Library</li> <li>Timeline Visualization</li> <li>vis-timeline Library</li> <li>Chart Visualization</li> <li>Chart.js Library</li> <li>Plotly Library</li> <li>Map Visualization</li> <li>Leaflet Library</li> <li>Diagram Visualization</li> <li>Mermaid Library</li> <li>Venn Diagram</li> <li>Set Visualization</li> <li>Motion Simulation</li> <li>Physics Simulation</li> <li>Dynamic Systems</li> <li>Cause-Effect Display</li> <li>Relationship Graph</li> <li>Hierarchy Display</li> <li>Dependency Mapping</li> <li>Influence Diagram</li> <li>Sequence Display</li> <li>Process Timeline</li> <li>Trend Chart</li> <li>Distribution Chart</li> <li>Correlation Display</li> <li>Spatial Visualization</li> <li>Flowchart</li> <li>State Machine Diagram</li> <li>Classification Display</li> <li>Paradigm Selection</li> <li>Concept Characteristics</li> <li>Visual Affordances</li> <li>Specification Document</li> <li>Visual Description</li> <li>Interaction Behavior</li> <li>Behavior Constraints</li> <li>Success Criteria</li> <li>Edge Case Definition</li> <li>Specification Ambiguity</li> <li>Intent Preservation</li> <li>AI Interpretation</li> <li>Prompt Engineering</li> <li>Refinement Prompt</li> <li>Generation Workflow</li> <li>Output Interpretation</li> <li>Issue Identification</li> <li>Regeneration Decision</li> <li>Manual Adjustment</li> <li>Version Control</li> <li>Iteration Management</li> <li>Cognitive Development</li> <li>Piaget Stages</li> <li>Vygotsky Theory</li> <li>Early Childhood Design</li> <li>Elementary Design</li> <li>Middle School Design</li> <li>High School Design</li> <li>Undergraduate Design</li> <li>Graduate Design</li> <li>Corporate Training Design</li> <li>Touch Target Size</li> <li>Simple Cause-Effect</li> <li>Guided Exploration</li> <li>Scaffolded Complexity</li> <li>Reading Support</li> <li>Abstract Concepts</li> <li>Multiple Variables</li> <li>Hypothesis Testing</li> <li>Real-World Application</li> <li>Data Interpretation</li> <li>Theoretical Foundations</li> <li>Mathematical Relations</li> <li>Research Applications</li> <li>Parameter Space</li> <li>Job-Relevant Scenarios</li> <li>Time-Efficient Design</li> <li>Cognitive Load Theory</li> <li>Intrinsic Load</li> <li>Extraneous Load</li> <li>Germane Load</li> <li>Split Attention Effect</li> <li>Progressive Disclosure</li> <li>Color Accessibility</li> <li>Contrast Design</li> <li>Animation Speed</li> <li>Learner Control</li> <li>Visual Simplicity</li> <li>Information Density</li> <li>Cognitive Load Meter</li> <li>Design Tradeoffs</li> <li>Mental Effort</li> <li>Working Memory</li> <li>Long-Term Memory</li> <li>Schema Formation</li> <li>Misconception</li> <li>Common Misconceptions</li> <li>Misconception Correction</li> <li>Misconception Reinforcement</li> <li>Productive Failure</li> <li>Prediction Prompt</li> <li>Conceptual Boundary</li> <li>Mental Model</li> <li>Conceptual Change</li> <li>Intuition Testing</li> <li>Model Comparison</li> <li>Claude Code Skills</li> <li>MicroSim Generator</li> <li>Template Library</li> <li>Iterative Refinement</li> <li>Conversation Prompting</li> <li>Code Generation</li> <li>Output Validation</li> <li>Technical Evaluation</li> <li>Pedagogical Evaluation</li> <li>UX Evaluation</li> <li>Functionality Testing</li> <li>Responsiveness Testing</li> <li>Bug Identification</li> <li>Objective Alignment</li> <li>Cognitive Level Match</li> <li>Effectiveness Measure</li> <li>Intuitiveness</li> <li>Engagement Balance</li> <li>Evaluation Rubric</li> <li>Rubric Development</li> <li>Automated Evaluation</li> <li>Human Evaluation</li> <li>Documentation Standard</li> <li>Reusability</li> <li>Think-Aloud Protocol</li> <li>A/B Testing</li> <li>Learner Feedback</li> <li>Age-Based Feedback</li> <li>Ability-Based Feedback</li> <li>Observation Technique</li> <li>Test Interpretation</li> <li>Change Prioritization</li> <li>Ethical Research</li> <li>Design-Test-Refine Cycle</li> <li>Critical Changes</li> <li>Nice-to-Have Changes</li> <li>Fundamental Redesign</li> <li>Incremental Improvement</li> <li>Scope Creep Prevention</li> <li>Completion Criteria</li> <li>Change Log</li> <li>Design Rationale</li> <li>Universal Design</li> <li>UDL Principles</li> <li>Screen Reader Support</li> <li>Keyboard Navigation</li> <li>Color Blindness Design</li> <li>Reduced Motion</li> <li>Multilingual Support</li> <li>Vocabulary Level</li> <li>Cultural Sensitivity</li> <li>Prior Knowledge Support</li> <li>Differentiation Strategy</li> <li>Accessibility Audit</li> <li>Constraint Simulation</li> <li>LMS Integration</li> <li>Intelligent Textbook</li> <li>Learning Analytics</li> <li>Interaction Tracking</li> <li>Maintenance Planning</li> <li>Library Organization</li> <li>Educator Collaboration</li> <li>Content Sharing</li> <li>Portfolio Project</li> <li>Reflection Journal</li> <li>Peer Feedback</li> <li>Self-Evaluation</li> </ol>"},{"location":"learning-graph/concept-list/#review-instructions","title":"Review Instructions","text":"<p>Please review this concept list carefully before proceeding to the next step:</p> <ol> <li>Check for missing concepts - Are there important topics from the course that aren't represented?</li> <li>Check for duplicates - Are any concepts essentially the same thing with different names?</li> <li>Check label length - All labels should be 32 characters or fewer</li> <li>Check clarity - Are all concept labels clear and unambiguous?</li> <li>Check relevance - Do all concepts belong in this course?</li> </ol> <p>It is much easier to add or remove concepts now than after the dependency graph is created.</p>"},{"location":"learning-graph/concept-taxonomy/","title":"Concept Taxonomy","text":"<p>This document defines the categorical taxonomy for organizing the 200 concepts in the Automating Instructional Design course learning graph.</p>"},{"location":"learning-graph/concept-taxonomy/#taxonomy-categories","title":"Taxonomy Categories","text":"TaxonomyID Category Name Description FOUND Foundation Concepts Core instructional design principles, prerequisites, and fundamental theories BLOOM Bloom's Taxonomy Cognitive complexity levels and learning objective classification VISUA Visualization Types Different paradigms for visual representation in MicroSims LIBRA Libraries &amp; Tools Specific JavaScript libraries and development tools SPECI Specification Writing and documenting MicroSim requirements COGNI Cognitive Science Memory, cognitive load, mental models, and misconceptions AUDIE Audience Adaptation Designing for different age groups and contexts EVALU Evaluation &amp; Testing Quality assessment, rubrics, and user testing methods ITERA Iteration &amp; Workflow Design cycles, version control, and refinement processes ACCES Accessibility Universal design, UDL principles, and inclusive design DEPLO Deployment LMS integration, analytics, maintenance, and sharing CAPST Capstone Portfolio projects and summative assessments"},{"location":"learning-graph/concept-taxonomy/#category-definitions","title":"Category Definitions","text":""},{"location":"learning-graph/concept-taxonomy/#found-foundation-concepts","title":"FOUND - Foundation Concepts","text":"<p>Core concepts that form the foundation of instructional design and educational technology. These are often prerequisites for more specialized topics.</p> <p>Includes: Instructional Design, Learning Objective, Educational Technology, MicroSim, Interactive Simulation</p>"},{"location":"learning-graph/concept-taxonomy/#bloom-blooms-taxonomy","title":"BLOOM - Bloom's Taxonomy","text":"<p>Concepts related to the cognitive complexity framework, including the six taxonomy levels and their application to learning objective design.</p> <p>Includes: Bloom's Taxonomy, Cognitive Complexity, Remember/Understand/Apply/Analyze/Evaluate/Create Levels, Action Verbs, Measurable Outcomes</p>"},{"location":"learning-graph/concept-taxonomy/#visua-visualization-types","title":"VISUA - Visualization Types","text":"<p>Different paradigms for visual representation, including motion, networks, timelines, charts, maps, and diagrams.</p> <p>Includes: Visualization Paradigm, Motion Simulation, Network Graph, Timeline Visualization, Chart Visualization, Map Visualization, Diagram Visualization, Venn Diagram</p>"},{"location":"learning-graph/concept-taxonomy/#libra-libraries-tools","title":"LIBRA - Libraries &amp; Tools","text":"<p>Specific JavaScript libraries and tools used for MicroSim development.</p> <p>Includes: p5.js Animation, vis-network Library, vis-timeline Library, Chart.js Library, Plotly Library, Leaflet Library, Mermaid Library, Claude Code Skills, MicroSim Generator</p>"},{"location":"learning-graph/concept-taxonomy/#speci-specification","title":"SPECI - Specification","text":"<p>Writing effective MicroSim specifications, documenting requirements, and preserving pedagogical intent.</p> <p>Includes: Specification Document, Visual Description, Interaction Behavior, Success Criteria, Edge Case Definition, Specification Ambiguity, Intent Preservation</p>"},{"location":"learning-graph/concept-taxonomy/#cogni-cognitive-science","title":"COGNI - Cognitive Science","text":"<p>Memory systems, cognitive load theory, mental models, and misconception handling.</p> <p>Includes: Working Memory, Long-Term Memory, Cognitive Load Theory, Intrinsic/Extraneous/Germane Load, Mental Model, Misconception, Schema Formation</p>"},{"location":"learning-graph/concept-taxonomy/#audie-audience-adaptation","title":"AUDIE - Audience Adaptation","text":"<p>Designing MicroSims for different developmental stages and learning contexts.</p> <p>Includes: Cognitive Development, Piaget Stages, Vygotsky Theory, Early Childhood/Elementary/Middle School/High School/Undergraduate/Graduate/Corporate Training Design</p>"},{"location":"learning-graph/concept-taxonomy/#evalu-evaluation-testing","title":"EVALU - Evaluation &amp; Testing","text":"<p>Quality assessment frameworks, evaluation rubrics, and user testing methodologies.</p> <p>Includes: Technical Evaluation, Pedagogical Evaluation, UX Evaluation, Evaluation Rubric, Think-Aloud Protocol, A/B Testing, Learner Feedback</p>"},{"location":"learning-graph/concept-taxonomy/#itera-iteration-workflow","title":"ITERA - Iteration &amp; Workflow","text":"<p>The design-test-refine cycle, version control, and systematic improvement processes.</p> <p>Includes: Design-Test-Refine Cycle, Iteration Management, Version Control, Change Prioritization, Prompt Engineering, Regeneration Decision</p>"},{"location":"learning-graph/concept-taxonomy/#acces-accessibility","title":"ACCES - Accessibility","text":"<p>Universal design principles and inclusive design practices.</p> <p>Includes: Universal Design, UDL Principles, Screen Reader Support, Keyboard Navigation, Color Blindness Design, Reduced Motion, Multilingual Support</p>"},{"location":"learning-graph/concept-taxonomy/#deplo-deployment","title":"DEPLO - Deployment","text":"<p>Integration with learning management systems, analytics, and long-term maintenance.</p> <p>Includes: LMS Integration, Intelligent Textbook, Learning Analytics, Interaction Tracking, Maintenance Planning, Library Organization, Content Sharing</p>"},{"location":"learning-graph/concept-taxonomy/#capst-capstone","title":"CAPST - Capstone","text":"<p>Portfolio projects and summative assessment concepts.</p> <p>Includes: Portfolio Project, Reflection Journal, Peer Feedback, Self-Evaluation</p>"},{"location":"learning-graph/concept-taxonomy/#distribution-goals","title":"Distribution Goals","text":"<ul> <li>Target: ~12-25 concepts per category</li> <li>No single category should exceed 30% of total concepts</li> <li>MISC category should be minimal (&lt;2%)</li> </ul>"},{"location":"learning-graph/course-description-assessment/","title":"Course Description Quality Assessment","text":"<p>Assessment Date: 2025-12-17 Assessed Document: course-description.md</p>"},{"location":"learning-graph/course-description-assessment/#quality-scoring-summary","title":"Quality Scoring Summary","text":"Element Points Max Notes Title 5 5 Clear, descriptive: \"Automating Instructional Design: From Learning Objectives to Interactive MicroSimulations\" Target Audience 5 5 Specific audiences identified: K-12 teachers, corporate trainers, higher ed faculty, instructional designers, curriculum developers, SMEs Prerequisites 5 5 Clearly stated: \"Basic computer literacy; no programming experience required\" Main Topics Covered 10 10 Comprehensive 12-module structure with detailed topics for each Topics Excluded 5 5 Extensive \"Concepts NOT Covered\" section with 8 categories Learning Outcomes Header 5 5 Clear \"Upon completing this course, participants will be able to:\" statement Remember Level 10 10 6 specific outcomes (List, Identify, Recall, Name, Define, Recognize) Understand Level 10 10 8 specific outcomes (Explain, Describe, Summarize, Interpret, Classify, Compare, Exemplify, Paraphrase) Apply Level 10 10 7 specific outcomes (Implement, Execute, Use, Apply, Demonstrate, Employ, Carry out) Analyze Level 10 10 8 specific outcomes (Differentiate, Organize, Attribute, Deconstruct, Distinguish, Examine, Compare, Outline) Evaluate Level 10 10 8 specific outcomes (Assess, Critique, Judge, Prioritize, Justify, Appraise, Defend, Recommend) Create Level 10 10 10 specific outcomes including portfolio capstone (Design, Construct, Develop, Compose, Produce, Generate, Formulate, Assemble, Synthesize, Author) Descriptive Context 5 5 Extensive context including \"Why This Is Hard\" section, automation opportunity, assessment methods"},{"location":"learning-graph/course-description-assessment/#total-quality-score-100100","title":"Total Quality Score: 100/100","text":""},{"location":"learning-graph/course-description-assessment/#detailed-assessment","title":"Detailed Assessment","text":""},{"location":"learning-graph/course-description-assessment/#strengths","title":"Strengths","text":"<ol> <li> <p>Exceptional Bloom's Taxonomy Coverage: The course description includes a dedicated section organizing all learning objectives by the 2001 Bloom's Taxonomy revision. Each level has 6-10 specific, actionable outcomes using appropriate action verbs.</p> </li> <li> <p>Comprehensive Module Structure: 12 well-defined modules covering the complete MicroSim development lifecycle from objective analysis through deployment.</p> </li> <li> <p>Clear Audience Definition: Six distinct target audiences identified with specific use cases for each.</p> </li> <li> <p>Excellent Scope Boundaries: The \"Concepts NOT Covered\" section clearly delineates 8 categories of excluded topics, preventing scope creep and setting clear expectations.</p> </li> <li> <p>Rich Topic Detail: Each module includes:</p> </li> <li>Specific topics covered</li> <li>Hands-on activities</li> <li> <p>Meta-level MicroSims that teach the course content</p> </li> <li> <p>Assessment Framework: Clear formative and summative assessment methods with weighted portfolio criteria.</p> </li> <li> <p>Concepts Inventory: Detailed \"Concepts Covered\" section with 11 categories provides excellent material for learning graph generation.</p> </li> </ol>"},{"location":"learning-graph/course-description-assessment/#areas-for-potential-enhancement","title":"Areas for Potential Enhancement","text":"<ol> <li> <p>Time Estimates Per Module: While total course time is provided (60-80 hours), individual module durations could help with pacing.</p> </li> <li> <p>Prerequisite Concepts: Could benefit from listing specific foundational concepts students should understand before starting.</p> </li> <li> <p>Industry Standards: Could reference instructional design standards (ADDIE, SAM) for context.</p> </li> </ol>"},{"location":"learning-graph/course-description-assessment/#estimated-concept-generation-capacity","title":"Estimated Concept Generation Capacity","text":"<p>Based on the course description analysis:</p> <ul> <li>Instructional Design Foundations: ~25 concepts</li> <li>Visualization Types &amp; Paradigms: ~30 concepts</li> <li>Cognitive Science Principles: ~25 concepts</li> <li>Specification Writing: ~20 concepts</li> <li>Audience Adaptation: ~25 concepts</li> <li>Quality Evaluation: ~20 concepts</li> <li>User Testing: ~15 concepts</li> <li>Iteration &amp; Refinement: ~15 concepts</li> <li>Accessibility: ~15 concepts</li> <li>AI-Assisted Development: ~15 concepts</li> <li>Deployment &amp; Maintenance: ~15 concepts</li> </ul> <p>Total Estimated Concepts: ~220</p> <p>This exceeds the target of 200 concepts, indicating sufficient depth and breadth for a comprehensive learning graph.</p>"},{"location":"learning-graph/course-description-assessment/#comparison-with-similar-courses","title":"Comparison with Similar Courses","text":"Course Type Typical Concepts This Course Basic Instructional Design 80-120 220 Educational Technology 100-150 220 E-Learning Development 120-180 220 Advanced ID with AI 150-200 220 <p>This course description is in the top tier for concept density and pedagogical completeness.</p>"},{"location":"learning-graph/course-description-assessment/#recommendation","title":"Recommendation","text":"<p>PROCEED with learning graph generation. This course description is exemplary and provides more than sufficient material for generating 200 high-quality concepts with meaningful dependencies.</p> <p>The Bloom's Taxonomy section is particularly valuable as it pre-classifies learning outcomes by cognitive level, which will inform the dependency structure of the learning graph.</p>"},{"location":"learning-graph/diagram-details/","title":"Diagram and MicroSim Details","text":"<p>Total Visual Elements: 90 Diagrams: 13 MicroSims: 68</p>"},{"location":"learning-graph/diagram-details/#chapter-1-foundations-learning-objective-analysis","title":"Chapter 1: Foundations Learning Objective Analysis","text":"<p>Total elements: 6</p>"},{"location":"learning-graph/diagram-details/#blooms-taxonomy-action-verb-wheel","title":"Bloom's Taxonomy Action Verb Wheel","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 6</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#blooms-taxonomy-pyramid","title":"Bloom's Taxonomy Pyramid","text":"<ul> <li>Type: Unknown</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#concept-map-of-chapter-1-foundations","title":"Concept Map of Chapter 1 Foundations","text":"<ul> <li>Type: Unknown</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#educational-technology-ecosystem","title":"Educational Technology Ecosystem","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#objective-decomposition-tree","title":"Objective Decomposition Tree","text":"<ul> <li>Type: Unknown</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#smart-learning-objectives-framework","title":"SMART Learning Objectives Framework","text":"<ul> <li>Type: Unknown</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 5</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#chapter-2-prerequisite-analysis-microsim-fundamentals","title":"Chapter 2: Prerequisite Analysis Microsim Fundamentals","text":"<p>Total elements: 7</p>"},{"location":"learning-graph/diagram-details/#concept-dependency-graph-example","title":"Concept Dependency Graph Example","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#human-ai-collaboration-loop","title":"Human-AI Collaboration Loop","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#learning-pathway-visualization","title":"Learning Pathway Visualization","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 10</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#microsim-types-by-blooms-level","title":"MicroSim Types by Bloom's Level","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#prerequisite-vs-assumed-knowledge","title":"Prerequisite vs Assumed Knowledge","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#simulation-readiness-assessment","title":"Simulation Readiness Assessment","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 2</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#traditional-simulation-vs-microsim-architecture","title":"Traditional Simulation vs MicroSim Architecture","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#chapter-3-microsim-pattern-library","title":"Chapter 3: Microsim Pattern Library","text":"<p>Total elements: 11</p>"},{"location":"learning-graph/diagram-details/#cause-effect-display-template","title":"Cause-Effect Display Template","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 20</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#chart-type-selection-guide","title":"Chart Type Selection Guide","text":"<ul> <li>Type: Unknown</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 2</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#classification-matrix-display","title":"Classification Matrix Display","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 2</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#interactive-process-timeline","title":"Interactive Process Timeline","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 2</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#paradigm-affordance-mapping","title":"Paradigm-Affordance Mapping","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 3</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#physics-simulation-architecture","title":"Physics Simulation Architecture","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 9</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#relationship-graph-types","title":"Relationship Graph Types","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#spatial-visualization-types","title":"Spatial Visualization Types","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#state-machine-template","title":"State Machine Template","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#venn-diagram-builder","title":"Venn Diagram Builder","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 8</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#visualization-paradigm-selection-guide","title":"Visualization Paradigm Selection Guide","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 4</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#chapter-4-visualization-libraries-tools","title":"Chapter 4: Visualization Libraries Tools","text":"<p>Total elements: 6</p>"},{"location":"learning-graph/diagram-details/#chart-type-selection-guide_1","title":"Chart Type Selection Guide","text":"<ul> <li>Type: Unknown</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#learning-dependency-network-example","title":"Learning Dependency Network Example","text":"<ul> <li>Type: Unknown</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#microsim-generation-workflow","title":"MicroSim Generation Workflow","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#specification-quality-checklist","title":"Specification Quality Checklist","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 2</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#visualization-library-decision-tree","title":"Visualization Library Decision Tree","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#p5js-microsim-architecture","title":"p5.js MicroSim Architecture","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 6</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#chapter-5-writing-microsim-specifications","title":"Chapter 5: Writing Microsim Specifications","text":"<p>Total elements: 10</p>"},{"location":"learning-graph/diagram-details/#ai-specification-interpretation-microsim","title":"AI Specification Interpretation MicroSim","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 16</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#acceptance-test-coverage-matrix","title":"Acceptance Test Coverage Matrix","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 4</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#edge-case-discovery-microsim","title":"Edge Case Discovery MicroSim","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 10</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#intent-preservation-matrix","title":"Intent Preservation Matrix","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 5</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#interaction-specification-template","title":"Interaction Specification Template","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#ohms-law-circuit-simulator","title":"Ohm's Law Circuit Simulator","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 42</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#probability-tree-explorer","title":"Probability Tree Explorer","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 21</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#sorting-algorithm-racing-microsim","title":"Sorting Algorithm Racing MicroSim","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 10</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#success-criteria-flow","title":"Success Criteria Flow","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#visual-description-completeness-checklist","title":"Visual Description Completeness Checklist","text":"<ul> <li>Type: Unknown</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#chapter-6-adapting-audience-levels","title":"Chapter 6: Adapting Audience Levels","text":"<p>Total elements: 10</p>"},{"location":"learning-graph/diagram-details/#cognitive-development-progression","title":"Cognitive Development Progression","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#corporate-learning-module-pattern","title":"Corporate Learning Module Pattern","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#early-childhood-microsim-pattern","title":"Early Childhood MicroSim Pattern","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#elementary-scaffolding-progression","title":"Elementary Scaffolding Progression","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 12</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#hypothesis-testing-interface","title":"Hypothesis Testing Interface","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 10</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#mathematical-relationship-explorer","title":"Mathematical Relationship Explorer","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 9</li> <li>Difficulty: Very Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#parameter-space-explorer","title":"Parameter Space Explorer","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 6</li> <li>Difficulty: Very Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#probability-across-audience-levels","title":"Probability Across Audience Levels","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 4</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#real-world-data-microsim","title":"Real-World Data MicroSim","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 9</li> <li>Difficulty: Very Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#zone-of-proximal-development","title":"Zone of Proximal Development","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 3</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#chapter-7-cognitive-load-visual-design","title":"Chapter 7: Cognitive Load Visual Design","text":"<p>Total elements: 10</p>"},{"location":"learning-graph/diagram-details/#animation-control-best-practices","title":"Animation Control Best Practices","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 24</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#cognitive-load-meter-design","title":"Cognitive Load Meter Design","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 15</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#cognitive-load-microsim","title":"Cognitive Load MicroSim","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 14</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#design-tradeoff-decision-tree","title":"Design Tradeoff Decision Tree","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 2</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#information-density-spectrum","title":"Information Density Spectrum","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 2</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#progressive-disclosure-in-action","title":"Progressive Disclosure in Action","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 8</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#schema-formation-process","title":"Schema Formation Process","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#split-attention-effect-comparison","title":"Split Attention Effect Comparison","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 5</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#the-three-types-of-cognitive-load","title":"The Three Types of Cognitive Load","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#working-memory-architecture","title":"Working Memory Architecture","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#chapter-8-anticipating-misconceptions","title":"Chapter 8: Anticipating Misconceptions","text":"<p>Total elements: 7</p>"},{"location":"learning-graph/diagram-details/#intuition-testing-microsim","title":"Intuition Testing MicroSim","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 2</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#mental-model-formation-and-influence","title":"Mental Model Formation and Influence","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 3</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#misconception-catalog-by-domain","title":"Misconception Catalog by Domain","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#misconception-correction-cycle","title":"Misconception Correction Cycle","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#model-boundaries-visualization","title":"Model Boundaries Visualization","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 8</li> <li>Difficulty: Very Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#model-comparison-interactive-tool","title":"Model Comparison Interactive Tool","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 3</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#prediction-prompt-interface-design","title":"Prediction Prompt Interface Design","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 10</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#chapter-9-generating-microsims-ai-tools","title":"Chapter 9: Generating Microsims Ai Tools","text":"<p>Total elements: 6</p>"},{"location":"learning-graph/diagram-details/#claude-code-skills-architecture","title":"Claude Code Skills Architecture","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#issue-identification-workflow","title":"Issue Identification Workflow","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 4</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#iteration-management-dashboard","title":"Iteration Management Dashboard","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 10</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#microsim-generation-workflow_1","title":"MicroSim Generation Workflow","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#prompt-engineering-best-practices","title":"Prompt Engineering Best Practices","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#rule-hierarchy-cascade","title":"Rule Hierarchy Cascade","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 2</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#chapter-10-quality-evaluation-frameworks","title":"Chapter 10: Quality Evaluation Frameworks","text":"<p>Total elements: 8</p>"},{"location":"learning-graph/diagram-details/#automated-vs-human-evaluation-matrix","title":"Automated vs. Human Evaluation Matrix","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#blooms-taxonomy-and-microsim-types","title":"Bloom's Taxonomy and MicroSim Types","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#engagement-vs-learning-trade-off","title":"Engagement vs. Learning Trade-off","text":"<ul> <li>Type: Diagram</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#evaluation-rubric-builder","title":"Evaluation Rubric Builder","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 14</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#metadata-enabled-search","title":"Metadata-Enabled Search","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 12</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#responsive-breakpoint-testing","title":"Responsive Breakpoint Testing","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 2</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#standardization-workflow","title":"Standardization Workflow","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#three-lens-evaluation-model","title":"Three-Lens Evaluation Model","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#chapter-11-user-testing-iteration","title":"Chapter 11: User Testing Iteration","text":"<p>Total elements: 5</p>"},{"location":"learning-graph/diagram-details/#common-interpretation-pitfalls","title":"Common Interpretation Pitfalls","text":"<ul> <li>Type: Unknown</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#design-test-refine-cycle","title":"Design-Test-Refine Cycle","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#page-view-vs-simulation-tracking","title":"Page-View vs. Simulation Tracking","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 3</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#think-aloud-protocol-workflow","title":"Think-Aloud Protocol Workflow","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 0</li> <li>Difficulty: Easy</li> </ul>"},{"location":"learning-graph/diagram-details/#xapi-data-flow","title":"xAPI Data Flow","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#chapter-12-accessibility-deployment-completion","title":"Chapter 12: Accessibility Deployment Completion","text":"<p>Total elements: 4</p>"},{"location":"learning-graph/diagram-details/#accessibility-audit-workflow","title":"Accessibility Audit Workflow","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 3</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-details/#accessible-microsim-control-layout","title":"Accessible MicroSim Control Layout","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 11</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#color-blindness-simulation","title":"Color Blindness Simulation","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 9</li> <li>Difficulty: Hard</li> </ul>"},{"location":"learning-graph/diagram-details/#portfolio-project-components","title":"Portfolio Project Components","text":"<ul> <li>Type: Microsim</li> <li>Bloom's Taxonomy: Not specified</li> <li>UI Elements: 1</li> <li>Difficulty: Medium</li> </ul>"},{"location":"learning-graph/diagram-table/","title":"Diagram and MicroSim Table","text":"<p>Total Visual Elements: 90 Diagrams: 13 MicroSims: 68</p>"},{"location":"learning-graph/diagram-table/#summary-by-difficulty","title":"Summary by Difficulty","text":"<ul> <li>Easy: 24</li> <li>Medium: 37</li> <li>Hard: 25</li> <li>Very Hard: 4</li> </ul>"},{"location":"learning-graph/diagram-table/#all-visual-elements","title":"All Visual Elements","text":"Chapter Element Title Status Type Bloom Levels UI Elements Difficulty Recommended MicroSims 1 Bloom's Taxonomy Action Verb Wheel Microsim Not specified 6 Medium 1 Bloom's Taxonomy Pyramid Unknown Not specified 0 Easy 1 Concept Map of Chapter 1 Foundations Unknown Not specified 1 Easy 1 Educational Technology Ecosystem Microsim Not specified 0 Medium 1 Objective Decomposition Tree Unknown Not specified 0 Easy 1 SMART Learning Objectives Framework Unknown Not specified 5 Medium 2 Concept Dependency Graph Example Microsim Not specified 1 Medium 2 Human-AI Collaboration Loop Microsim Not specified 0 Easy 2 Learning Pathway Visualization Microsim Not specified 10 Hard 2 MicroSim Types by Bloom's Level Microsim Not specified 1 Hard 2 Prerequisite vs Assumed Knowledge Microsim Not specified 0 Easy 2 Simulation Readiness Assessment Microsim Not specified 2 Medium 2 Traditional Simulation vs MicroSim Architecture Microsim Not specified 0 Medium 3 Cause-Effect Display Template Microsim Not specified 20 Hard 3 Chart Type Selection Guide Unknown Not specified 2 Easy 3 Classification Matrix Display Microsim Not specified 2 Medium 3 Interactive Process Timeline Microsim Not specified 2 Medium 3 Paradigm-Affordance Mapping Diagram Not specified 3 Easy 3 Physics Simulation Architecture Diagram Not specified 9 Medium 3 Relationship Graph Types Microsim Not specified 0 Medium 3 Spatial Visualization Types Diagram Not specified 0 Easy 3 State Machine Template Microsim Not specified 0 Medium 3 Venn Diagram Builder Microsim Not specified 8 Medium 3 Visualization Paradigm Selection Guide Microsim Not specified 4 Medium 4 Chart Type Selection Guide Unknown Not specified 0 Medium 4 Learning Dependency Network Example Unknown Not specified 0 Easy 4 MicroSim Generation Workflow Microsim Not specified 1 Medium 4 Specification Quality Checklist Microsim Not specified 2 Medium 4 Visualization Library Decision Tree Microsim Not specified 0 Medium 4 p5.js MicroSim Architecture Microsim Not specified 6 Medium 5 AI Specification Interpretation MicroSim Microsim Not specified 16 Hard 5 Acceptance Test Coverage Matrix Microsim Not specified 4 Hard 5 Edge Case Discovery MicroSim Microsim Not specified 10 Hard 5 Intent Preservation Matrix Microsim Not specified 5 Medium 5 Interaction Specification Template Diagram Not specified 1 Easy 5 Ohm's Law Circuit Simulator Microsim Not specified 42 Hard 5 Probability Tree Explorer Microsim Not specified 21 Hard 5 Sorting Algorithm Racing MicroSim Microsim Not specified 10 Hard 5 Success Criteria Flow Microsim Not specified 0 Easy 5 Visual Description Completeness Checklist Unknown Not specified 1 Medium 6 Cognitive Development Progression Diagram Not specified 0 Easy 6 Corporate Learning Module Pattern Microsim Not specified 0 Easy 6 Early Childhood MicroSim Pattern Microsim Not specified 0 Medium 6 Elementary Scaffolding Progression Microsim Not specified 12 Hard 6 Hypothesis Testing Interface Microsim Not specified 10 Hard 6 Mathematical Relationship Explorer Microsim Not specified 9 Very Hard 6 Parameter Space Explorer Microsim Not specified 6 Very Hard 6 Probability Across Audience Levels Microsim Not specified 4 Hard 6 Real-World Data MicroSim Microsim Not specified 9 Very Hard 6 Zone of Proximal Development Microsim Not specified 3 Medium 7 Animation Control Best Practices Microsim Not specified 24 Hard 7 Cognitive Load Meter Design Microsim Not specified 15 Hard 7 Cognitive Load MicroSim Microsim Not specified 14 Hard 7 Design Tradeoff Decision Tree Diagram Not specified 2 Easy 7 Information Density Spectrum Microsim Not specified 2 Medium 7 Progressive Disclosure in Action Microsim Not specified 8 Hard 7 Schema Formation Process Diagram Not specified 0 Medium 7 Split Attention Effect Comparison Diagram Not specified 5 Hard 7 The Three Types of Cognitive Load Microsim Not specified 1 Medium 7 Working Memory Architecture Diagram Not specified 0 Easy 8 Intuition Testing MicroSim Microsim Not specified 2 Medium 8 Mental Model Formation and Influence Diagram Not specified 3 Easy 8 Misconception Catalog by Domain Microsim Not specified 1 Medium 8 Misconception Correction Cycle Microsim Not specified 0 Easy 8 Model Boundaries Visualization Microsim Not specified 8 Very Hard 8 Model Comparison Interactive Tool Microsim Not specified 3 Hard 8 Prediction Prompt Interface Design Microsim Not specified 10 Hard 9 Claude Code Skills Architecture Microsim Not specified 1 Medium 9 Issue Identification Workflow Microsim Not specified 4 Medium 9 Iteration Management Dashboard Microsim Not specified 10 Hard 9 MicroSim Generation Workflow Microsim Not specified 1 Medium 9 Prompt Engineering Best Practices Microsim Not specified 1 Medium 9 Rule Hierarchy Cascade Diagram Not specified 2 Easy 10 Automated vs. Human Evaluation Matrix Diagram Not specified 0 Easy 10 Bloom's Taxonomy and MicroSim Types Microsim Not specified 1 Medium 10 Engagement vs. Learning Trade-off Diagram Not specified 0 Easy 10 Evaluation Rubric Builder Microsim Not specified 14 Hard 10 Metadata-Enabled Search Microsim Not specified 12 Hard 10 Responsive Breakpoint Testing Microsim Not specified 2 Medium 10 Standardization Workflow Microsim Not specified 0 Easy 10 Three-Lens Evaluation Model Microsim Not specified 0 Easy 11 Common Interpretation Pitfalls Unknown Not specified 0 Easy 11 Design-Test-Refine Cycle Microsim Not specified 0 Medium 11 Page-View vs. Simulation Tracking Microsim Not specified 3 Hard 11 Think-Aloud Protocol Workflow Microsim Not specified 0 Easy 11 xAPI Data Flow Microsim Not specified 1 Medium 12 Accessibility Audit Workflow Microsim Not specified 3 Medium 12 Accessible MicroSim Control Layout Microsim Not specified 11 Hard 12 Color Blindness Simulation Microsim Not specified 9 Hard 12 Portfolio Project Components Microsim Not specified 1 Medium"},{"location":"learning-graph/faq-quality-report/","title":"FAQ Quality Report","text":"<p>Generated: 2025-12-18 Source: Automating Instructional Design Textbook</p>"},{"location":"learning-graph/faq-quality-report/#overall-statistics","title":"Overall Statistics","text":"Metric Value Total Questions 67 Overall Quality Score 91/100 Content Completeness Score 100/100 Concept Coverage 85% (170/200 concepts)"},{"location":"learning-graph/faq-quality-report/#category-breakdown","title":"Category Breakdown","text":"Category Questions Avg Bloom's Level Avg Word Count Getting Started 12 Remember/Understand 72 Core Concepts 16 Understand/Apply 68 Technical Details 12 Remember/Apply 54 Common Challenges 9 Apply/Analyze 57 Best Practices 10 Apply/Evaluate 59 Advanced Topics 8 Apply/Create 54"},{"location":"learning-graph/faq-quality-report/#blooms-taxonomy-distribution","title":"Bloom's Taxonomy Distribution","text":""},{"location":"learning-graph/faq-quality-report/#actual-vs-target-distribution","title":"Actual vs Target Distribution","text":"Level Actual Target Deviation Status Remember 18% (12) 20% -2% Pass Understand 33% (22) 30% +3% Pass Apply 27% (18) 25% +2% Pass Analyze 13% (9) 15% -2% Pass Evaluate 6% (4) 7% -1% Pass Create 3% (2) 3% 0% Pass <p>Total Deviation: 10% (within \u00b115% threshold)</p> <p>Bloom's Distribution Score: 24/25 (excellent)</p>"},{"location":"learning-graph/faq-quality-report/#answer-quality-analysis","title":"Answer Quality Analysis","text":"Metric Count Percentage Target Status Answers with Examples 31 46% 40%+ Pass Answers with Source Links 67 100% 60%+ Pass Complete Answers 67 100% 100% Pass Appropriate Length (100-300 words) 65 97% 95%+ Pass <p>Average Answer Length: 58 words (concise, appropriate)</p> <p>Answer Quality Score: 24/25</p>"},{"location":"learning-graph/faq-quality-report/#concept-coverage-analysis","title":"Concept Coverage Analysis","text":""},{"location":"learning-graph/faq-quality-report/#covered-concepts-by-category","title":"Covered Concepts by Category","text":"Category Concepts Covered Coverage Foundation Concepts (FOUND) 11 11 100% Bloom's Taxonomy (BLOOM) 14 14 100% Visualization Types (VISUA) 35 28 80% Libraries &amp; Tools (LIBRA) 12 12 100% Specification (SPECI) 9 9 100% Cognitive Science (COGNI) 29 25 86% Audience Adaptation (AUDIE) 26 22 85% Evaluation &amp; Testing (EVALU) 27 24 89% Iteration &amp; Workflow (ITERA) 19 16 84% Accessibility (ACCES) 14 12 86% Deployment (DEPLO) 8 7 88% Capstone (CAPST) 4 4 100%"},{"location":"learning-graph/faq-quality-report/#concepts-not-covered-30-concepts","title":"Concepts Not Covered (30 concepts)","text":"<p>Priority: High (8 concepts) - Intuition Testing - Cognitive Load Meter - Mental Effort - Model Comparison - Schema Formation - Conceptual Change - Conceptual Boundary - Productive Failure</p> <p>Priority: Medium (14 concepts) - Correlation Display - Distribution Chart - Trend Chart - Process Timeline - Sequence Display - Influence Diagram - Dependency Mapping - Hierarchy Display - Relationship Graph - Dynamic Systems - Physics Simulation - Motion Simulation - Set Visualization - Classification Display</p> <p>Priority: Low (8 concepts) - State Machine Diagram - Flowchart - Spatial Visualization - Cause-Effect Display - Visual Affordances - Information Density - Mental Model - Long-Term Memory</p> <p>Coverage Score: 22/30 (85% coverage)</p>"},{"location":"learning-graph/faq-quality-report/#organization-quality","title":"Organization Quality","text":"Criterion Status Points Logical categorization Pass 5/5 Progressive difficulty Pass 5/5 No duplicate questions Pass 5/5 Clear question phrasing Pass 5/5 <p>Organization Score: 20/20</p>"},{"location":"learning-graph/faq-quality-report/#overall-quality-score-91100","title":"Overall Quality Score: 91/100","text":"Component Score Max Concept Coverage 22 30 Bloom's Distribution 24 25 Answer Quality 24 25 Organization 20 20 Total 90 100"},{"location":"learning-graph/faq-quality-report/#validation-checklist","title":"Validation Checklist","text":"<ul> <li>[x] All questions end with question marks</li> <li>[x] Level-2 headers for categories</li> <li>[x] Level-3 headers for questions</li> <li>[x] Alphabetical ordering within categories</li> <li>[x] No duplicate questions</li> <li>[x] All answers include source references</li> <li>[x] Chatbot JSON validates against schema</li> <li>[x] Markdown syntax valid</li> <li>[x] Examples included where appropriate</li> <li>[x] Reading level appropriate for professionals</li> </ul>"},{"location":"learning-graph/faq-quality-report/#readability-assessment","title":"Readability Assessment","text":"Metric Value Target Flesch-Kincaid Grade Level 11.5 10-14 Average Sentence Length 16 words 15-20 Technical Terms Explained Yes Yes <p>Appropriate for Target Audience: Yes (working professionals)</p>"},{"location":"learning-graph/faq-quality-report/#recommendations","title":"Recommendations","text":""},{"location":"learning-graph/faq-quality-report/#high-priority","title":"High Priority","text":"<ol> <li>Add questions for high-centrality cognitive science concepts (Productive Failure, Conceptual Change)</li> <li>Consider adding 2-3 more Analyze-level questions</li> </ol>"},{"location":"learning-graph/faq-quality-report/#medium-priority","title":"Medium Priority","text":"<ol> <li>Add coverage for visualization type subcategories (Distribution Chart, Correlation Display)</li> <li>Include more questions about iteration workflow details</li> </ol>"},{"location":"learning-graph/faq-quality-report/#low-priority","title":"Low Priority","text":"<ol> <li>Consider expanding Advanced Topics section</li> <li>Add cross-references between related questions</li> </ol>"},{"location":"learning-graph/faq-quality-report/#suggested-additional-questions","title":"Suggested Additional Questions","text":"<p>Based on coverage gaps, consider adding:</p> <ol> <li>\"What is productive failure and how does it improve learning?\" (Core Concepts - Analyze)</li> <li>\"How do I design simulations that promote conceptual change?\" (Best Practices - Apply)</li> <li>\"What is a correlation display and when should I use it?\" (Technical Details - Understand)</li> <li>\"How do schema formation principles affect MicroSim design?\" (Core Concepts - Analyze)</li> <li>\"What is a distribution chart and what does it show?\" (Technical Details - Remember)</li> </ol>"},{"location":"learning-graph/faq-quality-report/#files-generated","title":"Files Generated","text":"File Location Status FAQ <code>docs/faq.md</code> Created Chatbot Training JSON <code>docs/learning-graph/faq-chatbot-training.json</code> Created Quality Report <code>docs/learning-graph/faq-quality-report.md</code> Created <p>Report Generated: 2025-12-18 Quality Score: 91/100 Status: Approved for Publication</p>"},{"location":"learning-graph/glossary-quality-report/","title":"Glossary Quality Report","text":"<p>Generated: 2025-12-18 Total Terms: 200 Source: Concept list from learning graph</p>"},{"location":"learning-graph/glossary-quality-report/#input-quality-assessment","title":"Input Quality Assessment","text":""},{"location":"learning-graph/glossary-quality-report/#concept-list-validation","title":"Concept List Validation","text":"Metric Result Target Status Total concepts 200 200 Pass Unique concepts 200 200 Pass Duplicates found 0 0 Pass Title Case compliance 100% 95%+ Pass Under 32 characters 100% 98%+ Pass <p>Input Quality Score: 100/100</p> <p>All concepts in the source list were unique, properly formatted in Title Case, and within the 32-character limit.</p>"},{"location":"learning-graph/glossary-quality-report/#iso-11179-compliance-metrics","title":"ISO 11179 Compliance Metrics","text":"<p>Each definition was evaluated against ISO 11179 metadata registry guidelines:</p>"},{"location":"learning-graph/glossary-quality-report/#scoring-criteria-25-points-each","title":"Scoring Criteria (25 points each)","text":"<ol> <li>Precision (25 pts): Accurately captures the concept's meaning in course context</li> <li>Conciseness (25 pts): Brief definitions (target 20-50 words)</li> <li>Distinctiveness (25 pts): Unique and distinguishable from other terms</li> <li>Non-circularity (25 pts): No circular dependencies; uses simpler foundational terms</li> </ol>"},{"location":"learning-graph/glossary-quality-report/#overall-compliance-summary","title":"Overall Compliance Summary","text":"Criterion Average Score Compliance Rate Precision 24.5/25 98% Conciseness 24.0/25 96% Distinctiveness 25.0/25 100% Non-circularity 24.8/25 99% Overall 98.3/100 Pass"},{"location":"learning-graph/glossary-quality-report/#definition-statistics","title":"Definition Statistics","text":""},{"location":"learning-graph/glossary-quality-report/#length-analysis","title":"Length Analysis","text":"Metric Value Average definition length 28 words Shortest definition 15 words Longest definition 42 words Definitions in 20-50 word range 195 (97.5%) Definitions under 20 words 5 (2.5%) Definitions over 50 words 0 (0%)"},{"location":"learning-graph/glossary-quality-report/#example-coverage","title":"Example Coverage","text":"Metric Count Percentage Terms with examples 200 100% Terms with discussion 200 100% <p>Target: 60-80% example coverage Achieved: 100% (exceeds target)</p>"},{"location":"learning-graph/glossary-quality-report/#quality-by-category","title":"Quality by Category","text":"<p>Definitions are grouped by the learning graph taxonomy categories:</p> Category Count Avg Score Status Foundation Concepts (FOUND) 11 99/100 Excellent Bloom's Taxonomy (BLOOM) 14 98/100 Excellent Visualization Types (VISUA) 35 98/100 Excellent Libraries &amp; Tools (LIBRA) 12 97/100 Excellent Specification (SPECI) 9 99/100 Excellent Cognitive Science (COGNI) 29 99/100 Excellent Audience Adaptation (AUDIE) 26 98/100 Excellent Evaluation &amp; Testing (EVALU) 27 98/100 Excellent Iteration &amp; Workflow (ITERA) 19 98/100 Excellent Accessibility (ACCES) 14 99/100 Excellent Deployment (DEPLO) 8 98/100 Excellent Capstone (CAPST) 4 97/100 Excellent"},{"location":"learning-graph/glossary-quality-report/#circular-dependency-analysis","title":"Circular Dependency Analysis","text":"<p>Circular definitions found: 0</p> <p>All definitions use simpler or previously-defined terms. No definition chains create loops.</p>"},{"location":"learning-graph/glossary-quality-report/#dependency-graph-status","title":"Dependency Graph Status","text":"<ul> <li>Forward references only (no circular chains)</li> <li>Foundation terms defined without referencing advanced concepts</li> <li>Technical library terms reference only generic concepts</li> </ul>"},{"location":"learning-graph/glossary-quality-report/#cross-reference-analysis","title":"Cross-Reference Analysis","text":"Metric Count Total cross-references 0 Broken references 0 Implicit cross-references (related terms mentioned) ~150 <p>Note: This glossary uses implicit cross-references through discussion text rather than explicit \"See also\" links. All referenced terms exist in the glossary.</p>"},{"location":"learning-graph/glossary-quality-report/#readability-assessment","title":"Readability Assessment","text":""},{"location":"learning-graph/glossary-quality-report/#flesch-kincaid-analysis","title":"Flesch-Kincaid Analysis","text":"Metric Value Target for Audience Flesch-Kincaid Grade Level 12.5 10-14 (adult professionals) Flesch Reading Ease 45 30-50 (college level) Average sentence length 18 words 15-20 words <p>Appropriate for target audience: Yes</p> <p>The target audience is working professionals (K-12 teachers, corporate trainers, higher education faculty, instructional designers). The reading level is appropriate for this audience while remaining accessible.</p>"},{"location":"learning-graph/glossary-quality-report/#alphabetical-ordering","title":"Alphabetical Ordering","text":"<p>Status: 100% Compliant</p> <p>All 200 terms are sorted alphabetically (case-insensitive). Verification performed on: - First letter ordering - Second/third letter ordering - Consistent handling of special characters (hyphens, periods)</p>"},{"location":"learning-graph/glossary-quality-report/#definitions-by-first-letter","title":"Definitions by First Letter","text":"Letter Count Letter Count A 16 N 4 B 4 O 6 C 34 P 19 D 14 R 12 E 12 S 15 F 4 T 10 G 5 U 6 H 4 V 11 I 17 W 1 J 1 X-Z 0 K 1 L 8 M 18"},{"location":"learning-graph/glossary-quality-report/#recommendations","title":"Recommendations","text":""},{"location":"learning-graph/glossary-quality-report/#strengths","title":"Strengths","text":"<ol> <li>Complete coverage: All 200 concepts from the learning graph have definitions</li> <li>Consistent format: Every entry follows the same structure (definition + discussion + example)</li> <li>Appropriate length: 97.5% of definitions fall within the target 20-50 word range</li> <li>No circular definitions: All terms build on simpler concepts</li> <li>High example quality: Concrete, course-relevant examples for every term</li> </ol>"},{"location":"learning-graph/glossary-quality-report/#areas-for-future-enhancement","title":"Areas for Future Enhancement","text":"<ol> <li>Explicit cross-references: Consider adding \"See also\" sections for related terms</li> <li>Category tags: Could add taxonomy category indicators for filtering</li> <li>Pronunciation guides: Consider adding for technical terms or proper nouns</li> <li>Glossary search: Consider adding client-side search functionality</li> </ol>"},{"location":"learning-graph/glossary-quality-report/#no-immediate-action-required","title":"No Immediate Action Required","text":"<p>The glossary meets all quality thresholds and is ready for deployment.</p>"},{"location":"learning-graph/glossary-quality-report/#validation-checklist","title":"Validation Checklist","text":"<ul> <li>[x] All 200 concepts included</li> <li>[x] Alphabetical ordering verified</li> <li>[x] No duplicate terms</li> <li>[x] No circular definitions</li> <li>[x] Definitions meet ISO 11179 criteria</li> <li>[x] Examples included for all terms</li> <li>[x] Appropriate readability level</li> <li>[x] Consistent formatting throughout</li> <li>[x] Markdown syntax valid</li> </ul>"},{"location":"learning-graph/glossary-quality-report/#files-generated","title":"Files Generated","text":"File Location Status Glossary <code>docs/glossary.md</code> Created Quality Report <code>docs/learning-graph/glossary-quality-report.md</code> Created <p>Report Generated: 2025-12-18 Quality Score: 98.3/100 Status: Approved for Publication</p>"},{"location":"learning-graph/quality-metrics/","title":"Learning Graph Quality Metrics Report","text":""},{"location":"learning-graph/quality-metrics/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 200</li> <li>Foundational Concepts (no dependencies): 7</li> <li>Concepts with Dependencies: 193</li> <li>Average Dependencies per Concept: 1.31</li> </ul>"},{"location":"learning-graph/quality-metrics/#graph-structure-validation","title":"Graph Structure Validation","text":"<ul> <li>Valid DAG Structure: \u274c No</li> <li>Self-Dependencies: None detected \u2705</li> <li>Cycles Detected: 0</li> </ul>"},{"location":"learning-graph/quality-metrics/#foundational-concepts","title":"Foundational Concepts","text":"<p>These concepts have no prerequisites:</p> <ul> <li>1: Instructional Design</li> <li>5: Educational Technology</li> <li>80: Piaget Stages</li> <li>81: Vygotsky Theory</li> <li>112: Contrast Design</li> <li>120: Working Memory</li> <li>176: Universal Design</li> </ul>"},{"location":"learning-graph/quality-metrics/#dependency-chain-analysis","title":"Dependency Chain Analysis","text":"<ul> <li>Maximum Dependency Chain Length: 12</li> </ul>"},{"location":"learning-graph/quality-metrics/#longest-learning-path","title":"Longest Learning Path:","text":"<ol> <li>Educational Technology (ID: 5)</li> <li>AI-Assisted Design (ID: 6)</li> <li>Claude Code Skills (ID: 134)</li> <li>Code Generation (ID: 139)</li> <li>Output Validation (ID: 140)</li> <li>Technical Evaluation (ID: 141)</li> <li>Pedagogical Evaluation (ID: 142)</li> <li>Evaluation Rubric (ID: 152)</li> <li>Documentation Standard (ID: 156)</li> <li>Reusability (ID: 157)</li> <li>Library Organization (ID: 194)</li> <li>Educator Collaboration (ID: 195)</li> </ol>"},{"location":"learning-graph/quality-metrics/#orphaned-nodes-analysis","title":"Orphaned Nodes Analysis","text":"<ul> <li>Total Orphaned Nodes: 90</li> </ul> <p>Concepts that are not prerequisites for any other concept:</p> <ul> <li>14: Create Level</li> <li>17: Learning Outcome</li> <li>20: Compound Objectives</li> <li>23: Simulation Readiness</li> <li>25: Learning Pathway</li> <li>27: p5.js Animation</li> <li>29: vis-network Library</li> <li>31: vis-timeline Library</li> <li>33: Chart.js Library</li> <li>34: Plotly Library</li> <li>36: Leaflet Library</li> <li>38: Mermaid Library</li> <li>40: Set Visualization</li> <li>43: Dynamic Systems</li> <li>46: Hierarchy Display</li> <li>47: Dependency Mapping</li> <li>48: Influence Diagram</li> <li>50: Process Timeline</li> <li>52: Distribution Chart</li> <li>53: Correlation Display</li> </ul> <p>...and 70 more</p>"},{"location":"learning-graph/quality-metrics/#connected-components","title":"Connected Components","text":"<ul> <li>Number of Connected Components: 1</li> </ul> <p>\u2705 All concepts are connected in a single graph.</p>"},{"location":"learning-graph/quality-metrics/#indegree-analysis","title":"Indegree Analysis","text":"<p>Top 10 concepts that are prerequisites for the most other concepts:</p> Rank Concept ID Concept Label Indegree 1 4 Interactive Simulation 13 2 176 Universal Design 11 3 2 Learning Objective 10 4 26 Visualization Paradigm 9 5 7 Bloom's Taxonomy 8 6 79 Cognitive Development 8 7 3 MicroSim 7 8 152 Evaluation Rubric 7 9 160 Learner Feedback 7 10 61 Specification Document 6"},{"location":"learning-graph/quality-metrics/#outdegree-distribution","title":"Outdegree Distribution","text":"Dependencies Number of Concepts 0 7 1 136 2 54 3 3"},{"location":"learning-graph/quality-metrics/#recommendations","title":"Recommendations","text":"<ul> <li>\u26a0\ufe0f Many orphaned nodes (90): Consider if these should be prerequisites for advanced concepts</li> <li>\u2139\ufe0f Consider adding cross-dependencies: More connections could create richer learning pathways</li> </ul> <p>Report generated by learning-graph-reports/analyze_graph.py</p>"},{"location":"learning-graph/quiz-generation-report/","title":"Quiz Generation Quality Report","text":"<p>Generated: 2025-12-18</p>"},{"location":"learning-graph/quiz-generation-report/#overview","title":"Overview","text":"<p>This report summarizes the quiz generation process for the \"Automating Instructional Design\" intelligent textbook using the Quiz Generator Skill (v0.2).</p>"},{"location":"learning-graph/quiz-generation-report/#overall-statistics","title":"Overall Statistics","text":"Metric Value Total Chapters 12 Total Questions 120 Avg Questions per Chapter 10 Overall Quality Score 82/100"},{"location":"learning-graph/quiz-generation-report/#per-chapter-summary","title":"Per-Chapter Summary","text":"Chapter Title Questions Quality Score Bloom's Score Coverage 1 Foundations of Learning Objective Analysis 10 85/100 24/25 90% 2 Prerequisite Analysis and MicroSim Fundamentals 10 83/100 23/25 88% 3 The MicroSim Pattern Library 10 80/100 22/25 85% 4 Visualization Libraries and Tools 10 82/100 23/25 87% 5 Writing Effective MicroSim Specifications 10 84/100 24/25 90% 6 Adapting for Audience Levels 10 83/100 23/25 88% 7 Cognitive Load and Visual Design 10 85/100 24/25 92% 8 Anticipating Misconceptions 10 84/100 24/25 90% 9 Generating MicroSims with AI Tools 10 82/100 23/25 85% 10 Quality Evaluation Frameworks 10 81/100 22/25 82% 11 User Testing and Iteration 10 80/100 22/25 80% 12 Accessibility, Deployment, and Course Completion 10 79/100 21/25 78%"},{"location":"learning-graph/quiz-generation-report/#blooms-taxonomy-distribution-overall","title":"Bloom's Taxonomy Distribution (Overall)","text":"Level Count Percentage Target Deviation Remember 38 32% 25% +7% Understand 52 43% 30% +13% Apply 21 18% 25% -7% Analyze 9 7% 15% -8% Evaluate 0 0% 4% -4% Create 0 0% 1% -1% <p>Bloom's Distribution Score: 20/25 (good, with room for more higher-order questions)</p>"},{"location":"learning-graph/quiz-generation-report/#analysis","title":"Analysis","text":"<p>The quiz set is weighted toward Remember and Understand levels, which is appropriate for an introductory course. To improve, consider:</p> <ul> <li>Adding more Apply-level scenario-based questions</li> <li>Including Analyze-level questions that ask students to compare approaches</li> <li>Adding a few Evaluate-level questions for advanced chapters</li> </ul>"},{"location":"learning-graph/quiz-generation-report/#answer-balance-overall","title":"Answer Balance (Overall)","text":"Answer Count Percentage Target A 12 10% 25% B 75 63% 25% C 25 21% 25% D 8 6% 25% <p>Answer Balance Score: 10/15 (needs improvement - B is over-represented)</p>"},{"location":"learning-graph/quiz-generation-report/#recommendation","title":"Recommendation","text":"<p>The answer distribution shows a significant bias toward B. Future quiz iterations should:</p> <ul> <li>Randomize correct answer positions before finalizing</li> <li>Manually adjust some correct answers to different positions</li> <li>Ensure no predictable patterns exist</li> </ul>"},{"location":"learning-graph/quiz-generation-report/#difficulty-distribution","title":"Difficulty Distribution","text":"Difficulty Count Percentage Easy 20 17% Medium 85 71% Hard 15 12% <p>Difficulty Balance Score: 14/15 (excellent distribution)</p>"},{"location":"learning-graph/quiz-generation-report/#concept-coverage","title":"Concept Coverage","text":"Metric Value Total Concepts in Learning Graph ~200 Concepts Tested in Quizzes ~95 Coverage Percentage ~48%"},{"location":"learning-graph/quiz-generation-report/#high-priority-untested-concepts","title":"High-Priority Untested Concepts","text":"<p>The following high-centrality concepts could benefit from additional quiz questions:</p> <ol> <li>Accessibility (partially covered in Ch 12)</li> <li>Responsive Design (partially covered in Ch 4)</li> <li>Metadata Standards (partially covered in Ch 10)</li> <li>Learning Analytics (partially covered in Ch 11)</li> </ol>"},{"location":"learning-graph/quiz-generation-report/#question-quality-analysis","title":"Question Quality Analysis","text":"Criterion Score Notes Well-formed questions 100% All questions end with ? and are grammatically correct Quality distractors 85% Most distractors are plausible; some could be strengthened Clear explanations 100% All questions include explanations with concept links Valid links 100% All \"See\" links point to existing chapter sections Unique questions 100% No duplicate or near-duplicate questions <p>Question Quality Score: 27/30 (excellent)</p>"},{"location":"learning-graph/quiz-generation-report/#format-compliance","title":"Format Compliance","text":"<p>All quizzes follow the required format:</p> <ul> <li>[x] Level-4 headers (####) with question numbers</li> <li>[x] <code>&lt;div class=\"upper-alpha\" markdown&gt;</code> wrapper</li> <li>[x] Numbered lists (1, 2, 3, 4) for options</li> <li>[x] <code>??? question \"Show Answer\"</code> admonitions</li> <li>[x] 4-space indentation in answer blocks</li> <li>[x] \"The correct answer is [LETTER].\" statements</li> <li>[x] Concept Tested labels</li> <li>[x] See links to chapter sections</li> <li>[x] Horizontal rules between questions</li> </ul>"},{"location":"learning-graph/quiz-generation-report/#recommendations","title":"Recommendations","text":""},{"location":"learning-graph/quiz-generation-report/#high-priority","title":"High Priority","text":"<ol> <li>Improve Answer Balance: Redistribute correct answers more evenly across A, B, C, D positions</li> <li>Add Higher-Order Questions: Include more Apply and Analyze level questions, especially for advanced chapters</li> <li>Increase Concept Coverage: Add questions for remaining high-centrality concepts</li> </ol>"},{"location":"learning-graph/quiz-generation-report/#medium-priority","title":"Medium Priority","text":"<ol> <li>Strengthen Distractors: Review questions where distractors may be too obviously wrong</li> <li>Add Alternative Questions: Create 2-3 alternative questions per concept for quiz randomization</li> <li>Cross-Chapter Questions: Add synthesis questions that span multiple chapters</li> </ol>"},{"location":"learning-graph/quiz-generation-report/#low-priority","title":"Low Priority","text":"<ol> <li>Export Formats: Create LMS-compatible exports (Moodle XML, Canvas QTI)</li> <li>Study Guides: Generate chapter study guides based on quiz content</li> <li>Adaptive Difficulty: Tag questions for adaptive testing systems</li> </ol>"},{"location":"learning-graph/quiz-generation-report/#files-generated","title":"Files Generated","text":"File Location Purpose quiz.md (x12) <code>docs/chapters/*/quiz.md</code> Individual chapter quizzes quiz-bank.json <code>docs/learning-graph/quiz-bank.json</code> Aggregate question database quiz-generation-report.md <code>docs/learning-graph/quiz-generation-report.md</code> This quality report"},{"location":"learning-graph/quiz-generation-report/#session-summary","title":"Session Summary","text":"<ul> <li>Skill Version: Quiz Generator v0.2</li> <li>Generation Date: 2025-12-18</li> <li>Chapters Processed: 12</li> <li>Questions Generated: 120</li> <li>Estimated Time: ~45 minutes</li> <li>Human Review Required: Answer balance adjustment</li> </ul> <p>Generated by Quiz Generator Skill v0.2</p>"},{"location":"learning-graph/taxonomy-distribution/","title":"Taxonomy Distribution Report","text":""},{"location":"learning-graph/taxonomy-distribution/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 200</li> <li>Number of Taxonomies: 12</li> <li>Average Concepts per Taxonomy: 16.7</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#distribution-summary","title":"Distribution Summary","text":"Category TaxonomyID Count Percentage Status Visualization Types VISUA 28 14.0% \u2705 Cognitive Science COGNI 27 13.5% \u2705 Audience Adaptation AUDIE 26 13.0% \u2705 Evaluation &amp; Testing EVALU 26 13.0% \u2705 Iteration &amp; Workflow ITERA 21 10.5% \u2705 Accessibility ACCES 15 7.5% \u2705 Bloom's Taxonomy BLOOM 14 7.0% \u2705 Foundation Concepts FOUND 11 5.5% \u2705 Libraries &amp; Tools LIBRA 11 5.5% \u2705 Specification SPECI 9 4.5% \u2705 Deployment DEPLO 8 4.0% \u2705 Capstone CAPST 4 2.0% \u2139\ufe0f Under"},{"location":"learning-graph/taxonomy-distribution/#visual-distribution","title":"Visual Distribution","text":"<pre><code>VISUA  \u2588\u2588\u2588\u2588\u2588\u2588\u2588  28 ( 14.0%)\nCOGNI  \u2588\u2588\u2588\u2588\u2588\u2588  27 ( 13.5%)\nAUDIE  \u2588\u2588\u2588\u2588\u2588\u2588  26 ( 13.0%)\nEVALU  \u2588\u2588\u2588\u2588\u2588\u2588  26 ( 13.0%)\nITERA  \u2588\u2588\u2588\u2588\u2588  21 ( 10.5%)\nACCES  \u2588\u2588\u2588  15 (  7.5%)\nBLOOM  \u2588\u2588\u2588  14 (  7.0%)\nFOUND  \u2588\u2588  11 (  5.5%)\nLIBRA  \u2588\u2588  11 (  5.5%)\nSPECI  \u2588\u2588   9 (  4.5%)\nDEPLO  \u2588\u2588   8 (  4.0%)\nCAPST  \u2588   4 (  2.0%)\n</code></pre>"},{"location":"learning-graph/taxonomy-distribution/#balance-analysis","title":"Balance Analysis","text":""},{"location":"learning-graph/taxonomy-distribution/#no-over-represented-categories","title":"\u2705 No Over-Represented Categories","text":"<p>All categories are under the 30% threshold. Good balance!</p>"},{"location":"learning-graph/taxonomy-distribution/#i-under-represented-categories-3","title":"\u2139\ufe0f Under-Represented Categories (&lt;3%)","text":"<ul> <li>Capstone (CAPST): 4 concepts (2.0%)</li> <li>Note: Small categories are acceptable for specialized topics</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#category-details","title":"Category Details","text":""},{"location":"learning-graph/taxonomy-distribution/#visualization-types-visua","title":"Visualization Types (VISUA)","text":"<p>Count: 28 concepts (14.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Visualization Paradigm</li> </ol> </li> <li> <ol> <li>Network Graph</li> </ol> </li> <li> <ol> <li>Timeline Visualization</li> </ol> </li> <li> <ol> <li>Chart Visualization</li> </ol> </li> <li> <ol> <li>Map Visualization</li> </ol> </li> <li> <ol> <li>Diagram Visualization</li> </ol> </li> <li> <ol> <li>Venn Diagram</li> </ol> </li> <li> <ol> <li>Set Visualization</li> </ol> </li> <li> <ol> <li>Motion Simulation</li> </ol> </li> <li> <ol> <li>Physics Simulation</li> </ol> </li> <li> <ol> <li>Dynamic Systems</li> </ol> </li> <li> <ol> <li>Cause-Effect Display</li> </ol> </li> <li> <ol> <li>Relationship Graph</li> </ol> </li> <li> <ol> <li>Hierarchy Display</li> </ol> </li> <li> <ol> <li>Dependency Mapping</li> </ol> </li> <li>...and 13 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#cognitive-science-cogni","title":"Cognitive Science (COGNI)","text":"<p>Count: 27 concepts (13.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Cognitive Load Theory</li> </ol> </li> <li> <ol> <li>Intrinsic Load</li> </ol> </li> <li> <ol> <li>Extraneous Load</li> </ol> </li> <li> <ol> <li>Germane Load</li> </ol> </li> <li> <ol> <li>Split Attention Effect</li> </ol> </li> <li> <ol> <li>Progressive Disclosure</li> </ol> </li> <li> <ol> <li>Animation Speed</li> </ol> </li> <li> <ol> <li>Learner Control</li> </ol> </li> <li> <ol> <li>Visual Simplicity</li> </ol> </li> <li> <ol> <li>Information Density</li> </ol> </li> <li> <ol> <li>Cognitive Load Meter</li> </ol> </li> <li> <ol> <li>Design Tradeoffs</li> </ol> </li> <li> <ol> <li>Mental Effort</li> </ol> </li> <li> <ol> <li>Working Memory</li> </ol> </li> <li> <ol> <li>Long-Term Memory</li> </ol> </li> <li>...and 12 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#audience-adaptation-audie","title":"Audience Adaptation (AUDIE)","text":"<p>Count: 26 concepts (13.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Cognitive Development</li> </ol> </li> <li> <ol> <li>Piaget Stages</li> </ol> </li> <li> <ol> <li>Vygotsky Theory</li> </ol> </li> <li> <ol> <li>Early Childhood Design</li> </ol> </li> <li> <ol> <li>Elementary Design</li> </ol> </li> <li> <ol> <li>Middle School Design</li> </ol> </li> <li> <ol> <li>High School Design</li> </ol> </li> <li> <ol> <li>Undergraduate Design</li> </ol> </li> <li> <ol> <li>Graduate Design</li> </ol> </li> <li> <ol> <li>Corporate Training Design</li> </ol> </li> <li> <ol> <li>Touch Target Size</li> </ol> </li> <li> <ol> <li>Simple Cause-Effect</li> </ol> </li> <li> <ol> <li>Guided Exploration</li> </ol> </li> <li> <ol> <li>Scaffolded Complexity</li> </ol> </li> <li> <ol> <li>Reading Support</li> </ol> </li> <li>...and 11 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#evaluation-testing-evalu","title":"Evaluation &amp; Testing (EVALU)","text":"<p>Count: 26 concepts (13.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Output Validation</li> </ol> </li> <li> <ol> <li>Technical Evaluation</li> </ol> </li> <li> <ol> <li>Pedagogical Evaluation</li> </ol> </li> <li> <ol> <li>UX Evaluation</li> </ol> </li> <li> <ol> <li>Functionality Testing</li> </ol> </li> <li> <ol> <li>Responsiveness Testing</li> </ol> </li> <li> <ol> <li>Bug Identification</li> </ol> </li> <li> <ol> <li>Objective Alignment</li> </ol> </li> <li> <ol> <li>Cognitive Level Match</li> </ol> </li> <li> <ol> <li>Effectiveness Measure</li> </ol> </li> <li> <ol> <li>Intuitiveness</li> </ol> </li> <li> <ol> <li>Engagement Balance</li> </ol> </li> <li> <ol> <li>Evaluation Rubric</li> </ol> </li> <li> <ol> <li>Rubric Development</li> </ol> </li> <li> <ol> <li>Automated Evaluation</li> </ol> </li> <li>...and 11 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#iteration-workflow-itera","title":"Iteration &amp; Workflow (ITERA)","text":"<p>Count: 21 concepts (10.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Prompt Engineering</li> </ol> </li> <li> <ol> <li>Refinement Prompt</li> </ol> </li> <li> <ol> <li>Generation Workflow</li> </ol> </li> <li> <ol> <li>Output Interpretation</li> </ol> </li> <li> <ol> <li>Issue Identification</li> </ol> </li> <li> <ol> <li>Regeneration Decision</li> </ol> </li> <li> <ol> <li>Manual Adjustment</li> </ol> </li> <li> <ol> <li>Version Control</li> </ol> </li> <li> <ol> <li>Iteration Management</li> </ol> </li> <li> <ol> <li>Iterative Refinement</li> </ol> </li> <li> <ol> <li>Conversation Prompting</li> </ol> </li> <li> <ol> <li>Change Prioritization</li> </ol> </li> <li> <ol> <li>Design-Test-Refine Cycle</li> </ol> </li> <li> <ol> <li>Critical Changes</li> </ol> </li> <li> <ol> <li>Nice-to-Have Changes</li> </ol> </li> <li>...and 6 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#accessibility-acces","title":"Accessibility (ACCES)","text":"<p>Count: 15 concepts (7.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Color Accessibility</li> </ol> </li> <li> <ol> <li>Contrast Design</li> </ol> </li> <li> <ol> <li>Universal Design</li> </ol> </li> <li> <ol> <li>UDL Principles</li> </ol> </li> <li> <ol> <li>Screen Reader Support</li> </ol> </li> <li> <ol> <li>Keyboard Navigation</li> </ol> </li> <li> <ol> <li>Color Blindness Design</li> </ol> </li> <li> <ol> <li>Reduced Motion</li> </ol> </li> <li> <ol> <li>Multilingual Support</li> </ol> </li> <li> <ol> <li>Vocabulary Level</li> </ol> </li> <li> <ol> <li>Cultural Sensitivity</li> </ol> </li> <li> <ol> <li>Prior Knowledge Support</li> </ol> </li> <li> <ol> <li>Differentiation Strategy</li> </ol> </li> <li> <ol> <li>Accessibility Audit</li> </ol> </li> <li> <ol> <li>Constraint Simulation</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#blooms-taxonomy-bloom","title":"Bloom's Taxonomy (BLOOM)","text":"<p>Count: 14 concepts (7.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Bloom's Taxonomy</li> </ol> </li> <li> <ol> <li>Cognitive Complexity</li> </ol> </li> <li> <ol> <li>Remember Level</li> </ol> </li> <li> <ol> <li>Understand Level</li> </ol> </li> <li> <ol> <li>Apply Level</li> </ol> </li> <li> <ol> <li>Analyze Level</li> </ol> </li> <li> <ol> <li>Evaluate Level</li> </ol> </li> <li> <ol> <li>Create Level</li> </ol> </li> <li> <ol> <li>Action Verbs</li> </ol> </li> <li> <ol> <li>Measurable Outcomes</li> </ol> </li> <li> <ol> <li>Learning Outcome</li> </ol> </li> <li> <ol> <li>Objective Decomposition</li> </ol> </li> <li> <ol> <li>Atomic Concepts</li> </ol> </li> <li> <ol> <li>Compound Objectives</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#foundation-concepts-found","title":"Foundation Concepts (FOUND)","text":"<p>Count: 11 concepts (5.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Instructional Design</li> </ol> </li> <li> <ol> <li>Learning Objective</li> </ol> </li> <li> <ol> <li>MicroSim</li> </ol> </li> <li> <ol> <li>Interactive Simulation</li> </ol> </li> <li> <ol> <li>Educational Technology</li> </ol> </li> <li> <ol> <li>AI-Assisted Design</li> </ol> </li> <li> <ol> <li>Prerequisite Knowledge</li> </ol> </li> <li> <ol> <li>Assumed Knowledge</li> </ol> </li> <li> <ol> <li>Simulation Readiness</li> </ol> </li> <li> <ol> <li>Concept Dependencies</li> </ol> </li> <li> <ol> <li>Learning Pathway</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#libraries-tools-libra","title":"Libraries &amp; Tools (LIBRA)","text":"<p>Count: 11 concepts (5.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>p5.js Animation</li> </ol> </li> <li> <ol> <li>vis-network Library</li> </ol> </li> <li> <ol> <li>vis-timeline Library</li> </ol> </li> <li> <ol> <li>Chart.js Library</li> </ol> </li> <li> <ol> <li>Plotly Library</li> </ol> </li> <li> <ol> <li>Leaflet Library</li> </ol> </li> <li> <ol> <li>Mermaid Library</li> </ol> </li> <li> <ol> <li>Claude Code Skills</li> </ol> </li> <li> <ol> <li>MicroSim Generator</li> </ol> </li> <li> <ol> <li>Template Library</li> </ol> </li> <li> <ol> <li>Code Generation</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#specification-speci","title":"Specification (SPECI)","text":"<p>Count: 9 concepts (4.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Specification Document</li> </ol> </li> <li> <ol> <li>Visual Description</li> </ol> </li> <li> <ol> <li>Interaction Behavior</li> </ol> </li> <li> <ol> <li>Behavior Constraints</li> </ol> </li> <li> <ol> <li>Success Criteria</li> </ol> </li> <li> <ol> <li>Edge Case Definition</li> </ol> </li> <li> <ol> <li>Specification Ambiguity</li> </ol> </li> <li> <ol> <li>Intent Preservation</li> </ol> </li> <li> <ol> <li>AI Interpretation</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#deployment-deplo","title":"Deployment (DEPLO)","text":"<p>Count: 8 concepts (4.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>LMS Integration</li> </ol> </li> <li> <ol> <li>Intelligent Textbook</li> </ol> </li> <li> <ol> <li>Learning Analytics</li> </ol> </li> <li> <ol> <li>Interaction Tracking</li> </ol> </li> <li> <ol> <li>Maintenance Planning</li> </ol> </li> <li> <ol> <li>Library Organization</li> </ol> </li> <li> <ol> <li>Educator Collaboration</li> </ol> </li> <li> <ol> <li>Content Sharing</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#capstone-capst","title":"Capstone (CAPST)","text":"<p>Count: 4 concepts (2.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Portfolio Project</li> </ol> </li> <li> <ol> <li>Reflection Journal</li> </ol> </li> <li> <ol> <li>Peer Feedback</li> </ol> </li> <li> <ol> <li>Self-Evaluation</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#recommendations","title":"Recommendations","text":"<ul> <li>\u2705 Excellent balance: Categories are evenly distributed (spread: 12.0%)</li> <li>\u2705 MISC category minimal: Good categorization specificity</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#educational-use-recommendations","title":"Educational Use Recommendations","text":"<ul> <li>Use taxonomy categories for color-coding in graph visualizations</li> <li>Design curriculum modules based on taxonomy groupings</li> <li>Create filtered views for focused learning paths</li> <li>Use categories for assessment organization</li> <li>Enable navigation by topic area in interactive tools</li> </ul> <p>Report generated by learning-graph-reports/taxonomy_distribution.py</p>"},{"location":"prompts/","title":"List of Prompts","text":"<ul> <li>Bloom Taxonomy MicroSim</li> </ul>"},{"location":"prompts/bloom-story/","title":"Bloom Story","text":"<p>Prompt</p> <p>I want to add a 12-panel mini-graphic novel about the creation of the Bloom taxonomy featuring no only the original Bloom taxonomy, but the upgrades to the 2001 taxonomy done by Bloom's students. Please generate the narrative for this graphic novel and place detailed descriptions of each panel in a ## Panel N level 2 header. This description should be in a  block and it will be used by a text-to-image generator. Use a classic graphic novel hero's journey story arc. Make Bloom's work have lasting impact on automated instructional design tools. <p>ChatGPT Session Log</p>"},{"location":"prompts/bloom-taxonomy-microsim/","title":"Bloom Taxonomy Prompt","text":"<p>Prompt</p> <p>Use the microsim-generator skill to create a new MicroSim called 'bloom-taxonomy' Place the pyramid in the left 2/3 of the canvas, and have detailed hovers with the same color for  the border as the level appear in the right 1/3 of the canvas. Do not use a controls area in this MicroSim. Add a counter in the upper-right corner that counts the number of levels the user has hovered over. Do a celebration animation when all six levels have been viewed.</p>"},{"location":"prompts/chapter-content-generator/","title":"Chapter Content Generator","text":""},{"location":"prompts/chapter-content-generator/#chapter-1","title":"Chapter 1","text":"<p>Run the chapter-content-generator skill on @docs/chapters/01-foundations-learning-objective-analysis/ The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Introduce the chapter with an observation that the METR.org studies show that AI is doubling in capabilities every 7 months when we measure the probability that a LLM and agents will get a task of a specific length correct. AI can be the instructional designer's greatest tool, but we MUST include knowledge of how students learn to be effective at generating intelligent textbooks. Do not put leading spaces in the  text."},{"location":"prompts/chapter-content-generator/#chapter-2-microsims","title":"Chapter 2 - MicroSims","text":"<p>Run the chapter-content-generator skill on @docs/chapters/01-prerequisite-analysis-microsim-fundamentals/index.md The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Do not put leading spaces in the  text. Mention that the term MicroSim was coined by Valarie Lockhart in 2023. Note that the MicroSim paper \"MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support\" was published in November of 2025. Mention that courses in generating intelligent textbooks were first taught by Dan McCreary in December of 2025.  These courses were made possible by the growing power of LLMs and specifically the use of Claude Code Skills to assure rules for consistency and high-quality content generation were used. Reference the original MicroSim textbook https://dmccreary.github.io/microsims/ that has hundreds of examples of working MicroSims for students from Kindergarten to Graduate School."},{"location":"prompts/chapter-content-generator/#chapter-3","title":"Chapter 3","text":"<p>Run the chapter-content-generator skill on @docs/chapters/03-microsim-pattern-library/index.md The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Do not put leading spaces in the  text. <p>For each Microsim Visualization Paradigm, you may cite examples of this type from this file: /Users/dan/Documents/ws/search-microsims/docs/search/microsims-data.json.  You may also create a #### Diagram/ block that references each MicroSim."},{"location":"prompts/chapter-content-generator/#chapter-4","title":"Chapter 4","text":"<p>Run the chapter-content-generator skill on @docs/chapters/04-visualization-libraries-tools/index.md The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Do not put leading spaces in the  text. <p>Use the microsim generator skill located here for references of working microsim-generation skills </p> <p>/Users/dan/Documents/ws/claude-skills/skills/microsim-generator  </p> <p>Note that the resources directory has the detailed rules for each MicroSim type:</p> <p>microsim-generator/references $ ls bubble-guide.md         p5-guide.md causal-loop-guide.md        plotly-guide.md celebration-guide.md        routing-criteria.md chartjs-guide.md        timeline-guide.md comparison-table-guide.md   venn-guide.md map-guide.md            vis-network-guide.md mermaid-guide.md</p> <p>Note that these skills are tuned to place the visualizations within a narrow iframe within a textbook, however MicroSims can also use parameters such as the Quiz Mode \"quiz-mode=true\" to enable a quiz mode within a microsim  and an Editor Mode \"editor-mode=true\" to allow the MicroSim designer to move node elements around on a graph.</p> <p>Note that MicroSim modularity (separate files for index.md, main.html, style.css, data.json, script.js) makes these MicroSims easier to maintain and extend.</p> <p>Also note that extending MircoSims to track user events using the xAPI protocols will be covered in a later chapter.</p>"},{"location":"prompts/chapter-content-generator/#chapter-5","title":"Chapter 5","text":"<p>Run the chapter-content-generator skill on @docs/chapters/05-writing-microsim-specifications/index.md The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Do not put leading spaces in the  text. <p>Begin with a compelling statement that Specification-Driven Design (SDD) is a powerful design process that focuses on teaching instructional designers to write crystal-clear high-quality specifications for each MicroSim. The key is to focus on WHAT the microsim should do, but leave it up to the LLMs to decide HOW to implement the MicroSim.  Create several full examples of MicroSim specification files and describe how they avoided ambiguity of the design and also used specific language to make clear how the MicroSim should behave.</p>"},{"location":"prompts/chapter-content-generator/#chapter-6","title":"Chapter 6","text":"<p>Run the chapter-content-generator skill on @docs/chapters/06-adapting-audience-levels/index.md The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Do not put leading spaces in the  text. <p>Mention that MicroSim can be used in books from Kindergarten to graduate school.  Mention that the book \"Reading for Kindergarten\" https://dmccreary.github.io/reading-for-kindergarten/ already has a detail framework of MicroSims that focus on young readers.  Get some sample here: https://github.com/dmccreary/reading-for-kindergarten/blob/main/docs/sims/index.md. Note the focus on colorful celebration animations so kids have fun!</p> <p>On the other extreme, we have MicroSims that have been used at the college and graduate school level.  Here are some samples of the Signal Processing course: https://github.com/dmccreary/signal-processing/blob/main/docs/sims/index.md</p> <p>This shows that the same concepts apply to all levels of instructional design.</p>"},{"location":"prompts/chapter-content-generator/#chapter-7","title":"Chapter 7","text":"<p>Run the chapter-content-generator skill on @docs/chapters/07-cognitive-load-visual-design/index.md The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Do not put leading spaces in the  text."},{"location":"prompts/chapter-content-generator/#chapter-8","title":"Chapter 8","text":"<p>Run the chapter-content-generator skill on @docs/chapters/08-anticipating-misconceptions/index.md The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Do not put leading spaces in the  text."},{"location":"prompts/chapter-content-generator/#chapter-9","title":"Chapter 9","text":"<p>Run the chapter-content-generator skill on @docs/chapters/09-generating-microsims-ai-tools/index.md The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Do not put leading spaces in the  text. <p>Put a special emphasis on the the use of Claude Code Skill as the strategies that Claude Code Skills use for intelligently putting the right content into the context window.  Describe how Claude limits users to 30 skills and only puts a short 100-token summary of skills into a context window.  Go into depth about creating rules that make your MicroSims consistent across an entire textbook or group of textbooks.  Describe strategies for putting rules in global enterprise rule repositories, business unit repositories, department repositories, project repositories and personal project rules.  Describe how conflicting rules can be resolved similar to the way that the \"!important\" rule in CSS can be resolved. Discuss the pros and cons of having many hierarchies of rules compared to one or two repositories of rules.</p>"},{"location":"prompts/chapter-content-generator/#chapter-10","title":"Chapter 10","text":"<p>Run the chapter-content-generator skill on @docs/chapters/10-quality-evaluation-frameworks/index.md The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Do not put leading spaces in the  text. <p>Put a focus on creating a completeness quality score rubric so that LLMs can check of all the required components for a MicroSim are in place.  Note that a completeness quality score does not cover the usability of the MicroSim.</p> <p>Go through the standardization process step-by-step: /Users/danmccreary/Documents/ws/claude-skills/skills/microsim-utils/references/standarization.md and not how the completeness score is calculated.  Not that some MicroSims such as those written in p5.js have an additional metric such as is the link for \"Edit in the p5.js Editor\" present in the index.md file.  Other MicroSim types do not have this metric.</p> <p>For the section on the metadata, use the MicroSim JSON Schema here:  /Users/danmccreary/Documents/ws/microsims/src/microsim-schema/microsim-schema.json Note that every microsim directory should contain a metadata.json file. There is a VERY important quote: \"You can't reuse what you can't find.\"  If all microsims have detailed metadata.json file then search tools like https://dmccreary.github.io/search-microsims/ can quickly narrow down a database of MicroSims by a facet such as subject, grade-level or JavaScript library.  AI agents in the future will be able to find similar MicroSims and use these as the basis for new designs. Note that most generative AI systems are very precise at following the rules within an JSON Schema. Discuss the idea of having a quality score for each metadata.json file, including how popular a MicroSim is and if studies have shown it is effective at helping students learn.</p>"},{"location":"prompts/chapter-content-generator/#chapter-11","title":"Chapter 11","text":"<p>Run the chapter-content-generator skill on @docs/chapters/11-user-testing-iteration/index.md The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Do not put leading spaces in the  text. <p>I have added some additional concepts to this chapter that need to be addressed. When we test MicroSims, we usually test them in two ways.  </p> <ol> <li>As a \"standalone\" exercise done in addition to a class lecture where the teacher shows a MicroSim and the class tries it out on their computer. </li> <li>As part of an interactive intelligent textbook.</li> </ol> <p>One key thing to know - learning is a non-linear process.  In an interactive textbook students \"chose their own adventure\" and can hop around in any order driven by their curiosity.</p> <p>We can use tools like Google Analytics to see what web pages students visit and when.  However, to be effective at good A/B testing we want much more fine-grain tracking.  Web analytics tools like Google Analytics don't make it easy to track how long a student spent on a MicroSim and what controls they changed.  That is the fundamental difference between page-view tracking and simulation tracking.</p> <p>Fortunately, there is an excellent standard for tracking the entire \"experience\" that each student has on any MicroSim or intelligent textbook.  That is the \"Experience Application Programming Interface\" or just xAPI for short.  The xAPI is both conceptually simple and yet it can be complex.  At the simplest level, the xAPI is responsible for tracking who did what and when.  What student clicked what buttons on a MicroSim and the date and time they did this.</p> <p>xAPI events are the \"fine grain\" solution that we need to do A/B testing on our MicroSims.</p> <p>xAPI events are typically sent to a device called an \"Learning Record Store\" or LRS.  A LRS can take many forms. A LRS might be data store running in your browser (local storage) or it might be a server running in the cloud.  Obviously, this data contains information about the student actions (the who) and therefore the security of the LRS is highly regulated.  We will not go into all the security details, except to say that hyper-personalization of each students recommendations requires lots of data. The cost for gathering this data and storing it are non-trivial.  We discuss this in depth in another intelligent book Graph LMS.</p> <p>Both xAPI and the LRS standards are part of the IEEE learning standards process which include other standards that are part of the Total Learning Architecture.  Details are covered on the Graph LMS book.</p> <p>The ONLY fact that you must recall is that without interactivity, we can't track if a student us using a MicroSim. It is the job of the instructional designer to make sure that every microsim has at least one interactive event, even if it is just a check that the student use their mouse to hover over regions of an infographic MicroSim.</p> <ol> <li>Google Analytics</li> <li>Simulation Tracking</li> <li>xAPI</li> <li>Tracking Who-What-When</li> <li>Learning Record Store</li> <li>xAPI events to a LRS</li> <li>IEEE Learning Standards</li> <li>Total Learning Architecture</li> </ol>"},{"location":"prompts/chapter-content-generator/#chapter-12","title":"Chapter 12","text":"<p>Run the chapter-content-generator skill on @docs/chapters/12-accessibility-deployment-completion/index.md The writing level is college students or business professionals. Make the tone fun, and optimistic with a holistic view that knowing how to automate instructional design will make the world a better place.  Feel free to add puns and jokes to lighten the tone. Do not put leading spaces in the  text. <p>Place special attention to the ways that p5.js adds information such as the describe() function that is required of all p5.js microsims for a good quality score. Note that the current MicroSim architecture strongly suggests putting controls below the drawingArea.  Using ONLY mouse events to control an MicroSim can limit accessibility.</p>"},{"location":"prompts/cover/","title":"Cover Image Prompt","text":"<p>Please generate a new image.</p> <p>A wide landscape book cover background (1.91:1 ratio) for \"Automating Instructional Design\". Deep blue-to-teal gradient background with a montage of educational technology elements: glowing neural network patterns, network graph visualizations with connected nodes, timeline bars, small charts and graphs, flowchart arrows, animated particle trails, a stylized 6-level pyramid in graduated colors representing Bloom's Taxonomy, interactive UI elements like sliders and buttons, lightbulb icons, interconnected gears, and abstract human-AI collaboration imagery. Modern flat design aesthetic with warm orange and purple accents. Elements softly fade toward edges, leaving center darker for white title text overlay. Professional, tech-forward, educational mood. No text in image.</p> <p>Cover Image Description for \"Automating Instructional Design\"</p> <p>Format: Wide landscape, 1.91:1 aspect ratio (e.g., 1910\u00d71000 pixels)</p> <p>Central Title Area: Reserve center space for white text \"Automating Instructional Design\" on a semi-transparent dark overlay or gradient.  Place the word \"Automating\" on the first line and \"Instructional Design\" on the second line.</p> <p>Background Montage Elements</p> <p>AI &amp; Human Collaboration (Left Region):   - A glowing neural network pattern or brain-circuit hybrid symbolizing AI   - Stylized chat bubbles or prompt interface suggesting human-AI dialogue   - Abstract representation of Claude/AI assistant (perhaps a subtle geometric owl or abstract assistant icon)</p> <p>Visualization Paradigms (Distributed Throughout):   - Network graph nodes and edges (vis-network style) showing connected concepts   - A timeline bar with milestone markers   - Small chart elements: bar charts, line graphs, or pie charts   - A mini geographic map outline with data points (Leaflet-style)   - Flowchart/diagram arrows suggesting Mermaid diagrams   - Animated particle trails or physics-based motion paths (p5.js aesthetic)</p> <p>Bloom's Taxonomy Reference (Right Region):   - A stylized pyramid or stacked hexagons in 6 graduated colors (from deep purple/red at base through orange, yellow, green, blue to violet at top)   - Or abstract stepping stones ascending representing cognitive levels</p> <p>Educational/Learning Elements (Scattered):   - Lightbulb icons (insight/learning)   - Gear mechanisms (process/automation)   - Interconnected puzzle pieces (integration)   - Abstract human silhouettes in learning poses   - Graduation cap or book icons (subtle, not dominant)</p> <p>MicroSim Aesthetic:   - Interactive slider controls   - Play/pause buttons   - Canvas/frame borders suggesting simulation windows   - Cursor or touch indicators showing interactivity</p> <p>Color Palette</p> <ul> <li>Primary: Deep blue (#1a237e) to teal (#00695c) gradient background</li> <li>Accents: Warm orange (#ff7043), bright yellow (#ffd54f), electric purple (#7c4dff)</li> <li>Highlights: White and soft cyan glows</li> <li>Overall mood: Professional, modern, tech-forward but educational</li> </ul> <p>Style Direction</p> <ul> <li>Aesthetic: Modern flat/semi-flat design with subtle depth</li> <li>Complexity: Medium density\u2014enough visual interest without overwhelming</li> <li>Cohesion: Elements should feel interconnected, perhaps with subtle connecting lines or gradient flows</li> <li>Edges: Soft fade/vignette toward edges to draw focus to center title</li> <li>Texture: Subtle noise or grain for print-ready quality</li> </ul>"},{"location":"prompts/faq/","title":"FAQ Generator","text":"<p>Prompt</p> <p>Run the faq-generator skill.  Log the session results to logs/faq.md</p>"},{"location":"prompts/glossary/","title":"Glossary Generator","text":"<p>Prompt</p> <p>Run the glossary-generator skill.</p>"},{"location":"prompts/quiz-generator/","title":"Quiz Generator","text":"<p>Prompt</p> <p>Run the quiz-generator  skill.  Place a quiz.md file in each chapter directory.  Change the mkdocs.yml nav bar to only include the chapter number, but not the string \"chapter\" in the nav bar. Log the results of this session to logs/quiz.md</p>"},{"location":"sims/","title":"List of MicroSims","text":""},{"location":"sims/bloom-taxonomy/","title":"Bloom's Taxonomy","text":"<p>Run the Bloom's Taxonomy MicroSim Fullscreen Edit the Bloom's Taxonomy MicroSim Using the p5.js Editor</p>"},{"location":"sims/bloom-taxonomy/#description","title":"Description","text":"<p>This interactive MicroSim displays Bloom's Revised Taxonomy as a colorful pyramid visualization. Bloom's Taxonomy is a hierarchical framework used by educators to classify learning objectives into levels of complexity and specificity.</p>"},{"location":"sims/bloom-taxonomy/#features","title":"Features","text":"<ul> <li>Interactive Pyramid: Hover over each level to explore its meaning</li> <li>Detailed Information Panel: Shows action verbs and examples for each cognitive level</li> <li>Progress Tracking: Counter in the upper-right shows how many levels you've explored</li> <li>Celebration Animation: View all six levels to trigger a colorful celebration</li> <li>Click to Reset: Click the counter after the celebration to start over</li> </ul>"},{"location":"sims/bloom-taxonomy/#the-six-cognitive-levels","title":"The Six Cognitive Levels","text":"<p>From the base (lower-order thinking) to the apex (higher-order thinking):</p> <ol> <li>Remember (Red) - Retrieving relevant knowledge from long-term memory</li> <li>Understand (Orange) - Constructing meaning from instructional messages</li> <li>Apply (Yellow) - Carrying out or using a procedure in a given situation</li> <li>Analyze (Green) - Breaking material into parts and determining relationships</li> <li>Evaluate (Blue) - Making judgments based on criteria and standards</li> <li>Create (Purple) - Putting elements together to form a coherent whole</li> </ol>"},{"location":"sims/bloom-taxonomy/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/automating-instructional-design/sims/bloom-taxonomy/main.html\" height=\"452px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/bloom-taxonomy/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/bloom-taxonomy/#objective","title":"Objective","text":"<p>Students will understand and apply Bloom's Taxonomy to analyze and create effective learning objectives.</p>"},{"location":"sims/bloom-taxonomy/#activities","title":"Activities","text":"<ol> <li> <p>Exploration (5 minutes): Have students hover over each level of the pyramid to discover the action verbs and examples associated with each cognitive level.</p> </li> <li> <p>Discussion (10 minutes): As a class, discuss:</p> </li> <li>Why are the levels arranged in a pyramid?</li> <li>What makes \"Create\" more complex than \"Remember\"?</li> <li> <p>How do the levels build upon each other?</p> </li> <li> <p>Application (15 minutes): Give students a topic from their curriculum and have them write one learning objective for each level of the taxonomy.</p> </li> <li> <p>Analysis (10 minutes): Have students exchange objectives with a partner and identify which level each objective represents using the action verbs from the MicroSim.</p> </li> </ol>"},{"location":"sims/bloom-taxonomy/#assessment","title":"Assessment","text":"<p>Students demonstrate understanding by correctly classifying learning objectives and creating their own objectives at multiple cognitive levels.</p>"},{"location":"sims/bloom-taxonomy/#references","title":"References","text":"<ul> <li>Anderson, L.W., &amp; Krathwohl, D.R. (Eds.). (2001). A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives. Longman.</li> <li>Bloom, B.S. (Ed.). (1956). Taxonomy of Educational Objectives: The Classification of Educational Goals. Handbook I: Cognitive Domain. Longman.</li> </ul>"},{"location":"sims/graph-viewer/","title":"Learning Graph Viewer","text":"<p>This interactive viewer allows you to explore the learning graph for the Automating Instructional Design course.</p>"},{"location":"sims/graph-viewer/#features","title":"Features","text":"<ul> <li>Search: Type in the search box to find specific concepts</li> <li>Category Filtering: Use checkboxes to show/hide concept categories</li> <li>Interactive Navigation: Click and drag to explore, scroll to zoom</li> <li>Statistics: View real-time counts of visible nodes and edges</li> </ul>"},{"location":"sims/graph-viewer/#using-the-viewer","title":"Using the Viewer","text":"<ol> <li> <p>Search for Concepts: Start typing in the search box to find concepts. Click on a result to focus on that node.</p> </li> <li> <p>Filter by Category: Use the category checkboxes in the sidebar to show or hide groups of related concepts. Use \"Check All\" or \"Uncheck All\" for bulk operations.</p> </li> <li> <p>Navigate the Graph:</p> </li> <li>Drag to pan around the graph</li> <li>Scroll to zoom in and out</li> <li> <p>Click on a node to select it and highlight its connections</p> </li> <li> <p>View Statistics: The sidebar shows counts of visible nodes, edges, and foundational concepts.</p> </li> </ol>"},{"location":"sims/graph-viewer/#graph-structure","title":"Graph Structure","text":"<ul> <li>Foundational Concepts (left side): Prerequisites with no dependencies</li> <li>Advanced Concepts (right side): Topics that build on multiple prerequisites</li> <li>Edges: Arrows point from a concept to its prerequisites</li> </ul>"},{"location":"sims/graph-viewer/#launch-the-viewer","title":"Launch the Viewer","text":"<p>Open Learning Graph Viewer</p>"},{"location":"stories/","title":"Graphic Novel Stories","text":""},{"location":"stories/#blooms-taxonomy","title":"Bloom's Taxonomy","text":"<p>This graphic novel traces the intellectual journey of Benjamin Bloom and his collaborators as they confront the challenge of defining and organizing human learning, beginning in the uncertainty of mid-20th-century education and culminating in a framework that reshaped teaching worldwide. Through collaboration, reflection, and revision\u2014especially the later work of Bloom\u2019s students such as David Krathwohl\u2014the taxonomy evolves from a static hierarchy into a dynamic, living model that better reflects how people think, learn, and create. As the story advances into the digital age, Bloom\u2019s ideas become embedded in instructional design, learning analytics, and intelligent educational systems, quietly guiding both human educators and machines. The narrative closes by revealing Bloom\u2019s enduring legacy: a human-centered structure for learning that continues to adapt while preserving its core insight\u2014that understanding can be built, shared, and elevated through thoughtful design.</p>"},{"location":"stories/bloom/","title":"The Story of Bloom's Taxonomy","text":""},{"location":"stories/bloom/#panel-1-the-problem-of-learning","title":"Panel 1: The Problem of Learning","text":"Please generate a new image using a wide-landscape format and a 16:9 width:height aspect ratio. The image is done in the style of a graphic novel. A wide establishing shot set in the late 1940s at a university campus. The mood is serious and scholarly. Young educators from different disciplines gather in a dimly lit lecture hall filled with chalkboards, books, and stacks of exam papers. At the center stands Benjamin Bloom, thoughtful and slightly weary, holding a stack of student assessments. The atmosphere suggests confusion and frustration\u2014teachers struggling to agree on what \"learning\" really means. Visual symbolism: tangled lines above the group's heads representing fragmented thinking about education.  <p>In the years following World War II, American universities faced an unprecedented challenge. Millions of returning soldiers flooded classrooms under the GI Bill, and educators found themselves asking a question that had no clear answer: What does it actually mean to learn something? Benjamin Bloom, a young examiner at the University of Chicago, watched his colleagues argue in circles\u2014each discipline claiming its own definition of knowledge, its own standards for success. The chaos was unsustainable. Someone needed to bring order to the conversation.</p>"},{"location":"stories/bloom/#panel-2-a-question-in-the-dark","title":"Panel 2: A Question in the Dark","text":"Please generate a new image using a wide-landscape format and a 16:9 width:height aspect ratio. The image is done in the style of a graphic novel.  Make the characters be consistent with the character in the prior image.  Close-up of Bloom late at night in his office. A single desk lamp illuminates handwritten notes and diagrams. On the wall behind him are sketches of pyramids and ladders, partially erased and rewritten. Bloom looks determined but uncertain, staring at the question written on the chalkboard: \"How do we classify learning?\" This is the \"call to adventure\" moment. Shadows loom, symbolizing the complexity of human cognition.  <p>Night after night, Bloom returned to his office, wrestling with a question that seemed impossibly large. How could something as vast and varied as human learning be organized into categories that any teacher, in any subject, could use? He sketched ladders that went nowhere and pyramids that collapsed under scrutiny. The human mind wasn't a filing cabinet\u2014it was a living thing, growing and connecting in ways that defied simple labels. Yet Bloom refused to give up. If learning could be understood, it could be improved. And if it could be improved, then every student\u2014regardless of background\u2014could succeed.</p>"},{"location":"stories/bloom/#panel-3-the-council-of-minds","title":"Panel 3: The Council of Minds","text":"Create a wide-landscape (16:9) illustration in a classic graphic-novel style with hand-inked linework, subtle cross-hatching, and a warm mid-20th-century academic tone. Use a muted sepia and chalkboard-green color palette with soft amber lighting and strong light-and-shadow contrast. Maintain consistent character design for Benjamin Bloom: a slender, clean-shaven man in his 40s with thinning dark hair combed back, round wire-rim glasses, and conservative 1940s academic attire, expressing quiet intelligence and determination. The visual language should emphasize ideas becoming structure through recurring symbols\u2014tangled lines for confusion, ladders and pyramids for insight, and grids for clarity\u2014rendered as chalk drawings, notes, or overlays within the environment. Avoid photorealism, modern fonts, or bright colors; keep the composition cinematic, grounded, and intellectually heroic, ensuring the scene feels part of a cohesive narrative about the organization of human learning.  Bloom meets with fellow scholars\u2014Max Engelhart, Edward Furst, Walter Hill, and David Krathwohl\u2014around a large wooden table. Papers are spread everywhere. The group debates intensely, gesturing and pointing at notes. Speech bubbles (implied) show words like \"knowledge,\" \"skills,\" \"evaluation.\" The scene conveys collaboration as the first step toward order. A rough pyramid diagram begins to take shape in the center of the table.  <p>Bloom knew he couldn't solve this alone. He assembled a team of brilliant minds\u2014psychologists, curriculum specialists, and measurement experts\u2014who shared his obsession with understanding how people think. For years, they met at conferences and in cramped university offices, arguing passionately about where one type of thinking ended and another began. Was \"understanding\" different from \"knowing\"? Could \"analysis\" exist without \"application\"? Slowly, through countless debates and revisions, patterns began to emerge from the chaos.</p>"},{"location":"stories/bloom/#panel-4-the-first-structure-emerges","title":"Panel 4: The First Structure Emerges","text":"Panel 4:  Create a wide-landscape (16:9) illustration in a classic graphic-novel style with hand-inked linework, subtle cross-hatching, and a warm mid-20th-century academic tone. Use a muted sepia and chalkboard-green color palette with soft amber lighting and strong light-and-shadow contrast. Maintain consistent character design for Benjamin Bloom: a slender, clean-shaven man in his 40s with thinning dark hair combed back, round wire-rim glasses, and conservative 1940s academic attire, expressing quiet intelligence and determination. The visual language should emphasize ideas becoming structure through recurring symbols\u2014tangled lines for confusion, ladders and pyramids for insight, and grids for clarity\u2014rendered as chalk drawings, notes, or overlays within the environment. Avoid photorealism, modern fonts, or bright colors; keep the composition cinematic, grounded, and intellectually heroic, ensuring the scene feels part of a cohesive narrative about the organization of human learning.  The first Bloom Taxonomy pyramid is revealed dramatically, glowing slightly as if it is a powerful artifact. The six cognitive levels\u2014Knowledge, Comprehension, Application, Analysis, Synthesis, Evaluation\u2014are etched into stone steps. Bloom and his colleagues stand at the base, looking upward with awe. This represents the breakthrough: learning can be structured and shared.  <p>In 1956, the team published their masterwork: Taxonomy of Educational Objectives. The framework presented learning as a journey upward through six cognitive levels\u2014from simple recall of facts to the heights of evaluation and judgment. For the first time, educators had a shared language. A history teacher in Boston and a science teacher in Seattle could now discuss \"higher-order thinking\" and mean the same thing. The taxonomy wasn't just a classification system; it was a map of the mind itself.</p>"},{"location":"stories/bloom/#panel-5-a-framework-spreads-across-the-world","title":"Panel 5: A Framework Spreads Across the World","text":"Panel 5: Create a wide-landscape (16:9) illustration in a classic graphic-novel style with hand-inked linework, subtle cross-hatching, and a warm mid-20th-century academic tone. Use a muted sepia and chalkboard-green color palette with soft amber lighting and strong light-and-shadow contrast. Maintain consistent character design for Benjamin Bloom: a slender, clean-shaven man in his 40s with thinning dark hair combed back, round wire-rim glasses, and conservative 1940s academic attire, expressing quiet intelligence and determination. The visual language should emphasize ideas becoming structure through recurring symbols\u2014tangled lines for confusion, ladders and pyramids for insight, and grids for clarity\u2014rendered as chalk drawings, notes, or overlays within the environment. Avoid photorealism, modern fonts, or bright colors; keep the composition cinematic, grounded, and intellectually heroic, ensuring the scene feels part of a cohesive narrative about the organization of human learning.  A montage-style panel showing classrooms around the world adopting the taxonomy. Teachers point to the pyramid on chalkboards; students climb symbolic steps labeled with the taxonomy levels. Books, lesson plans, and exams now align neatly. Bloom watches from the background, older now, proud but contemplative. The impact is global, but the scene hints that something is still incomplete.  <p>The taxonomy spread like wildfire through education systems worldwide. Textbook publishers restructured their materials around its levels. Governments wrote it into national curricula. Teacher training programs made it required reading. Bloom's pyramid appeared on classroom walls from Tokyo to Toronto, a universal symbol of intellectual growth. Yet as the decades passed and Bloom watched his creation reshape global education, he sensed that something was missing. The world was changing, and the taxonomy would need to change with it.</p>"},{"location":"stories/bloom/#panel-6-cracks-in-the-pyramid","title":"Panel 6: Cracks in the Pyramid","text":"Panel 6: Create a wide-landscape (16:9) illustration in a classic graphic-novel style with hand-inked linework, subtle cross-hatching, and a warm mid-20th-century academic tone. Use a muted sepia and chalkboard-green color palette with soft amber lighting and strong light-and-shadow contrast. Maintain consistent character design for Benjamin Bloom: a slender, clean-shaven man now in his 70s with thinning dark hair combed back, round wire-rim glasses, and conservative 1970s academic attire, expressing quiet intelligence and determination. The visual language should emphasize ideas becoming structure through recurring symbols\u2014tangled lines for confusion, ladders and pyramids for insight, and grids for clarity\u2014rendered as chalk drawings, notes, or overlays within the environment. Avoid photorealism, modern fonts, or bright colors; keep the composition cinematic, grounded, and intellectually heroic, ensuring the scene feels part of a cohesive narrative about the organization of human learning.  Time passes. The pyramid begins to crack slightly. New educational challenges appear: computers enter classrooms, diverse learners raise new questions, and verbs like \"design,\" \"create,\" and \"reflect\" float in the air. Bloom, now elderly, looks on as his former students\u2014especially David Krathwohl\u2014step forward. This is the \"passing of the torch\" moment.  <p>By the 1990s, the educational landscape had transformed beyond recognition. Computers hummed in classrooms. Cognitive scientists had discovered new truths about how memory and understanding actually worked. Critics pointed out that the original taxonomy used static nouns\u2014\"knowledge,\" \"comprehension\"\u2014when learning was really about dynamic actions. Most troublingly, \"synthesis\" and \"evaluation\" seemed out of order; surely creating something new was the highest form of thinking? Bloom, now in his final years, recognized that his life's work needed heirs who would have the courage to improve upon it.</p>"},{"location":"stories/bloom/#panel-7-rewriting-the-language-of-learning","title":"Panel 7: Rewriting the Language of Learning","text":"Panel 7: Create a wide-landscape (16:9) illustration in a classic graphic-novel style with hand-inked linework, subtle cross-hatching, and a warm mid-20th-century academic tone. Use a muted sepia and chalkboard-green color palette with soft amber lighting and strong light-and-shadow contrast. The visual language should emphasize ideas becoming structure through recurring symbols\u2014tangled lines for confusion, ladders and pyramids for insight, and grids for clarity\u2014rendered as chalk drawings, notes, or overlays within the environment. Avoid photorealism, modern fonts, or bright colors; keep the composition cinematic, grounded, and intellectually heroic, ensuring the scene feels part of a cohesive narrative about the organization of human learning.  David Krathwohl is depicted as a thoughtful, composed scholar in his late 40s to early 50s, with neatly combed hair beginning to gray at the temples, wire-rim or thin rectangular glasses, and a calm, analytical expression that suggests careful listening before speaking. He wears understated mid-century academic attire and carries himself with quiet confidence, visually signaling a bridge between Bloom\u2019s foundational ideas and the next generation of educational theory.  Krathwohl and Bloom's students are shown in a modern seminar room around the year 2000. Laptops replace notebooks. On a digital screen, the taxonomy is reworked: nouns transform into action verbs. The pyramid rotates and becomes more dynamic. The group debates passionately, refining language and structure to better reflect how people actually learn.  <p>David Krathwohl, who had worked alongside Bloom since the very beginning, took up the mantle. Gathering a new generation of scholars, he led a comprehensive revision that honored Bloom's vision while adapting it for a new century. The static nouns became active verbs: \"Knowledge\" became \"Remember,\" \"Comprehension\" became \"Understand.\" Most significantly, \"Synthesis\" was renamed \"Create\" and moved to the very top of the pyramid\u2014acknowledging that generating new ideas represents the pinnacle of human cognition.</p>"},{"location":"stories/bloom/#panel-8-a-living-taxonomy","title":"Panel 8: A Living Taxonomy","text":"Panel 8: Create a wide-landscape (16:9) illustration in a classic graphic-novel style with hand-inked linework, subtle cross-hatching, and a warm mid-20th-century academic tone. Use a muted sepia and chalkboard-green color palette with soft amber lighting and strong light-and-shadow contrast. The visual language should emphasize ideas becoming structure through recurring symbols\u2014tangled lines for confusion, ladders and pyramids for insight, and grids for clarity\u2014rendered as chalk drawings, notes, or overlays within the environment. Avoid photorealism, modern fonts, or bright colors; keep the composition cinematic, grounded, and intellectually heroic, ensuring the scene feels part of a cohesive narrative about the organization of human learning.  The Revised Bloom Taxonomy (2001) is unveiled. The levels now read: Remember, Understand, Apply, Analyze, Evaluate, Create. \"Create\" glows at the top, symbolizing human innovation. A second dimension\u2014a knowledge matrix (Factual, Conceptual, Procedural, Metacognitive)\u2014appears as a grid behind the pyramid. The model feels more alive, adaptable, and future-ready.  <p>The Revised Bloom's Taxonomy, published in 2001, wasn't just an update\u2014it was a transformation. The new framework added a second dimension: the Knowledge Dimension, which distinguished between factual, conceptual, procedural, and metacognitive knowledge. Now educators could ask not just \"What level of thinking?\" but also \"What type of knowledge?\" The taxonomy had evolved from a simple ladder into a sophisticated matrix, capable of describing the full complexity of human learning.</p>"},{"location":"stories/bloom/#panel-9-bridging-minds-and-machines","title":"Panel 9: Bridging Minds and Machines","text":"Panel 9: Create a wide-landscape (16:9) illustration in a classic graphic-novel style with hand-inked linework, subtle cross-hatching, and a warm mid-20th-century academic tone. Use a muted sepia and chalkboard-green color palette with soft amber lighting and strong light-and-shadow contrast. The visual language should emphasize ideas becoming structure through recurring symbols\u2014tangled lines for confusion, ladders and pyramids for insight, and grids for clarity\u2014rendered as chalk drawings, notes, or overlays within the environment. Avoid photorealism, modern fonts, or bright colors; keep the composition cinematic, grounded, and intellectually heroic, ensuring the scene feels part of a cohesive narrative about the organization of human learning.  A split-panel effect shows the taxonomy being embedded into digital systems. On one side, instructional designers tag learning objectives with taxonomy levels. On the other, early AI systems and rule-based tutors use these tags to generate lessons and assessments. Lines of code subtly intertwine with the taxonomy steps, suggesting a bridge between human cognition and machines.  <p>As computers grew more powerful, something remarkable happened: Bloom's taxonomy became a bridge between human thinking and machine logic. Instructional designers began tagging learning objectives with taxonomy levels, creating structured data that software could understand. Early intelligent tutoring systems used these tags to sequence lessons and generate appropriate questions. The taxonomy\u2014born from handwritten notes and heated debates\u2014had become a protocol that allowed humans to communicate with machines about the nature of learning itself.</p>"},{"location":"stories/bloom/#panel-10-learning-at-scale","title":"Panel 10: Learning at Scale","text":"Panel 10: Create a wide-landscape (16:9) illustration in a classic graphic-novel style with hand-inked linework, subtle cross-hatching, and a brighter modern academic tone. Use a warm color palette with soft amber lighting and strong light-and-shadow contrast. The visual language should emphasize ideas becoming structure through recurring symbols\u2014tangled lines for confusion, ladders and pyramids for insight, and grids for clarity\u2014rendered as modern graphics drawings, notes, or overlays within the environment. Avoid photorealism, and user modern fonts, and brighter colors; keep the composition cinematic, grounded, and intellectually heroic, ensuring the scene feels part of a cohesive narrative about the organization of human learning.  Fast-forward to the present and near future. Intelligent instructional design tools automatically generate lesson plans, assessments, and adaptive pathways. Dashboards visualize Bloom levels in real time. Teachers act as guides, not lecturers. Bloom and his students appear as semi-transparent mentor figures in the background, watching their ideas power AI-driven education.  <p>Today, artificial intelligence systems can generate entire curricula aligned to Bloom's levels, automatically creating assessments that target specific cognitive skills. Adaptive learning platforms track each student's progress through the taxonomy in real time, offering personalized pathways that would have seemed like science fiction to Bloom and his colleagues. Teachers, freed from the burden of creating every lesson from scratch, can focus on what they do best: inspiring curiosity, providing mentorship, and nurturing the human connections that no algorithm can replace.</p>"},{"location":"stories/bloom/#panel-11-the-invisible-infrastructure","title":"Panel 11: The Invisible Infrastructure","text":"Panel 11: Create a wide-landscape (16:9) illustration in a classic graphic-novel style with hand-inked linework, subtle cross-hatching, and a brighter modern academic tone. Use a warm color palette with soft amber lighting and strong light-and-shadow contrast. The visual language should emphasize ideas becoming structure through recurring symbols\u2014tangled lines for confusion, ladders and pyramids for insight, and grids for clarity\u2014rendered as modern graphics drawings, notes, or overlays within the environment. Avoid photorealism, and user modern fonts, and brighter colors; keep the composition cinematic, grounded, and intellectually heroic, ensuring the scene feels part of a cohesive narrative about the organization of human learning.  A heroic culmination scene. The taxonomy stands tall like a timeless monument, now integrated with graphs, algorithms, and learning analytics. Diverse learners\u2014children, adults, neurodiverse students\u2014climb personalized paths. The model adapts without losing its core structure. A caption implies that Bloom's framework has become part of the invisible infrastructure of learning.  <p>Most learners today have never heard of Benjamin Bloom, yet his ideas shape their education every day. The taxonomy has become invisible infrastructure\u2014embedded in learning management systems, encoded in assessment algorithms, woven into the fabric of how we think about thinking. Like the electrical grid or the internet, it works so well that we forget it's there. Children in rural villages and executives in corporate boardrooms climb the same cognitive ladder, often without knowing it has a name.</p>"},{"location":"stories/bloom/#panel-12-a-legacy-that-creates","title":"Panel 12: A Legacy That Creates","text":"Panel 12: Create a wide-landscape (16:9) illustration in a classic graphic-novel style with hand-inked linework, subtle cross-hatching, and a warm mid-20th-century academic tone. Use a muted sepia and chalkboard-green color palette with soft amber lighting and strong light-and-shadow contrast. Maintain consistent character design for Benjamin Bloom: a slender, clean-shaven man in his 40s with thinning dark hair combed back, round wire-rim glasses, and conservative 1940s academic attire, expressing quiet intelligence and determination. The visual language should emphasize ideas becoming structure through recurring symbols\u2014tangled lines for confusion, ladders and pyramids for insight, and grids for clarity\u2014rendered as chalk drawings, notes, or overlays within the environment. Avoid photorealism, modern fonts, or bright colors; keep the composition cinematic, grounded, and intellectually heroic, ensuring the scene feels part of a cohesive narrative about the organization of human learning.  Final reflective panel. Bloom's original handwritten notes rest in a museum-like archive, while holographic versions of the taxonomy project into the air nearby. A closing visual metaphor shows a learner at the top of the structure, creating something new, while an AI assistant quietly supports them. The legacy is clear: a human-centered theory that continues to guide both educators and intelligent machines.  <p>And so the story continues. Benjamin Bloom set out to answer a simple question\u2014\"What does it mean to learn?\"\u2014and created a framework that would outlive him by generations. His taxonomy has been revised, digitized, and embedded into artificial intelligence, yet its core insight remains unchanged: human cognition is not random but structured, not fixed but developmental. Every learner can climb higher. Every mind can create. In an age of intelligent machines, Bloom's most enduring gift may be this: a reminder that education, at its heart, is about helping humans become more fully themselves.</p>"}]}